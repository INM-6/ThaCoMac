{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data preprocessing </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pypdfium2 as pdfium\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib\n",
    "import extract_info\n",
    "import parameters as params\n",
    "import download_and_process_pdf as dpp\n",
    "import dataframe_columns as df_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Predefined fucntions: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pubmed(source_path, output_path, start, end):\n",
    "    print(\"Starting preprocessing search results from PubMed...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, sep=',')\n",
    "    df = df[[\"DOI\", \"PMID\", \"PMCID\", \"Title\"]]\n",
    "    \n",
    "    for ind in range(start, end):\n",
    "        # sleep to avoid to be blocked\n",
    "        time.sleep(random.randint(1, 3))\n",
    "        \n",
    "        # request the webpage\n",
    "        # the columns PMID, Title don't contain np.nan\n",
    "        pmid = str(df[\"PMID\"][ind]).strip()\n",
    "        url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "        soup = plib.request_webpage(url)\n",
    "        # print(soup)\n",
    "        \n",
    "        # get pmcid\n",
    "        if df[\"PMCID\"][ind] != df[\"PMCID\"][ind]: # PMCID is np.nan\n",
    "            try:\n",
    "                pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                pmcid = np.nan\n",
    "        else: # PMCID is not np.nan\n",
    "            pmcid = str(df[\"PMCID\"][ind]).strip()\n",
    "        # print(pmcid)\n",
    "\n",
    "        # get doi\n",
    "        if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # DOI is np.nan\n",
    "            try:\n",
    "                doi = soup.find_all(\"span\", {\"class\": \"identifier doi\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                doi  = np.nan\n",
    "        else: # DOI is not np.nan\n",
    "            doi = str(df[\"DOI\"][ind]).strip()\n",
    "        # print(doi)\n",
    "\n",
    "        full_text_url = np.nan\n",
    "        pdf_url = np.nan\n",
    "        title = (df.at[ind, \"Title\"]).strip()\n",
    "        abstract = np.nan\n",
    "        keywords = np.nan\n",
    "        \n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"Title\": [title],\n",
    "            \"Abstract\": [abstract],\n",
    "            \"Keywords\": [keywords]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_pubmed\n",
    "# output_path = fpath.poten_litera_pubmed_processed\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, sep=',')\n",
    "# print(df.shape)\n",
    "# # (2612, 11)\n",
    "# df = df[[\"DOI\", \"PMID\", \"PMCID\", \"Title\"]]\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "\n",
    "# print(df[\"DOI\"].isnull().values.any())\n",
    "# print(df[\"PMID\"].isnull().values.any())\n",
    "# print(df[\"PMCID\"].isnull().values.any())\n",
    "# print(df[\"Title\"].isnull().values.any())\n",
    "# # True, False, True, Flase\n",
    "# # PMID, Title don't contain np.nan\n",
    "# # DOI, PMCID contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_pubmed(source_path, output_path, start, end)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_webofscience(source_path, output_path, start, end):\n",
    "    print(\"Starting preprocessing search results from Web of Science...\")\n",
    "    \n",
    "    df = pd.read_csv(source_path, sep=\",\")\n",
    "    df = df[[\"DOI\", \"Pubmed Id\", \"Article Title\", \"Abstract\", \"Author Keywords\", \"Keywords Plus\"]]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # sleep to avoid to be blocked\n",
    "        time.sleep(random.randint(1, 3))\n",
    "        \n",
    "        # the columns Article Title don't contain np.nan\n",
    "        # the columns DOI and PMID contain np.nan\n",
    "\n",
    "        # get pmid, doi\n",
    "        if df[\"Pubmed Id\"][ind] != df[\"Pubmed Id\"][ind]: # Pubmed Id is np.nan\n",
    "            if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # DOI is np.nan\n",
    "                doi = np.nan\n",
    "                pmid = np.nan\n",
    "            else: # DOI is not np.nan\n",
    "                doi = str(df[\"DOI\"][ind]).strip()\n",
    "                pmid = plib.doi2pmid(doi)\n",
    "        else: # Pubmed Id is not np.nan\n",
    "            pmid = str(int(df[\"Pubmed Id\"][ind])).strip()\n",
    "            if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # DOI is not np.nan\n",
    "                doi, a = plib.pmid2doi_pmcid(pmid)\n",
    "            else: # DOI is not np.nan\n",
    "                doi = str(df[\"DOI\"][ind]).strip()\n",
    "        \n",
    "        # get pmcid\n",
    "        if pmid != pmid: # pmid is np.nan\n",
    "            pmcid = np.nan\n",
    "        else: # pmid is not np.nan\n",
    "            # request the webpage\n",
    "            url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "            soup = plib.request_webpage(url)\n",
    "            # print(soup)\n",
    "\n",
    "            # get pmcid\n",
    "            try:\n",
    "                pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                pmcid = np.nan\n",
    "            # print(pmcid)\n",
    "            \n",
    "        full_text_url = np.nan\n",
    "        pdf_url = np.nan\n",
    "        title = str(df[\"Article Title\"][ind]).strip()\n",
    "        abstract = str(df[\"Abstract\"][ind]).strip()\n",
    "        keywords = str(df[\"Author Keywords\"][ind]).strip() + \"; \" + str(df[\"Keywords Plus\"][ind]).strip()\n",
    "\n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"Title\": [title],\n",
    "            \"Abstract\": [abstract],\n",
    "            \"Keywords\": [keywords]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# # source_path = fpath.poten_litera_wos\n",
    "# # output_path = fpath.poten_litera_wos_processed\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, sep=';')\n",
    "# df = df[[\"DOI\", \"Pubmed Id\", \"Article Title\", \"Abstract\", \"Author Keywords\", \"Keywords Plus\"]]\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "\n",
    "# print(df[\"DOI\"].isnull().values.any())\n",
    "# print(df[\"Pubmed Id\"].isnull().values.any())\n",
    "# print(df[\"Article Title\"].isnull().values.any())\n",
    "# print(df[\"Abstract\"].isnull().values.any())\n",
    "# print(df[\"Author Keywords\"].isnull().values.any())\n",
    "# print(df[\"Keywords Plus\"].isnull().values.any())\n",
    "# # True, True, False\n",
    "# # Article Title don't contain np.nan\n",
    "# # DOI, Pubmed Id contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code--------------------- \n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_webofscience(source_path, output_path, 0, 10)\n",
    "# ---------------------end of test code--------------------- \n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=';')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_eupmc(source_path, output_path, start, end):\n",
    "    print(\"Starting preprocessing search results from Europe PMC...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, sep=\",\")\n",
    "    df = df[[\"SOURCE\", \"DOI\", \"EXTERNAL_ID\", \"PMCID\", \"TITLE\"]]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # sleep to avoid to be blocked\n",
    "        time.sleep(random.randint(1, 3))\n",
    "\n",
    "        # get pmid, doi\n",
    "        # SOURCE = {'PMC', 'MED', 'ETH', 'PPR'}\n",
    "        if df[\"SOURCE\"][ind] != \"MED\": # SOURCE is not \"MED\" \n",
    "            if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # doi is np.nan\n",
    "                doi = np.nan\n",
    "                pmid = np.nan\n",
    "            else:\n",
    "                doi = str(df[\"DOI\"][ind]).strip()\n",
    "                pmid = plib.doi2pmid(doi)\n",
    "        else: # SOURCE is \"MED\"\n",
    "            # get doi, pmid\n",
    "            if df[\"EXTERNAL_ID\"][ind] != df[\"EXTERNAL_ID\"][ind]: # EXTERNAL_ID is np.nan\n",
    "                if df[\"DOI\"][ind] != df[\"DOI\"][ind](): # DOI is np.nan\n",
    "                    doi = np.nan\n",
    "                    pmid = np.nan\n",
    "                else: # DOI is not np.nan\n",
    "                    doi = str(df[\"DOI\"][ind]).strip()\n",
    "                    pmid = plib.doi2pmid(doi)\n",
    "            else: # EXTERNAL_ID is not np.nan\n",
    "                pmid = str(df[\"EXTERNAL_ID\"][ind]).strip()\n",
    "                if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # DOI is np.nan\n",
    "                    doi, a = plib.pmid2doi_pmcid(pmid)\n",
    "                else: # DOI is not np.nan\n",
    "                    doi = str(df[\"DOI\"][ind]).strip()\n",
    "                \n",
    "        # get pmcid\n",
    "        if pmid != pmid: # pmid is np.nan\n",
    "            pmcid = df[\"PMCID\"][ind]\n",
    "        else: # pmid is not np.nan\n",
    "            # request the webpage\n",
    "            url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "            soup = plib.request_webpage(url)\n",
    "            # print(soup)\n",
    "\n",
    "            # get pmcid\n",
    "            try:\n",
    "                pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                pmcid = np.nan\n",
    "            # print(pmcid)\n",
    "        \n",
    "        full_text_url = np.nan\n",
    "        pdf_url = np.nan\n",
    "        title = (df.at[ind, \"TITLE\"]).strip()\n",
    "        abstract = np.nan\n",
    "        keywords = np.nan\n",
    "        \n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"Title\": [title],\n",
    "            \"Abstract\": [abstract],\n",
    "            \"Keywords\": [keywords]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_eupmc\n",
    "# output_path = fpath.poten_litera_eupmc_processed\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, sep=',')\n",
    "# df = df[[\"SOURCE\", \"DOI\", \"EXTERNAL_ID\", \"PMCID\", \"TITLE\"]]\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "\n",
    "# col_one_list = set(df['SOURCE'].tolist())\n",
    "# print(col_one_list)\n",
    "# # ['PMC', 'MED', 'ETH', 'PPR']\n",
    "\n",
    "# print(df[\"SOURCE\"].isnull().values.any())\n",
    "# print(df[\"DOI\"].isnull().values.any())\n",
    "# print(df[\"EXTERNAL_ID\"].isnull().values.any())\n",
    "# print(df[\"PMCID\"].isnull().values.any())\n",
    "# print(df[\"TITLE\"].isnull().values.any())\n",
    "# # False, True, False, True, False\n",
    "# # SOURCE, EXTERNAL_ID, Title don't contain np.nan\n",
    "# # DOI, PMCID contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_eupmc(source_path, output_path, 0, 10)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_google_shcolar_step1(source_path, output_path, start, end):\n",
    "    print(\"Starting merging search results from Google Scholar...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, header=None, sep=',')\n",
    "    df.columns = [\"title\", \"url\", \"url_tag\", \"full_text_url\", \"full_text_tag\"]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # df[\"url_tag\"]: {'[CITATION][C]', '[HTML][HTML]', '[PDF][PDF]', '[BOOK][B]', nan}\n",
    "        # we don't need books, as they are not likely to include connecivity information\n",
    "        if df.at[ind, \"url_tag\"] == \"[BOOK][B]\":\n",
    "            continue\n",
    "        \n",
    "        # if url or title doesn't exsit AND full_text_url exist\n",
    "        if (df.at[ind, \"url\"] != df.at[ind, \"url\"] or df.at[ind, \"title\"] != df.at[ind, \"title\"]) and (df.at[ind, \"full_text_tag\"] == \"[PDF]\" or df.at[ind, \"full_text_tag\"] == \"[HTML]\"):\n",
    "            raise Exception(ind, \": url or title are both nan, but full_text_tag is [PDF] or [HTML]!\")\n",
    "\n",
    "        # if url or title doesn't exsit AND full_text_url doesn't exist\n",
    "        if (df.at[ind, \"url\"] != df.at[ind, \"url\"] or df.at[ind, \"title\"] != df.at[ind, \"title\"]):\n",
    "            continue \n",
    "        \n",
    "        title = str(df[\"title\"][ind]).strip()\n",
    "\n",
    "        # now every row has at least title and url\n",
    "        if df[\"url_tag\"][ind] == \"[PDF][PDF]\": # {'[CITATION][C]', '[HTML][HTML]', '[PDF][PDF]', nan}\n",
    "            if df[\"full_text_tag\"][ind] == \"[HTML]\": # {'[PDF]', '[HTML]', nan}\n",
    "                link = str(df[\"full_text_url\"][ind]).strip()\n",
    "                full_text_url, status_code  = plib.get_final_redirected_url(link)\n",
    "                if full_text_url == full_text_url:\n",
    "                    full_text_source = full_text_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                else:\n",
    "                    full_text_source = np.nan\n",
    "            else:\n",
    "                full_text_url = np.nan\n",
    "                full_text_source = np.nan\n",
    "            # get pdf_url, pdf_source\n",
    "            link = str(df[\"url\"][ind]).strip()\n",
    "            pdf_url, status_code = plib.get_final_redirected_url(link)\n",
    "        else: # {'[CITATION][C]', '[HTML][HTML]', nan}\n",
    "            link = str(df[\"url\"][ind]).strip()\n",
    "            full_text_url, status_code = plib.get_final_redirected_url(link)\n",
    "            if full_text_url == full_text_url:\n",
    "                full_text_source = full_text_url.split(\"://\")[1].split(\"/\")[0]\n",
    "            else:\n",
    "                full_text_source = np.nan\n",
    "            # get pdf_url, pdf_source\n",
    "            if df[\"full_text_tag\"][ind] == \"[PDF]\": # full_text_type = {'[HTML]', nan, '[PDF]'}\n",
    "                link = str(df[\"full_text_url\"][ind]).strip()\n",
    "                pdf_url, status_code  = plib.get_final_redirected_url(link)\n",
    "            else:\n",
    "                pdf_url = np.nan\n",
    "        \n",
    "        columns = [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\"]\n",
    "        row = {\n",
    "            \"Title\": [title],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"full_text_source\": [full_text_source],\n",
    "            \"pdf_url\": [pdf_url]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_gs\n",
    "# output_path = fpath.poten_litera_gs_processed_step1\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"title\", \"url\", \"url_tag\", \"full_text_url\", \"full_text_tag\"]\n",
    "# # print(df.head(3))\n",
    "# print(df.shape)\n",
    "# # (980, 5)\n",
    "\n",
    "# url_type = set(df['url_tag'].tolist())\n",
    "# print(url_type)\n",
    "# # {'[CITATION][C]', '[HTML][HTML]', '[PDF][PDF]', '[BOOK][B]', nan}\n",
    "# full_text_tag = set(df['full_text_tag'].tolist())\n",
    "# print(full_text_tag)\n",
    "# # {'[PDF]', '[HTML]', nan}\n",
    "# # ---------------------end of test code---------------------\n",
    "\n",
    "# # --------------------start of test code--------------------\n",
    "# # [\"title\", \"url\", \"url_tag\", \"full_text_url\", \"full_text_tag\"]\n",
    "# print(df[\"title\"].isnull().any().any())\n",
    "# print(df[\"url\"].isnull().any().any())\n",
    "# print(df[\"url_tag\"].isnull().any().any())\n",
    "# print(df[\"full_text_url\"].isnull().any().any())\n",
    "# print(df[\"full_text_tag\"].isnull().any().any())\n",
    "# # True, True, True, True, True\n",
    "# # title, url, url_tag, full_text_url, full_text_tag, all contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_google_shcolar_step1(source_path, output_path, 0, 1000)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_google_shcolar_step2(source_path, output_path, start, end):\n",
    "    print(\"Starting merging search results from Google Scholar...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, header=None, sep=',')\n",
    "    df.columns = [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\"]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # get doi from url\n",
    "        if df[\"full_text_url\"][ind] == df[\"full_text_url\"][ind]: # there's a full_text_url\n",
    "            url = str(df[\"full_text_url\"][ind]).strip()\n",
    "            source = url.split(\"://\")[1].split(\"/\")[0]\n",
    "            # check if the full_text_url is one of our websites\n",
    "            flag = False\n",
    "            for website in params.websites_gs:\n",
    "                if website in source:\n",
    "                    flag = True\n",
    "                    break\n",
    "            if not flag:\n",
    "                continue\n",
    "            info = extract_info.extract_info_from_webpage(url, params.websites_gs)\n",
    "            doi = info[\"doi\"]\n",
    "            pmid = info[\"pmid\"]\n",
    "            pmcid = info[\"pmcid\"]\n",
    "        else:\n",
    "            url = np.nan\n",
    "            doi = np.nan\n",
    "            pmid = np.nan\n",
    "            pmcid = np.nan\n",
    "        \n",
    "        full_text_url = url\n",
    "        pdf_url = df.at[ind, \"pdf_url\"]\n",
    "        title = df.at[ind, \"Title\"]\n",
    "        abstract = np.nan\n",
    "        keywords = np.nan\n",
    "\n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"Title\": [title],\n",
    "            \"Abstract\": [abstract],\n",
    "            \"Keywords\": [keywords]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(doi)\n",
    "        if doi != doi:\n",
    "            print([df[\"full_text_url\"][ind]])\n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_gs_processed_step1\n",
    "# output_path = fpath.poten_litera_gs_processed_step2\n",
    "# plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\"]\n",
    "# # print(df.head(3))\n",
    "# print(df.shape)\n",
    "# # (926, 4)\n",
    "# full_text_source = set(df['full_text_source'].tolist())\n",
    "# print(full_text_source)\n",
    "# # {'www.elibrary.ru', 'n.neurology.org', 'jnnp.bmj.com', 'anatomypubs.onlinelibrary.wiley.com', 'academic.oup.com', \n",
    "# #  'nyaspubs.onlinelibrary.wiley.com', 'cir.nii.ac.jp', 'link.springer.com', 'www.mdpi.com', 'pure.mpg.de', \n",
    "# #  'bmcneurosci.biomedcentral.com', 'elibrary.ru', 'journals.sagepub.com', 'tbiomed.biomedcentral.com', \n",
    "# #  'onlinelibrary.wiley.com', 'www.cambridge.org', 'wakespace.lib.wfu.edu', nan, 'www.cell.com', 'europepmc.org', \n",
    "# #  'var.scholarpedia.org', 'jpet.aspetjournals.org', 'journal.psych.ac.cn', 'www.biorxiv.org', 'ieeexplore.ieee.org', \n",
    "# #  'www.jstor.org', 'www.cabdirect.org', 'royalsocietypublishing.org', 'analyticalsciencejournals.onlinelibrary.wiley.com', \n",
    "# #  'open.bu.edu', 'journals.lww.com', 'www.eneuro.org', 'www.jstage.jst.go.jp', 'journals.plos.org', 'www.ncbi.nlm.nih.gov', \n",
    "# #  'www.liebertpub.com', 'neuro.psychiatryonline.org', 'www.sciencedirect.com', 'psycnet.apa.org', 'www.taylorfrancis.com', \n",
    "# #  'www.degruyter.com', 'www.nature.com', 'jamanetwork.com', 'karger.com', 'www.tandfonline.com', 'journals.physiology.org', \n",
    "# #  'movementdisorders.onlinelibrary.wiley.com', 'www.pnas.org', 'www.jneurosci.org', 'thejns.org', 'pascal-francis.inist.fr', \n",
    "# #  'physoc.onlinelibrary.wiley.com', 'agro.icm.edu.pl', 'elifesciences.org', 'www.frontiersin.org', 'escholarship.mcgill.ca', \n",
    "# #  'ajp.psychiatryonline.org', 'www.science.org', 'books.google.de'}\n",
    "\n",
    "# # {'elibrary.ru', 'neurology.org', 'bmj.com', 'wiley.com', 'oup.com', 'cir.nii.ac.jp', 'springer.com', 'mdpi.com', 'mpg.de', \n",
    "# #  'biomedcentral.com', 'sagepub.com', 'cambridge.org', 'wfu.edu', nan, 'cell.com', 'europepmc.org', 'scholarpedia.org', \n",
    "# #  'aspetjournals.org', 'psych.ac.cn', 'biorxiv.org', 'ieee.org', 'jstor.org', 'cabdirect.org', 'royalsocietypublishing.org', \n",
    "# #  'bu.edu', 'lww.com', 'eneuro.org', 'jst.go.jp', 'plos.org', 'ncbi.nlm.nih.gov', 'liebertpub.com', 'psychiatryonline.org', \n",
    "# #  'sciencedirect.com', 'psycnet.apa.org', 'taylorfrancis.com', 'degruyter.com', 'nature.com', 'jamanetwork.com', \n",
    "# #  'karger.com', 'www.tandfonline.com', 'physiology.org', 'www.pnas.org', 'jneurosci.org', 'thejns.org', \n",
    "# #  'pascal-francis.inist.fr', 'agro.icm.edu.pl', 'elifesciences.org', 'frontiersin.org', 'mcgill.ca', \n",
    "# #  'science.org', 'books.google.de'}\n",
    "\n",
    "# # websites_gs = {\n",
    "# #     'neurology.org', 'bmj.com', 'wiley.com', 'oup.com', 'springer.com', 'mdpi.com', \n",
    "# #     'biomedcentral.com', 'sagepub.com', 'cambridge.org', 'wfu.edu', 'cell.com', 'europepmc.org', \n",
    "# #     'aspetjournals.org', 'psych.ac.cn', 'biorxiv.org', 'ieee.org', 'jstor.org', 'royalsocietypublishing.org', \n",
    "# #     'bu.edu', 'lww.com', 'eneuro.org', 'jst.go.jp', 'plos.org', 'ncbi.nlm.nih.gov', 'liebertpub.com', \n",
    "# #     'psychiatryonline.org', 'sciencedirect.com', 'psycnet.apa.org', 'degruyter.com', 'nature.com', 'jamanetwork.com', \n",
    "# #     'karger.com', 'tandfonline.com', 'physiology.org', 'pnas.org', 'jneurosci.org', 'thejns.org', \n",
    "# #     'agro.icm.edu.pl', 'elifesciences.org', 'frontiersin.org', 'science.org'}\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# # [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\"]\n",
    "# print(df[\"Title\"].isnull().any().any())\n",
    "# print(df[\"full_text_url\"].isnull().any().any())\n",
    "# print(df[\"full_text_source\"].isnull().any().any())\n",
    "# print(df[\"pdf_url\"].isnull().any().any())\n",
    "# # False, True, True, True\n",
    "# # full_text_url, full_text_source, pdf_url contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_google_shcolar_step2(source_path, output_path, 0, 905)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_seed_paper_spanning(source_path, output_path):\n",
    "    print(\"Starting preprocessing search results from spanning citations of seed paper...\")\n",
    "    return True\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cocomac_paper(source_path, output_path):\n",
    "    print(\"Starting preprocessing search results from CoCoMac papers...\")\n",
    "    return True\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(input, output_path):\n",
    "    # combine all results\n",
    "    df = pd.DataFrame()\n",
    "    for search_result in input:\n",
    "        df_single = pd.read_csv(search_result, header=None, sep = \",\")\n",
    "        df = pd.concat([df, df_single], ignore_index=True, sort=False)\n",
    "    \n",
    "    df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.to_csv(output_path, header=False, index=False)\n",
    "# --------------------start of test code--------------------\n",
    "# gos = fpath.poten_litera_gs_processed_step2\n",
    "# wos = fpath.poten_litera_wos_processed\n",
    "# pubmed = fpath.poten_litera_pubmed_processed\n",
    "# eupmc = fpath.poten_litera_eupmc_processed\n",
    "# input = [gos, wos, pubmed, eupmc]\n",
    "# output_path = fpath.poten_litera_combined\n",
    "# # plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# combine(input, output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "# # (14627, 8)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_in_identifiers(input_path, output_path, start, end):\n",
    "    df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "    df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "    \n",
    "    # fill in missing identifiers\n",
    "    for ind in range(start, end):\n",
    "        # if all 3 identifiers are missing, and full_text_url and pdf_url are missing, skip\n",
    "        if df.at[ind, \"DOI\"] != df.at[ind, \"DOI\"] and df.at[ind, \"PMID\"] != df.at[ind, \"PMID\"] and df.at[ind, \"PMCID\"] != df.at[ind, \"PMCID\"] and df.at[ind, \"full_text_url\"] != df.at[ind, \"full_text_url\"] and df.at[ind, \"pdf_url\"] != df.at[ind, \"pdf_url\"]:\n",
    "            continue\n",
    "        \n",
    "        # initialzie\n",
    "        doi = np.nan\n",
    "        pmid = np.nan\n",
    "        pmcid = np.nan\n",
    "        full_text_url = df.at[ind, \"full_text_url\"]\n",
    "        pdf_url = df.at[ind, \"pdf_url\"]\n",
    "        title = df.at[ind, \"Title\"]\n",
    "        abstract = df.at[ind, \"Abstract\"]\n",
    "        keywords = df.at[ind, \"Keywords\"]\n",
    "\n",
    "        # if all 3 identifiers are missing\n",
    "        if df.at[ind, \"DOI\"] != df.at[ind, \"DOI\"] and df.at[ind, \"PMID\"] != df.at[ind, \"PMID\"] and df.at[ind, \"PMCID\"] != df.at[ind, \"PMCID\"]:\n",
    "            columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "            row = {\n",
    "                \"DOI\": [doi],\n",
    "                \"PMID\": [pmid],\n",
    "                \"PMCID\": [pmcid],\n",
    "                \"full_text_url\": [full_text_url],\n",
    "                \"pdf_url\": [pdf_url],\n",
    "                \"Title\": [title],\n",
    "                \"Abstract\": [abstract],\n",
    "                \"Keywords\": [keywords]\n",
    "            }\n",
    "\n",
    "            if not plib.add_row_to_csv(output_path, row, columns):\n",
    "                print(\"Error detected when adding a row to csv!\")\n",
    "\n",
    "            print(ind)\n",
    "            continue\n",
    "        \n",
    "        # we have at least one of the 3 identifiers\n",
    "        # doi, pmid\n",
    "        if df[\"DOI\"][ind] == df[\"DOI\"][ind]: # DOI -> PMID\n",
    "            doi = str(df[\"DOI\"][ind]).strip().lower()\n",
    "            # print(doi)\n",
    "            if df[\"PMID\"][ind] == df[\"PMID\"][ind]:\n",
    "                pmid = str(df[\"PMID\"][ind]).strip()\n",
    "                # print(pmid)\n",
    "            else:\n",
    "                pmid = plib.doi2pmid(doi)\n",
    "                # print(pmid)\n",
    "                if pmid != pmid:\n",
    "                    pmid_cadidate = plib.title2pmid(title)\n",
    "                    # print(pmid_cadidate)\n",
    "        elif df[\"PMID\"][ind] == df[\"PMID\"][ind]: # PMID -> DOI\n",
    "            pmid = str(int(df[\"PMID\"][ind])).strip()\n",
    "            # print(pmid)\n",
    "            doi, pmcid = plib.pmid2doi_pmcid(pmid)\n",
    "            # print(doi)\n",
    "        elif df[\"PMCID\"][ind] == df[\"PMCID\"][ind]: # PMCID -> DOI, PMID\n",
    "            pmcid = str(df[\"PMCID\"][ind]).strip()\n",
    "            try:\n",
    "                doi, pmid = plib.pmcid2doi_pmid(pmcid)\n",
    "            except:\n",
    "                doi = np.nan\n",
    "                pmid = np.nan\n",
    "            # print(doi)\n",
    "            # print(pmid)\n",
    "        else:\n",
    "            doi = np.nan\n",
    "            pmid = np.nan\n",
    "        # print(doi)\n",
    "        # print(pmid)\n",
    "        \n",
    "        # pmcid\n",
    "        if df[\"PMCID\"][ind] == df[\"PMCID\"][ind]:\n",
    "            pmcid = str(df[\"PMCID\"][ind]).strip()\n",
    "        elif pmid == pmid:\n",
    "            a, pmcid = plib.pmid2doi_pmcid(pmid)\n",
    "        else:\n",
    "            pmcid = np.nan\n",
    "        # print(pmcid)\n",
    "    \n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"Title\": [title],\n",
    "            \"Abstract\": [abstract],\n",
    "            \"Keywords\": [keywords]\n",
    "        }\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "\n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# input_path = fpath.poten_litera_combined\n",
    "# # output_path = fpath.poten_litera_filled\n",
    "# # plib.clear_file(output_path)\n",
    "# df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "# print(df.shape)\n",
    "# df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "# print(df[\"DOI\"].isnull().any().any())\n",
    "# print(df[\"PMID\"].isnull().any().any())\n",
    "# print(df[\"PMCID\"].isnull().any().any())\n",
    "# print(df[\"full_text_url\"].isnull().any().any())\n",
    "# print(df[\"pdf_url\"].isnull().any().any())\n",
    "# print(df[\"Title\"].isnull().any().any())\n",
    "# print(df[\"Abstract\"].isnull().any().any())\n",
    "# print(df[\"Keywords\"].isnull().any().any())\n",
    "# True, True, True, True, True, False, True, True\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# fill_in_identifiers(input_path, output_path, 0, 14690)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_remove_dupli(input_path, output_path, columns, identifiers): \n",
    "    df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "    df.columns = columns\n",
    "\n",
    "    # remove all duplicates\n",
    "    for identifier in identifiers:\n",
    "        remove_dup_by = identifier\n",
    "        df = df[df[remove_dup_by].isnull() | ~df[df[remove_dup_by].notnull()].duplicated(subset=remove_dup_by, keep='first')]\n",
    "        # df = df.drop_duplicates(subset=['DOI'])\n",
    "        # df = df.drop_duplicates(subset=['PMID'])\n",
    "        # df = df.drop_duplicates(subset=['PMCID'])\n",
    "\n",
    "    # reset index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df.to_csv(output_path, header=False, index=False)\n",
    "    print(\"Duplication in the potential related literature removed.\")\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_ids_filled\n",
    "# output_path = fpath.poten_litra_filtered\n",
    "# plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# merge all search results\n",
    "# identifiers = [\"DOI\", \"PMID\", \"PMCID\"]\n",
    "# columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "# merge_remove_dupli(source_path, output_path, identifiers)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_text_url_filling(input_path, output_path, start, end):\n",
    "    df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "    df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "    \n",
    "    for ind in range(start, end):\n",
    "        time.sleep(random.randint(1, 3))\n",
    "\n",
    "        # initialize\n",
    "        doi = df.at[ind, \"DOI\"]\n",
    "        pmid = df.at[ind, \"PMID\"]\n",
    "        pmcid = df.at[ind, \"PMCID\"]\n",
    "        full_text_url = np.nan\n",
    "        pdf_url = df.at[ind, \"pdf_url\"]\n",
    "        title = df.at[ind, \"Title\"]\n",
    "        abstract = df.at[ind, \"Abstract\"]\n",
    "        keywords = df.at[ind, \"Keywords\"]\n",
    "\n",
    "        # get full text link from pmcid\n",
    "        if full_text_url!= full_text_url and pmcid == pmcid:\n",
    "            url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + str(pmcid).strip() + \"/\"\n",
    "            full_text_url, status_code = plib.get_final_redirected_url(url)\n",
    "            if status_code == 200: # some papers are assined pmcid, but the pmc wepage isn't avaiable until 2024\n",
    "                full_text_url = full_text_url\n",
    "            else:\n",
    "                print(status_code, \"Error when trying to get final redirected url from\", url)\n",
    "                full_text_url = np.nan\n",
    "        \n",
    "        # get full text url from doi\n",
    "        if full_text_url != full_text_url and doi == doi:\n",
    "            url = \"https://doi.org/\" + str(doi).strip().lower()\n",
    "            full_text_url, status_code = plib.get_final_redirected_url(url)\n",
    "            if status_code == 200 or status_code == 403:\n",
    "                full_text_url = full_text_url\n",
    "            else:\n",
    "                print(status_code, \"Error when trying to get final redirected url from\", url)\n",
    "                full_text_url = np.nan\n",
    "                \n",
    "        # get full text url from pmid\n",
    "        if full_text_url != full_text_url and pmid == pmid:\n",
    "            url = \"https://pubmed.ncbi.nlm.nih.gov/\" + str(int(df.at[ind, \"PMID\"])).strip() + \"/\"\n",
    "            try:\n",
    "                soup = plib.request_webpage(url)\n",
    "                link = soup.find(\"div\", {\"class\": \"full-text-links-list\"}).find(\"a\", {\"class\": \"link-item pmc\"})[\"href\"]\n",
    "                full_text_url, status_code = plib.get_final_redirected_url(link)\n",
    "                if status_code == 200:\n",
    "                    full_text_url = full_text_url\n",
    "                else:\n",
    "                    print(status_code, \"Error when trying to get final redirected url from\", url)\n",
    "                    full_text_url = np.nan\n",
    "                    raise Exception()\n",
    "            except:\n",
    "                try:\n",
    "                    soup = plib.request_webpage(url)\n",
    "                    link = soup.find(\"div\", {\"class\": \"full-text-links-list\"}).find(\"a\", {\"class\": \"link-item dialog-focus\"})[\"href\"]\n",
    "                    full_text_url, status_code = plib.get_final_redirected_url(link)\n",
    "                    if status_code == 200 or status_code == 403:\n",
    "                        full_text_url = full_text_url\n",
    "                    else:\n",
    "                        print(status_code, \"Error when trying to get final redirected url from\", url)\n",
    "                        full_text_url = np.nan\n",
    "                        raise Exception()\n",
    "                except:\n",
    "                    full_text_url = np.nan\n",
    "               \n",
    "        # get full text url from df.at[ind, \"full_text_url\"] \n",
    "        if full_text_url != full_text_url and df.at[ind, \"full_text_url\"] == df.at[ind, \"full_text_url\"]:\n",
    "            full_text_url, status_code = plib.get_final_redirected_url(df.at[ind, \"full_text_url\"])\n",
    "            if status_code == 200 or status_code == 403:\n",
    "                full_text_url = full_text_url\n",
    "            else:\n",
    "                print(status_code, \"Error when trying to get final redirected url from\", df.at[ind, \"full_text_url\"])\n",
    "                full_text_url = np.nan                  \n",
    "        \n",
    "        # get full text url from pmid\n",
    "        if full_text_url != full_text_url and pmid == pmid:\n",
    "            url = \"https://pubmed.ncbi.nlm.nih.gov/\" + str(int(df.at[ind, \"PMID\"])).strip() + \"/\"\n",
    "            full_text_url, status_code = plib.get_final_redirected_url(url)\n",
    "            if status_code == 200:\n",
    "                full_text_url = full_text_url\n",
    "            else:\n",
    "                print(status_code, \"Error when trying to get final redirected url from\", url)\n",
    "                full_text_url = np.nan  \n",
    "        \n",
    "        # get pdf url, pdf source\n",
    "        if pdf_url == pdf_url:\n",
    "            try:\n",
    "                pdf_url, status_code = plib.get_final_redirected_url(pdf_url)\n",
    "                if pdf_url == pdf_url:\n",
    "                    pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                else:\n",
    "                    pdf_source = np.nan\n",
    "            except:\n",
    "                print(\"error when getting final redirected url from: \", pdf_url)\n",
    "                pdf_url = np.nan\n",
    "                pdf_source = np.nan\n",
    "        else:\n",
    "            pdf_source = np.nan\n",
    "\n",
    "        # get full text source\n",
    "        if full_text_url == full_text_url:\n",
    "            full_text_source = full_text_url.split(\"://\")[1].split(\"/\")[0]\n",
    "        else:\n",
    "            full_text_source = np.nan\n",
    "        \n",
    "        # make sure doi is in lower case\n",
    "        if doi == doi:\n",
    "            doi = doi.lower()\n",
    "\n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"full_text_source\": [full_text_source],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"pdf_source\": [pdf_source],\n",
    "            \"Title\": [title],\n",
    "            \"Abstract\": [abstract],\n",
    "            \"Keywords\": [keywords]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_filling(input_path, output_path, start, end):\n",
    "    # scan each row in the potential related literature and extract information\n",
    "    df = pd.read_csv(input_path, header=None, sep=\",\")\n",
    "    df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # info = {\n",
    "        #     \"doi\": np.nan,\n",
    "        #     \"pmid\": np.nan,\n",
    "        #     \"pmcid\": np.nan,\n",
    "        #     \"title\": np.nan,\n",
    "        #     \"abstract\": np.nan,\n",
    "        #     \"keywords\": np.nan,\n",
    "        #     \"pdf_link\": np.nan\n",
    "        # }\n",
    "\n",
    "        # initialzie\n",
    "        index = df.at[ind, \"INDEX\"]\n",
    "        doi = df.at[ind, \"DOI\"]\n",
    "        pmid = df.at[ind, \"PMID\"]\n",
    "        pmcid = df.at[ind, \"PMCID\"]\n",
    "        full_text_url = df.at[ind, \"FULL_TEXT_URL\"]\n",
    "        full_text_source = df.at[ind, \"FULL_TEXT_SOURCE\"]\n",
    "        pdf_url = np.nan\n",
    "        pdf_source = np.nan\n",
    "        title = df.at[ind, \"TITLE\"]\n",
    "        abstract = df.at[ind, \"ABSTRACT\"]\n",
    "        keywords = df.at[ind, \"KEYWORDS\"]\n",
    "\n",
    "        if full_text_url != full_text_url: # full text url not found\n",
    "            if df.at[ind, \"PDF_URL\"] == df.at[ind, \"PDF_URL\"]:\n",
    "                url = str(df.at[ind, \"PDF_URL\"]).strip()\n",
    "                url1, status_code = plib.get_final_redirected_url(url)\n",
    "                if status_code == 200:\n",
    "                    pdf_url = url\n",
    "                    pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                elif status_code == 403:\n",
    "                    # print(status_code, \"when getting final redirected url: \", url)\n",
    "                    pdf_url = url\n",
    "                    pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                else:\n",
    "                    print(status_code, \"when getting final redirected url: \", url)\n",
    "                    pdf_url = np.nan\n",
    "                    pdf_source = np.nan  \n",
    "            else:\n",
    "                print(\"full text url and pdf url are not available!\")\n",
    "                pdf_url = np.nan\n",
    "                pdf_source = np.nan\n",
    "        elif ('.pdf' in full_text_url and full_text_url.split('.pdf')[1] == '') or ('.PDF' in full_text_url and full_text_url.split('.PDF')[1] == ''):\n",
    "            print(\"full text url is a pdf file: \", full_text_url)\n",
    "            pdf_url = full_text_url\n",
    "            pdf_source = full_text_source\n",
    "            full_text_url = np.nan\n",
    "            full_text_source = np.nan\n",
    "        else: # full text url found\n",
    "            flag = False\n",
    "            for website in params.websites:\n",
    "                if website in full_text_source:\n",
    "                    flag = True\n",
    "                    break\n",
    "            if not flag:\n",
    "                continue\n",
    "\n",
    "            url = str(full_text_url).strip()\n",
    "            try:\n",
    "                info = extract_info.extract_info_from_webpage(url, params.websites)\n",
    "            except:\n",
    "                raise Exception(\"Error! Cannot extract information from the webpage: \", url)\n",
    "            \n",
    "            # doi\n",
    "            if info['doi'] == info['doi'] and doi == doi and info['doi'] != doi:\n",
    "                print(doi)\n",
    "                print(info['doi'])\n",
    "\n",
    "            if info['doi'] == info['doi']:\n",
    "                doi = info['doi'].lower()\n",
    "            else:\n",
    "                doi = doi\n",
    "            \n",
    "            # pmid\n",
    "            if info['pmid'] == info['pmid'] and df.at[ind, \"PMID\"] == df.at[ind, \"PMID\"] and str(int(info['pmid'])) != str(int(df.at[ind, \"PMID\"])):\n",
    "                print(str(int(df.at[ind, \"PMID\"]))) \n",
    "                print(str(int(info['pmid'])))      \n",
    "\n",
    "            if info['pmid'] == info['pmid']:\n",
    "                pmid = str(int(info['pmid']))\n",
    "            elif pmid == pmid:\n",
    "                pmid = str(int(pmid)).strip()\n",
    "            else:\n",
    "                pmid = np.nan\n",
    "            \n",
    "            # pmcid\n",
    "            if info['pmcid'] == info['pmcid'] and pmcid == pmcid and info['pmcid'] != pmcid:\n",
    "                print(pmcid)\n",
    "                print(info['pmcid'])\n",
    "\n",
    "            if info['pmcid'] == info['pmcid']:\n",
    "                pmcid = info['pmcid']\n",
    "            else:\n",
    "                pmcid = df.at[ind, \"PMCID\"]\n",
    "            \n",
    "            # full_text_url, full_text_surce\n",
    "            if full_text_url == full_text_url:\n",
    "                full_text_source = full_text_url.split(\"://\")[1].split(\"/\")[0]\n",
    "            else:\n",
    "                print(\"full text url is not available\")\n",
    "                full_text_url = np.nan\n",
    "                full_text_source = np.nan\n",
    "\n",
    "            if pdf_url != pdf_url and info['pdf_link'] == info['pdf_link']:\n",
    "                pdf_url = str(info['pdf_link']).strip()\n",
    "                pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                # try:\n",
    "                #     pdf_url, status_code = plib.get_final_redirected_url(url)\n",
    "                #     if status_code == 200:\n",
    "                #         pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                #     elif status_code == 403:\n",
    "                #         # print(status_code, \"when getting final redirected url: \", url)\n",
    "                #         pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                #     else:\n",
    "                #         print(status_code, \"when getting final redirected url: \", url)\n",
    "                #         pdf_url = np.nan\n",
    "                #         pdf_source = np.nan\n",
    "                # except:\n",
    "                #     pdf_url = np.nan\n",
    "                #     pdf_source = np.nan \n",
    "                    \n",
    "            if pdf_url != pdf_url and df.at[ind, \"PDF_URL\"] == df.at[ind, \"PDF_URL\"]:\n",
    "                print(\"PDF_URL not extracted from info: , but existed already: \", df.at[ind, \"PDF_URL\"])\n",
    "                pdf_url = df.at[ind, \"PDF_URL\"]\n",
    "                pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "            \n",
    "            if pdf_url != pdf_url:\n",
    "                if full_text_url == full_text_url and full_text_url.split(\"://\")[1].split(\"/\")[0] == 'www.ncbi.nlm.nih.gov':\n",
    "                    if doi == doi:\n",
    "                        url = \"https://doi.org/\" + doi\n",
    "                        url, status_code = plib.get_final_redirected_url(url)\n",
    "                        info = extract_info.extract_info_from_webpage(url, params.websites)\n",
    "                        full_text_url = url\n",
    "                        full_text_source = url.split(\"://\")[1].split(\"/\")[0]\n",
    "                        pdf_url = info[\"pdf_link\"]\n",
    "                        pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                    else:\n",
    "                        pdf_url = np.nan\n",
    "                        pdf_source = np.nan\n",
    "                else:  \n",
    "                    print(\"PDF_URL not found for: \", doi, pmid, pmcid, full_text_url)\n",
    "                    pdf_url = np.nan\n",
    "                    pdf_source = np.nan\n",
    "                \n",
    "            # title\n",
    "            if info['title'] == info['title']:\n",
    "                title = info['title']\n",
    "                title = title.replace(\";\", \",\")\n",
    "            else:\n",
    "                title = title\n",
    "            \n",
    "            # abstract\n",
    "            if info['abstract'] == info['abstract']:\n",
    "                abstract = info['abstract']\n",
    "                abstract = ''.join(e for e in abstract if (e.isalpha() or e == \" \" or e == \"-\"))\n",
    "            else:\n",
    "                abstract = abstract\n",
    "            \n",
    "            # keywords\n",
    "            if info['keywords'] == info['keywords']:\n",
    "                keywords = info['keywords']\n",
    "                keywords = keywords.replace(\";\", \",\")\n",
    "                keywords = ''.join(e for e in keywords if (e.isalpha() or e == \" \" or e == \"-\" or e == \",\"))\n",
    "            else:\n",
    "                keywords = keywords\n",
    "        \n",
    "        columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "        \n",
    "        row = {\n",
    "            \"INDEX\": [index],\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"FULL_TEXT_URL\": [full_text_url],\n",
    "            \"FULL_TEXT_SOURCE\": [full_text_source],\n",
    "            \"PDF_URL\": [pdf_url],\n",
    "            \"PDF_SOURCE\": [pdf_source],\n",
    "            \"TITLE\": [title],\n",
    "            \"ABSTRACT\": [abstract],\n",
    "            \"KEYWORDS\": [keywords]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# input_path = fpath.poten_litera_ids_ftl_filled\n",
    "# output_path = fpath.poten_litera_litera_db\n",
    "\n",
    "# # clear file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# info_filling(input_path, output_path)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(input_path, pdf_folder, start, end):\n",
    "    df = pd.read_csv(input_path, header=None, sep=',')\n",
    "    df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # time.sleep(3)\n",
    "        \n",
    "        # the flag to indicate whether the pdf is downloaded successfully, if not, save the row to poten_litera_pdf_not_available.csv\n",
    "        flag = False\n",
    "\n",
    "        pdf_url = df.at[ind, \"PDF_URL\"]\n",
    "\n",
    "        # pdf_url not found\n",
    "        if pdf_url != pdf_url:\n",
    "            flag = False\n",
    "        # pdf_url found\n",
    "        else:\n",
    "            doi = df.at[ind, \"DOI\"]\n",
    "            if dpp.download_and_rename_pdf(pdf_url, doi, df.at[ind, \"INDEX\"], pdf_folder):\n",
    "                flag = True\n",
    "            else:\n",
    "                print(df.at[ind, \"FULL_TEXT_URL\"])\n",
    "                flag = False\n",
    "\n",
    "        # # pdf_url not found\n",
    "        # if pdf_url != \"://linkinghub.elsevier.com/\":\n",
    "        #     continue\n",
    "        # # pdf_url found\n",
    "        # else:\n",
    "        #     doi = df.at[ind, \"DOI\"]\n",
    "        #     if dpp.download_and_rename_pdf(pdf_url, doi, df.at[ind, \"INDEX\"], pdf_folder):\n",
    "        #         flag = True\n",
    "        #     else:\n",
    "        #         print(df.at[ind, \"FULL_TEXT_URL\"])\n",
    "        #         flag = False\n",
    "        \n",
    "        if not flag:\n",
    "            print(\"PDF_URL not found or PDF not successfully downloaded for: \")\n",
    "            print(\"\\n\")\n",
    "            print(df.at[ind, \"INDEX\"], df.at[ind, \"DOI\"], df.at[ind, \"PMID\"], df.at[ind, \"PMCID\"], df.at[ind, \"FULL_TEXT_URL\"], df.at[ind, \"FULL_TEXT_SOURCE\"], df.at[ind, \"PDF_URL\"], df.at[ind, \"PDF_SOURCE\"], df.at[ind, \"TITLE\"], df.at[ind, \"ABSTRACT\"], df.at[ind, \"KEYWORDS\"])\n",
    "            # print(\"\\n\")\n",
    "\n",
    "        line_number_in_csv = ind + 1\n",
    "        print(ind, \" Line number:\", line_number_in_csv, \" INDEX:\", int(df.at[ind, \"INDEX\"]))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf2text(pdf_path, text_path): \n",
    "    # try:   \n",
    "        text = \"\"\n",
    "        pdf = pdfium.PdfDocument(pdf_path)\n",
    "        n_pages = len(pdf)\n",
    "        # print(n_pages)\n",
    "\n",
    "        for i in range(n_pages):\n",
    "            page = pdf[i]\n",
    "            textpage = page.get_textpage()\n",
    "            text += textpage.get_text_range() + \" \"\n",
    "            [g.close() for g in (textpage, page)]\n",
    "        pdf.close()\n",
    "\n",
    "        # preprocess the text\n",
    "        text = plib.process_text(text, lower=False)\n",
    "\n",
    "        with open(text_path, \"w\", encoding='ascii') as f:\n",
    "            f.write(text)\n",
    "        f.close()\n",
    "    # except:\n",
    "    #     print(\"ERROR!\")\n",
    "    #     print(\"When converting pdf to text for: \", pdf_path)\n",
    "# --------------------start of test code--------------------\n",
    "# index = 99\n",
    "# # index = 0\n",
    "# pdf_folder = fpath.pdf_folder\n",
    "# text_folder = fpath.text_folder\n",
    "\n",
    "# pdf_file_name = str(index) + \".pdf\"\n",
    "# pdf_path = os.path.join(pdf_folder, pdf_file_name)\n",
    "# text_path = os.path.join(text_folder, pdf_file_name.split(\".pdf\")[0] + \".txt\")\n",
    "# pdf2text(pdf_path, text_path)\n",
    "\n",
    "# with open(text_path, \"r\", encoding='ascii') as f:\n",
    "#     text = f.read()\n",
    "#     print(text)\n",
    "# f.close()\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json2text(json_path, text_path):    \n",
    "    with open(json_path, \"r\") as f:\n",
    "        json_file = json.load(f)\n",
    "        text = json_file[\"full-text-retrieval-response\"][\"originalText\"]\n",
    "    f.close()\n",
    "\n",
    "    # print(text)\n",
    "    title = json_file[\"full-text-retrieval-response\"][\"coredata\"][\"dc:title\"]\n",
    "\n",
    "    # preprocess the text\n",
    "    text = text.split(title, 1)[1].strip()\n",
    "    text = title + \". \" + text\n",
    "    # print(text)\n",
    "    text = plib.process_text(text, lower=False)\n",
    "    # print(text)\n",
    "\n",
    "    with open(text_path, \"w\", encoding='ascii') as f:\n",
    "        f.write(text)\n",
    "    f.close()\n",
    "# --------------------start of test code--------------------\n",
    "# index = 99\n",
    "# pdf_folder = fpath.pdf_folder\n",
    "# text_folder = fpath.text_folder\n",
    "\n",
    "# json_file_name = str(index) + \".json\"\n",
    "# json_path = os.path.join(pdf_folder, json_file_name)\n",
    "# text_path = os.path.join(text_folder, str(index) + \".txt\")\n",
    "# # print(pdf_path)\n",
    "# # print(text_path)\n",
    "# json2text(json_path, text_path)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Main program: </h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 1. Preprocess and combine search results </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from PubMed\n",
    "\n",
    "# source_path = fpath.poten_litera_pubmed\n",
    "# output_path = fpath.poten_litera_pubmed_processed\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from PubMed\n",
    "# # 2612 results\n",
    "# preprocess_pubmed(source_path, output_path, 0, 2612)\n",
    "# print(\"preprocessing results from PubMed succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from PubMed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine search results from Web of Science\n",
    "# # clear the file\n",
    "# plib.clear_file(fpath.poten_litera_wos)\n",
    "\n",
    "# # combine the 2 files of search results from web of science\n",
    "# source_path_1 = fpath.poten_litera_wos_1\n",
    "# source_path_2 = fpath.poten_litera_wos_2\n",
    "# df_1 = pd.read_csv(source_path_1, sep=',')\n",
    "# df_2 = pd.read_csv(source_path_2, sep=',')\n",
    "# df_1.to_csv(fpath.poten_litera_wos, header=True, index=False, sep=\",\")\n",
    "# df_2.to_csv(fpath.poten_litera_wos, mode=\"a\", header=False, index=False, sep=\",\")\n",
    "# # --------------------start of test code--------------------\n",
    "# df = pd.read_csv(fpath.poten_litera_wos, sep=',')\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "# # (1993, 72)\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from Web of Science\n",
    "\n",
    "# source_path = fpath.poten_litera_wos\n",
    "# output_path = fpath.poten_litera_wos_processed\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from Web of Science\n",
    "# # 1993 results\n",
    "# preprocess_webofscience(source_path, output_path, 0, 1993)\n",
    "# print(\"preprocessing results from Web of Science succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from Web of Science!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from Europe PMC\n",
    "\n",
    "# source_path = fpath.poten_litera_eupmc\n",
    "# output_path = fpath.poten_litera_eupmc_processed\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from Europe PMC\n",
    "# preprocess_eupmc(source_path, output_path, 2980, 9178)\n",
    "# # 9178 results\n",
    "# print(\"preprocessing results from Europe PMC succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from Europe PMC!\")\n",
    "\n",
    "# # 2980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from Google Scholar step 1\n",
    "\n",
    "# source_path = fpath.poten_litera_gs\n",
    "# # 980 results\n",
    "# output_path = fpath.poten_litera_gs_processed_step1\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from Google Scholar\n",
    "# preprocess_google_shcolar_step1(source_path, output_path, 0, 980)\n",
    "# # 926 results\n",
    "# print(\"step 1 of preprocessing results from Google Scholar succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from Google Scholar step 1!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reset index for poten_litera_gs_processed_step1\n",
    "# input_path = fpath.poten_litera_gs_processed_step1\n",
    "# output_path = fpath.poten_litera_gs_processed_step1\n",
    "# df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df.to_csv(output_path, header=False, index=False)\n",
    "\n",
    "# input_path = fpath.poten_litera_gs_processed_step1\n",
    "# df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "# print(df.shape)\n",
    "# # (926, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from Google Scholar step 2\n",
    "\n",
    "# source_path = fpath.poten_litera_gs_processed_step1\n",
    "# # (926, 4)\n",
    "# output_path = fpath.poten_litera_gs_processed_step2\n",
    "\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from Google Scholar\n",
    "# preprocess_google_shcolar_step2(source_path, output_path, 0, 926)\n",
    "# print(\"step 2 of preprocessing results from Google Scholar succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from Google Scholar step 2!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reset index for poten_litera_gs_processed_step2\n",
    "# input_path = fpath.poten_litera_gs_processed_step2\n",
    "# output_path = fpath.poten_litera_gs_processed_step2\n",
    "# df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df.to_csv(output_path, header=False, index=False)\n",
    "\n",
    "# input_path = fpath.poten_litera_gs_processed_step2\n",
    "# df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "# print(df.shape)\n",
    "# # (926, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from spanning citations of seed paper\n",
    "\n",
    "# preprocess_seed_paper_spanning(source_path, output_path, columns):\n",
    "# print(\"preprocessing results from spanning citations of seed papers succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from spanning citations of seed papers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from CoCoMac papers\n",
    "\n",
    "# preprocess_cocomac_paper(source_path, output_path, columns)\n",
    "# print(\"preprocessing results from CoCoMac papers succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from CoCoMac papers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # take a look at all the preprossed search results\n",
    "# gos = fpath.poten_litera_gs_processed_step2\n",
    "# wos = fpath.poten_litera_wos_processed\n",
    "# pubmed = fpath.poten_litera_pubmed_processed\n",
    "# eupmc = fpath.poten_litera_eupmc_processed\n",
    "\n",
    "# df_gs = pd.read_csv(gos, header=None, sep=',')\n",
    "# print(df_gs.shape)\n",
    "# # (907, 6)\n",
    "# df_wos = pd.read_csv(wos, header=None, sep=',')\n",
    "# print(df_wos.shape)\n",
    "# # (1993, 8)\n",
    "# df_pubmed = pd.read_csv(pubmed, header=None, sep=',')\n",
    "# print(df_pubmed.shape)\n",
    "# # (2612, 8)\n",
    "# df_eupmc = pd.read_csv(eupmc, header=None, sep=',')\n",
    "# print(df_eupmc.shape)\n",
    "# # (9178, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine all search results\n",
    "\n",
    "# gos = fpath.poten_litera_gs_processed_step2\n",
    "# wos = fpath.poten_litera_wos_processed\n",
    "# pubmed = fpath.poten_litera_pubmed_processed\n",
    "# eupmc = fpath.poten_litera_eupmc_processed\n",
    "# input = [gos, wos, pubmed, eupmc]\n",
    "# output_path = fpath.poten_litera_combined\n",
    "\n",
    "# # clear the file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# combine(input, output_path)\n",
    "# # (14627, 8)\n",
    "# print(\"Combining all search results succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when combining all search results!\")\n",
    "\n",
    "# df_combined = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df_combined.shape)\n",
    "# # (14690, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fill in missing identifiers\n",
    "# input_path = fpath.poten_litera_combined\n",
    "# output_path = fpath.poten_litera_ids_filled\n",
    "\n",
    "# # clear file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# fill_in_identifiers(input_path, output_path, 0, 14690)\n",
    "# print(\"Filling in missing elements succeeded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check the missing elements in the combined search results\n",
    "# source_path = fpath.poten_litera_ids_filled\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "# # for ind in df.index:\n",
    "# #     if df.at[ind, \"DOI\"] != df.at[ind, \"DOI\"] and df.at[ind, \"PMID\"] != df.at[ind, \"PMID\"] and df.at[ind, \"PMCID\"] != df.at[ind, \"PMCID\"] and df.at[ind, \"full_text_url\"] != df.at[ind, \"full_text_url\"] and df.at[ind, \"pdf_url\"] != df.at[ind, \"pdf_url\"]:\n",
    "# #         print(ind)\n",
    "# #         print(df.at[ind, \"Title\"])\n",
    "# #         print(df.at[ind, \"full_text_url\"])\n",
    "# #         print(df.at[ind, \"pdf_url\"])\n",
    "\n",
    "# # for ind in df.index:\n",
    "# #     if df.at[ind, \"DOI\"] != df.at[ind, \"DOI\"] and df.at[ind, \"PMID\"] != df.at[ind, \"PMID\"] and df.at[ind, \"PMCID\"] != df.at[ind, \"PMCID\"] and df.at[ind, \"full_text_url\"] != df.at[ind, \"full_text_url\"]:\n",
    "# #         print(ind)\n",
    "# #         print(df.at[ind, \"Title\"])\n",
    "# #         print(df.at[ind, \"full_text_url\"])\n",
    "# #         print(df.at[ind, \"pdf_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge all search results and remove duplication by identifiers\n",
    "\n",
    "# source_path = fpath.poten_litera_ids_filled\n",
    "# output_path = fpath.poten_litra_filtered\n",
    "\n",
    "# # clear the file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# # merge all search results\n",
    "# identifiers = [\"DOI\", \"PMID\", \"PMCID\"]\n",
    "# columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "# merge_remove_dupli(source_path, output_path, columns, identifiers)\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litra_filtered\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# print(df.shape)\n",
    "# # (10982, 8)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2. Filling in missing information and construct database </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fill in full_text_url\n",
    "\n",
    "# source_path = fpath.poten_litra_filtered\n",
    "# output_path = fpath.poten_litera_ids_ftl_filled\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # merge all search results\n",
    "# full_text_url_filling(source_path, output_path, 0, 10980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove duplications and reset the index and add index column\n",
    "# source_path = fpath.poten_litera_ids_ftl_filled\n",
    "# output_path = fpath.poten_litera_ids_ftl_filled_filtered\n",
    "\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "# identifiers = [\"DOI\", \"PMID\", \"PMCID\"]\n",
    "# # merge_remove_dupli(source_path, output_path, columns, identifiers)\n",
    "\n",
    "# for identifier in identifiers:\n",
    "#     remove_dup_by = identifier\n",
    "#     df = df[df[remove_dup_by].isnull() | ~df[df[remove_dup_by].notnull()].duplicated(subset=remove_dup_by, keep='last')]\n",
    "\n",
    "# # reset index\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# df.to_csv(output_path, header=False, index=True)\n",
    "# print(\"Duplication in the potential related literature removed.\")\n",
    "# # --------------------start of test code--------------------\n",
    "# output_path = fpath.poten_litera_ids_ftl_filled_filtered\n",
    "# df_output = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df_output.head(5))\n",
    "# print(df_output.shape)\n",
    "# # (10980, 11)\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check the missing elements\n",
    "# source_path = fpath.poten_litera_ids_ftl_filled_filtered\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "\n",
    "# # # if all 3 identifiers are missing, and full_text_url and pdf_url are missing\n",
    "# # for ind in df.index:\n",
    "# #     if df.at[ind, \"DOI\"] != df.at[ind, \"DOI\"] and df.at[ind, \"PMID\"] != df.at[ind, \"PMID\"] and df.at[ind, \"PMCID\"] != df.at[ind, \"PMCID\"] and df.at[ind, \"full_text_url\"] != df.at[ind, \"full_text_url\"] and df.at[ind, \"pdf_url\"] != df.at[ind, \"pdf_url\"]:\n",
    "# #         print(ind)\n",
    "# #         print(df.at[ind, \"Title\"])\n",
    "# #         print(df.at[ind, \"full_text_url\"])\n",
    "# #         print(df.at[ind, \"pdf_url\"])\n",
    "# # # None\n",
    "\n",
    "# # if all 3 identifiers are missing and full_text_url is missing, but pdf_url is available\n",
    "# for ind in df.index:\n",
    "#     if df.at[ind, \"DOI\"] != df.at[ind, \"DOI\"] and df.at[ind, \"PMID\"] != df.at[ind, \"PMID\"] and df.at[ind, \"PMCID\"] != df.at[ind, \"PMCID\"] and df.at[ind, \"full_text_url\"] != df.at[ind, \"full_text_url\"]:\n",
    "#         print(ind)\n",
    "#         print(df.at[ind, \"Title\"])\n",
    "#         print(df.at[ind, \"full_text_url\"])\n",
    "#         print(df.at[ind, \"pdf_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check all possible full_text_source\n",
    "# input_path = fpath.poten_litera_ids_ftl_filled_filtered\n",
    "# df = pd.read_csv(input_path, header=None, sep=\",\")\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "# print(df.shape)\n",
    "# # (10980, 11)\n",
    "\n",
    "# # [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "# print(df[\"FULL_TEXT_URL\"].isnull().any().any()) # True\n",
    "# print(df[\"FULL_TEXT_SOURCE\"].isnull().any().any()) # True\n",
    "# print(df[\"TITLE\"].isnull().any().any()) # False\n",
    "\n",
    "# print(df[\"INDEX\"].dtypes) # int64\n",
    "# print(df[\"DOI\"].dtypes) # object\n",
    "# print(df[\"PMID\"].dtypes) # float64\n",
    "# print(df[\"PMCID\"].dtypes) # object\n",
    "# print(df[\"FULL_TEXT_URL\"].dtypes) # object\n",
    "# print(df[\"FULL_TEXT_SOURCE\"].dtypes) # object\n",
    "# print(df[\"PDF_URL\"].dtypes) # object\n",
    "# print(df[\"PDF_SOURCE\"].dtypes) # object\n",
    "# print(df[\"TITLE\"].dtypes) # object\n",
    "# print(df[\"ABSTRACT\"].dtypes) # object\n",
    "# print(df[\"KEYWORDS\"].dtypes) # object\n",
    "\n",
    "# full_text_source_dict = set(df['FULL_TEXT_SOURCE'].tolist())\n",
    "# print(full_text_source_dict)\n",
    "# full_text_source_dict = {'pharmrev.aspetjournals.org', 'www.ingentaconnect.com', 'pubs.aip.org', 'journal.psych.ac.cn', 'iovs.arvojournals.org', \n",
    "# 'linkinghub.elsevier.com', 'www.architalbiol.org', 'open.bu.edu', 'psycnet.apa.org:443', 'jamanetwork.com', \n",
    "# 'link.springer.com', 'europepmc.org', 'www.ncbi.nlm.nih.gov', 'thejns.org', 'ujms.net', 'jpet.aspetjournals.org', \n",
    "# 'journals.biologists.com', 'www.thieme-connect.de', 'academic.oup.com', 'direct.mit.edu', 'ajp.psychiatryonline.org', \n",
    "# 'journals.lww.com', 'wakespace.lib.wfu.edu', 'www.ahajournals.org', 'symposium.cshlp.org', 'www.microbiologyresearch.org', \n",
    "# 'journals.aps.org', 'www.cambridge.org', 'www.imrpress.com', 'www.jstor.org', 'www.researchsquare.com', 'www.science.org', \n",
    "# 'content.iospress.com:443', 'n.neurology.org', 'royalsocietypublishing.org', 'ieeexplore.ieee.org', 'neuro.psychiatryonline.org', \n",
    "# 'pubmed.ncbi.nlm.nih.gov', 'analyticalsciencejournals.onlinelibrary.wiley.com', 'www.rbojournal.org', 'papers.ssrn.com', \n",
    "# 'www.worldscientific.com', 'www.jstage.jst.go.jp', 'webview.isho.jp', 'www.degruyter.com', 'www.taylorfrancis.com', \n",
    "# 'www.biorxiv.org', 'nan', 'www.liebertpub.com', 'opg.optica.org', 'jnm.snmjournals.org', 'neurologia.com', 'www.nature.com', \n",
    "# 'karger.com', 'www.tandfonline.com', 'onlinelibrary.wiley.com', 'www.ajtmh.org', 'pubs.acs.org', 'www.annualreviews.org', \n",
    "# 'journals.physiology.org', 'journals.sagepub.com', 'pubs.asahq.org', 'nrc-prod.literatumonline.com'}\n",
    "\n",
    "# # dict2 = {}\n",
    "\n",
    "# # print(full_text_source_dict == dict2) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # websites_hosts\n",
    "# # {'pharmrev.aspetjournals.org', 'www.ingentaconnect.com', 'pubs.aip.org', 'journal.psych.ac.cn', 'iovs.arvojournals.org', \n",
    "# # 'linkinghub.elsevier.com', 'www.architalbiol.org', 'open.bu.edu', 'psycnet.apa.org:443', 'jamanetwork.com', \n",
    "# # 'link.springer.com', 'europepmc.org', 'www.ncbi.nlm.nih.gov', 'thejns.org', 'ujms.net', 'jpet.aspetjournals.org', \n",
    "# # 'journals.biologists.com', 'www.thieme-connect.de', 'academic.oup.com', 'direct.mit.edu', 'ajp.psychiatryonline.org', \n",
    "# # 'journals.lww.com', 'wakespace.lib.wfu.edu', 'www.ahajournals.org', 'symposium.cshlp.org', 'www.microbiologyresearch.org', \n",
    "# # 'journals.aps.org', 'www.cambridge.org', 'www.imrpress.com', 'www.jstor.org', 'www.researchsquare.com', 'www.science.org', \n",
    "# # 'content.iospress.com:443', 'n.neurology.org', 'royalsocietypublishing.org', 'ieeexplore.ieee.org', 'neuro.psychiatryonline.org', \n",
    "# # 'pubmed.ncbi.nlm.nih.gov', 'analyticalsciencejournals.onlinelibrary.wiley.com', 'www.rbojournal.org', 'papers.ssrn.com', \n",
    "# # 'www.worldscientific.com', 'www.jstage.jst.go.jp', 'webview.isho.jp', 'www.degruyter.com', 'www.taylorfrancis.com', \n",
    "# # 'www.biorxiv.org', 'nan', 'www.liebertpub.com', 'opg.optica.org', 'jnm.snmjournals.org', 'neurologia.com', 'www.nature.com', \n",
    "# # 'karger.com', 'www.tandfonline.com', 'onlinelibrary.wiley.com', 'www.ajtmh.org', 'pubs.acs.org', 'www.annualreviews.org', \n",
    "# # 'journals.physiology.org', 'journals.sagepub.com', 'pubs.asahq.org', 'nrc-prod.literatumonline.com'}\n",
    "\n",
    "# websites_hosts = [\n",
    "#     'aspetjournals.org', 'www.ingentaconnect.com', 'pubs.aip.org', 'journal.psych.ac.cn', 'iovs.arvojournals.org', \n",
    "#     'linkinghub.elsevier.com', 'www.architalbiol.org', 'open.bu.edu', 'psycnet.apa.org', 'jamanetwork.com', \n",
    "#     'link.springer.com', 'europepmc.org', 'www.ncbi.nlm.nih.gov', 'thejns.org', 'ujms.net',\n",
    "#     'journals.biologists.com', 'www.thieme-connect.de', 'academic.oup.com', 'direct.mit.edu', 'psychiatryonline.org', \n",
    "#     'journals.lww.com', 'wakespace.lib.wfu.edu', 'www.ahajournals.org', 'symposium.cshlp.org', 'www.microbiologyresearch.org', \n",
    "#     'journals.aps.org', 'www.cambridge.org', 'www.imrpress.com', 'www.jstor.org', 'www.researchsquare.com', 'www.science.org', \n",
    "#     'content.iospress.com', 'neurology.org', 'royalsocietypublishing.org', 'ieeexplore.ieee.org', \n",
    "#     'pubmed.ncbi.nlm.nih.gov', 'wiley.com', \n",
    "#     'www.rbojournal.org', 'papers.ssrn.com', 'www.worldscientific.com', 'www.jstage.jst.go.jp', 'webview.isho.jp', \n",
    "#     'www.degruyter.com', 'www.taylorfrancis.com', 'www.biorxiv.org', 'nan', 'www.liebertpub.com', 'opg.optica.org', \n",
    "#     'jnm.snmjournals.org', 'neurologia.com', 'www.nature.com', 'karger.com', 'www.tandfonline.com',\n",
    "#     'www.ajtmh.org', 'pubs.acs.org', 'www.annualreviews.org', 'journals.physiology.org', 'journals.sagepub.com', \n",
    "#     'pubs.asahq.org', 'literatumonline.com'\n",
    "# ]\n",
    "\n",
    "# # --------------------start of test code--------------------\n",
    "# if len(websites_hosts) == len(set(websites_hosts)):\n",
    "#     print(\"There are no duplicates in the list.\")\n",
    "# else:\n",
    "#     print(\"There are duplicates in the list.\")\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sort the websites by the number of articles they have\n",
    "# input_path = fpath.poten_litera_ids_ftl_filled_filtered\n",
    "# df = pd.read_csv(input_path, header=None, sep=\",\")\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "# func_dict = {website: 0 for website in websites_hosts}\n",
    "# # print(func_dict)\n",
    "\n",
    "# for ind in df.index:\n",
    "#     if df.at[ind, \"FULL_TEXT_SOURCE\"] != df.at[ind, \"FULL_TEXT_SOURCE\"]:\n",
    "#         func_dict[\"nan\"] += 1\n",
    "#         continue\n",
    "#     for website in websites_hosts:\n",
    "#         if website in df.at[ind, \"FULL_TEXT_SOURCE\"]:\n",
    "#             func_dict[website] += 1\n",
    "#             break\n",
    "\n",
    "# # Sort dictionary by values\n",
    "# sorted_dict = dict(sorted(func_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "# print(sorted_dict)\n",
    "# # {'www.ncbi.nlm.nih.gov': 7913, 'linkinghub.elsevier.com': 1023, 'wiley.com': 701, 'link.springer.com': 288, \n",
    "# # 'journals.physiology.org': 200, 'academic.oup.com': 152, 'pubmed.ncbi.nlm.nih.gov': 147, 'www.cambridge.org': 74, \n",
    "# # 'karger.com': 54, 'journals.lww.com': 49, 'www.nature.com': 45, 'nan': 38, 'www.science.org': 30, \n",
    "# # 'www.tandfonline.com': 29, 'journals.sagepub.com': 21, 'jamanetwork.com': 20, 'neurology.org': 16, \n",
    "# # 'www.biorxiv.org': 15, 'europepmc.org': 14, 'iovs.arvojournals.org': 13, 'royalsocietypublishing.org': 13, \n",
    "# # 'psycnet.apa.org': 12, 'psychiatryonline.org': 12, 'direct.mit.edu': 11, 'www.jstage.jst.go.jp': 11, \n",
    "# # 'thejns.org': 8, 'www.annualreviews.org': 8, 'aspetjournals.org': 7, 'jnm.snmjournals.org': 7, \n",
    "# # 'www.architalbiol.org': 4, 'www.ahajournals.org': 4, 'content.iospress.com': 3, 'www.worldscientific.com': 3, \n",
    "# # 'www.liebertpub.com': 3, 'pubs.acs.org': 3, 'www.thieme-connect.de': 2, 'opg.optica.org': 2, \n",
    "# # 'neurologia.com': 2, 'pubs.asahq.org': 2, 'www.ingentaconnect.com': 1, 'pubs.aip.org': 1, 'journal.psych.ac.cn': 1, \n",
    "# # 'open.bu.edu': 1, 'ujms.net': 1, 'journals.biologists.com': 1, 'wakespace.lib.wfu.edu': 1, \n",
    "# # 'symposium.cshlp.org': 1, 'www.microbiologyresearch.org': 1, 'journals.aps.org': 1, 'www.imrpress.com': 1, \n",
    "# # 'www.jstor.org': 1, 'www.researchsquare.com': 1, 'ieeexplore.ieee.org': 1, 'www.rbojournal.org': 1, \n",
    "# # 'papers.ssrn.com': 1, 'webview.isho.jp': 1, 'www.degruyter.com': 1, 'www.taylorfrancis.com': 1, \n",
    "# # 'www.ajtmh.org': 1, 'literatumonline.com': 1}\n",
    "\n",
    "# non_zero_keys = [key for key, value in sorted_dict.items() if value != 0]\n",
    "# print(non_zero_keys)\n",
    "# # ['www.ncbi.nlm.nih.gov', 'linkinghub.elsevier.com', 'wiley.com', 'link.springer.com', 'journals.physiology.org', \n",
    "# # 'academic.oup.com', 'pubmed.ncbi.nlm.nih.gov', 'www.cambridge.org', 'karger.com', 'journals.lww.com', \n",
    "# # 'www.nature.com', 'nan', 'www.science.org', 'www.tandfonline.com', 'journals.sagepub.com', 'jamanetwork.com', \n",
    "# # 'neurology.org', 'www.biorxiv.org', 'europepmc.org', 'iovs.arvojournals.org', 'royalsocietypublishing.org', \n",
    "# # 'psycnet.apa.org', 'psychiatryonline.org', 'direct.mit.edu', 'www.jstage.jst.go.jp', 'thejns.org', \n",
    "# # 'www.annualreviews.org', 'aspetjournals.org', 'jnm.snmjournals.org', 'www.architalbiol.org', 'www.ahajournals.org', \n",
    "# # 'content.iospress.com', 'www.worldscientific.com', 'www.liebertpub.com', 'pubs.acs.org', 'www.thieme-connect.de', \n",
    "# # 'opg.optica.org', 'neurologia.com', 'pubs.asahq.org', 'www.ingentaconnect.com', 'pubs.aip.org', 'journal.psych.ac.cn', \n",
    "# # 'open.bu.edu', 'ujms.net', 'journals.biologists.com', 'wakespace.lib.wfu.edu', 'symposium.cshlp.org', \n",
    "# # 'www.microbiologyresearch.org', 'journals.aps.org', 'www.imrpress.com', 'www.jstor.org', 'www.researchsquare.com', \n",
    "# # 'ieeexplore.ieee.org', 'www.rbojournal.org', 'papers.ssrn.com', 'webview.isho.jp', 'www.degruyter.com', \n",
    "# # 'www.taylorfrancis.com', 'www.ajtmh.org', 'literatumonline.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # websites\n",
    "# websites = [\n",
    "#     'www.ncbi.nlm.nih.gov', 'linkinghub.elsevier.com', 'wiley.com', 'link.springer.com', 'journals.physiology.org', \n",
    "#     'academic.oup.com', 'pubmed.ncbi.nlm.nih.gov', 'www.cambridge.org', 'karger.com', 'journals.lww.com', \n",
    "#     'www.nature.com', 'nan', 'www.science.org', 'www.tandfonline.com', 'journals.sagepub.com', 'jamanetwork.com', \n",
    "#     'neurology.org', 'www.biorxiv.org', 'europepmc.org', 'iovs.arvojournals.org', 'royalsocietypublishing.org', \n",
    "#     'psycnet.apa.org', 'psychiatryonline.org', 'direct.mit.edu', 'www.jstage.jst.go.jp', 'thejns.org', \n",
    "#     'www.annualreviews.org', 'aspetjournals.org', 'jnm.snmjournals.org', 'www.architalbiol.org', 'www.ahajournals.org', \n",
    "#     'content.iospress.com', 'www.worldscientific.com', 'www.liebertpub.com', 'pubs.acs.org', 'www.thieme-connect.de', \n",
    "#     'opg.optica.org', 'neurologia.com', 'pubs.asahq.org', 'www.ingentaconnect.com', 'pubs.aip.org', 'journal.psych.ac.cn', \n",
    "#     'open.bu.edu', 'ujms.net', 'journals.biologists.com', 'wakespace.lib.wfu.edu', 'symposium.cshlp.org', \n",
    "#     'www.microbiologyresearch.org', 'journals.aps.org', 'www.imrpress.com', 'www.jstor.org', 'www.researchsquare.com', \n",
    "#     'ieeexplore.ieee.org', 'www.rbojournal.org', 'papers.ssrn.com', 'webview.isho.jp', 'www.degruyter.com', \n",
    "#     'www.taylorfrancis.com', 'www.ajtmh.org', 'literatumonline.com'\n",
    "# ]\n",
    "# # --------------------start of test code--------------------\n",
    "# # if len(websites) == len(websites_hosts):\n",
    "# #     print('The number of websites is correct')\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # websites to remove\n",
    "\n",
    "# # Reasons to move literature from these websites:\n",
    "# # 1. nan\n",
    "# # 2. full text not available\n",
    "# # 3. non-English\n",
    "# # 4. theses or dissertations\n",
    "\n",
    "# # 'pubmed.ncbi.nlm.nih.gov', \n",
    "# # https://pubmed.ncbi.nlm.nih.gov/4984008/\n",
    "# # full text not available\n",
    "# # 'nan', \n",
    "# # full text not available\n",
    "# # 'psycnet.apa.org', \n",
    "# # https://psycnet.apa.org/doiLanding?doi=10.1037%2F0735-7044.112.3.719\n",
    "# # full text not available\n",
    "# # 'content.iospress.com', \n",
    "# # https://content.iospress.com:443/articles/restorative-neurology-and-neuroscience/rnn140440\n",
    "# # full text not available\n",
    "# # 'www.worldscientific.com', \n",
    "# # https://www.worldscientific.com/doi/abs/10.1142/S0192415X06004296\n",
    "# # full text not available\n",
    "# # 'www.liebertpub.com', \n",
    "# # https://www.liebertpub.com/doi/10.1089/hum.2006.17.291\n",
    "# # full text not available\n",
    "# # 'opg.optica.org', \n",
    "# # https://opg.optica.org/josaa/viewmedia.cfm?uri=josaa-3-10-1726&seq=0\n",
    "# # full text not available\n",
    "# # 'neurologia.com', \n",
    "# # https://neurologia.com/articulo/99529\n",
    "# # non-English\n",
    "# # 'pubs.aip.org', \n",
    "# # https://pubs.aip.org/asa/jasa/article-abstract/104/5/2935/560140/Click-train-encoding-in-primary-auditory-cortex-of?redirectedFrom=fulltext\n",
    "# # full text not available\n",
    "# # 'journal.psych.ac.cn', \n",
    "# # https://journal.psych.ac.cn/adps/EN/abstract/abstract3663.shtml\n",
    "# # full text not available\n",
    "# # 'open.bu.edu', \n",
    "# # https://open.bu.edu/handle/2144/12127\n",
    "# # theses or dissertations\n",
    "# # 'wakespace.lib.wfu.edu', \n",
    "# # https://wakespace.lib.wfu.edu/handle/10339/37434\n",
    "# # theses or dissertations, full text not available\n",
    "# # 'symposium.cshlp.org', \n",
    "# # https://symposium.cshlp.org/content/61/39.long\n",
    "# # full text not available\n",
    "# # 'www.jstor.org', \n",
    "# # https://www.jstor.org/stable/82698\n",
    "# # full text not available\n",
    "# # 'www.rbojournal.org', \n",
    "# # https://www.rbojournal.org/article/influencia-da-thalamosinusotomia-sob-diferentes-pressoes-intra-oculares-constantes-na-facilidade-de-drenagem-do-humor-aquoso-em-olhos-de-porco/\n",
    "# # non-English\n",
    "# # 'webview.isho.jp', \n",
    "# # https://webview.isho.jp/openurl?rft.genre=article&rft.issn=1881-6096&rft.volume=63&rft.issue=5&rft.spage=473\n",
    "# # non-English\n",
    "# # 'www.degruyter.com', \n",
    "# # https://www.degruyter.com/document/doi/10.1515/REVNEURO.1998.9.4.291/html\n",
    "# # full text not available\n",
    "# # 'www.taylorfrancis.com', \n",
    "# # https://www.taylorfrancis.com/books/edit/10.4324/9780203449226/attention-action-glyn-humphreys-jane-riddoch,www.taylorfrancis.com\n",
    "# # book\n",
    "# # 'www.ajtmh.org', \n",
    "# # https://www.ajtmh.org/view/journals/tpmd/60/3/article-p338.xml\n",
    "# # full text not available\n",
    "# # 'literatumonline.com'\n",
    "# # https://nrc-prod.literatumonline.com/doi/10.1139/y94-079\n",
    "# # full text not available\n",
    "# # www.architalbiol.org\n",
    "# # https://www.architalbiol.org/index.php/aib/article/view/122237/\n",
    "# # full text not available \n",
    "# websites_to_remove = [\n",
    "#     'pubmed.ncbi.nlm.nih.gov', 'nan', 'psycnet.apa.org', 'content.iospress.com', 'www.worldscientific.com', 'www.liebertpub.com', \n",
    "#     'opg.optica.org', 'neurologia.com', 'pubs.aip.org', 'journal.psych.ac.cn', 'open.bu.edu', 'wakespace.lib.wfu.edu', \n",
    "#     'symposium.cshlp.org', 'www.jstor.org', 'www.rbojournal.org', 'webview.isho.jp', 'www.degruyter.com', 'www.taylorfrancis.com', \n",
    "#     'www.ajtmh.org', 'literatumonline.com', 'www.architalbiol.org'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# websites = [i for i in websites if i not in websites_to_remove]\n",
    "# print(websites)\n",
    "# # ['www.ncbi.nlm.nih.gov', 'linkinghub.elsevier.com', 'wiley.com', 'link.springer.com', 'journals.physiology.org', \n",
    "# # 'academic.oup.com', 'www.cambridge.org', 'karger.com', 'journals.lww.com', 'www.nature.com', 'www.science.org', \n",
    "# # 'www.tandfonline.com', 'journals.sagepub.com', 'jamanetwork.com', 'neurology.org', 'www.biorxiv.org', \n",
    "# # 'europepmc.org', 'iovs.arvojournals.org', 'royalsocietypublishing.org', 'psychiatryonline.org', 'direct.mit.edu', \n",
    "# # 'www.jstage.jst.go.jp', 'thejns.org', 'www.annualreviews.org', 'aspetjournals.org', 'jnm.snmjournals.org', \n",
    "# # 'www.ahajournals.org', 'pubs.acs.org', 'www.thieme-connect.de', 'pubs.asahq.org', \n",
    "# # 'www.ingentaconnect.com', 'ujms.net', 'journals.biologists.com', 'www.microbiologyresearch.org', \n",
    "# # 'journals.aps.org', 'www.imrpress.com', 'www.researchsquare.com', 'ieeexplore.ieee.org', 'papers.ssrn.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract and filling info to construct litera_db\n",
    "# input_path = fpath.poten_litera_ids_ftl_filled_filtered\n",
    "# output_path = fpath.poten_litera_litera_db\n",
    "\n",
    "# # clear file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# info_filling(input_path, output_path, 0, 10980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # process the resuls: remove duplicates and reset index\n",
    "# input_path = fpath.poten_litera_litera_db\n",
    "# output_path = fpath.poten_litera_litera_db\n",
    "\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "# identifiers = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\"]\n",
    "\n",
    "# for identifier in identifiers:\n",
    "#     remove_dup_by = identifier\n",
    "#     df = df[df[remove_dup_by].isnull() | ~df[df[remove_dup_by].notnull()].duplicated(subset=remove_dup_by, keep='last')]\n",
    "\n",
    "# # reset index\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# df.to_csv(output_path, header=False, index=False)\n",
    "# print(\"Duplication removed.\")\n",
    "\n",
    "# input_path = fpath.poten_litera_litera_db\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# print(df.shape)\n",
    "# # (10776, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # process the db so that no text contains no \",\" and are identified as seperators and if keywords starts with \"nan\", remove it\n",
    "# input_path = fpath.poten_litera_db\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "# for ind in df.index:\n",
    "#     if df.at[ind, \"TITLE\"] == df.at[ind, \"TITLE\"]:\n",
    "#         df.at[ind, \"TITLE\"] = df.at[ind, \"TITLE\"].replace(\",\", \";\").strip()\n",
    "#     if df.at[ind, \"ABSTRACT\"] == df.at[ind, \"ABSTRACT\"]:\n",
    "#         df.at[ind, \"ABSTRACT\"] = df.at[ind, \"ABSTRACT\"].replace(\",\", \";\").strip()\n",
    "#     if df.at[ind, \"KEYWORDS\"] == df.at[ind, \"KEYWORDS\"]:\n",
    "#         k = df.at[ind, \"KEYWORDS\"].replace(\",\", \";\").strip()\n",
    "#         if k.startswith(\"nan\"):\n",
    "#             k = k.split(\"nan;\")[1].strip()\n",
    "#         df.at[ind, \"KEYWORDS\"] = k\n",
    "\n",
    "# df.to_csv(input_path, header=False, index=False)\n",
    "\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# print(df.shape)\n",
    "# print(df.head(5))\n",
    "# # (10776, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 3. Download full text (pdf/json) and extract text </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # list all pdf source\n",
    "# input_path = fpath.poten_litera_litera_db\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "# pdf_source_set = set(df['PDF_SOURCE'].tolist())\n",
    "# print(pdf_source_set)\n",
    "# # {'www.ahajournals.org', 'anatomypubs.onlinelibrary.wiley.com', 'citeseerx.ist.psu.edu', 'www.nature.com', \n",
    "# # 'iovs.arvojournals.org', 'www.microbiologyresearch.org', 'nyaspubs.onlinelibrary.wiley.com', 'ahuman.org', \n",
    "# # 'karger.com', 'www.imrpress.com', 'www.researchsquare.com', 'link.springer.com', 'www.ijpp.com', \n",
    "# # 'europepmc.org', nan, 'www.cell.com', 'www.bu.edu', 'www.ncbi.nlm.nih.gov', 'jamanetwork.com', \n",
    "# # 'www.thieme-connect.de', 'www.science.org', 'physoc.onlinelibrary.wiley.com', 'deepblue.lib.umich.edu', \n",
    "# # 'bpb-us-e1.wpmucdn.com', 'www.researchgate.net', 'ieeexplore.ieee.org', 'zsp.com.pk', 'journals.biologists.com', \n",
    "# # 'journals.aps.org', 'papers.ssrn.com', 'academic.oup.com', 'onlinelibrary.wiley.com', 'www.hifo.uzh.ch', \n",
    "# # 'royalsocietypublishing.org', 'www.biorxiv.org', 'www.ingentaconnect.com', 'ujms.net', 'enpubs.faculty.ucdavis.edu', \n",
    "# # 'ajp.psychiatryonline.org', 'n.neurology.org', 'www.annualreviews.org', 'ruor.uottawa.ca', 'neuro.psychiatryonline.org', \n",
    "# # 'www.jstage.jst.go.jp', 'synapse.koreamed.org', 'journals.physiology.org', 'linkinghub.elsevier.com', \n",
    "# # 'www.tandfonline.com', 'www.jneurosci.org', 'analyticalsciencejournals.onlinelibrary.wiley.com', 'pubs.asahq.org', \n",
    "# # 'thejns.org', 'biomedical-engineering-online.biomedcentral.com', 'journals.sagepub.com', 'direct.mit.edu', \n",
    "# # 'pubs.acs.org', 'pharmrev.aspetjournals.org', 'journals.lww.com', 'jnm.snmjournals.org', 'jpet.aspetjournals.org', \n",
    "# # 'movementdisorders.onlinelibrary.wiley.com'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # list the pdf_source and prepare the test code for downloading the pdfs\n",
    "# pdf_source_set = [\n",
    "#     'www.ahajournals.org', 'wiley.com', 'citeseerx.ist.psu.edu', 'www.nature.com', \n",
    "#     'iovs.arvojournals.org', 'www.microbiologyresearch.org', 'ahuman.org', \n",
    "#     'karger.com', 'www.imrpress.com', 'www.researchsquare.com', 'link.springer.com', 'www.ijpp.com', \n",
    "#     'europepmc.org', 'www.cell.com', 'www.bu.edu', 'www.ncbi.nlm.nih.gov', 'jamanetwork.com', \n",
    "#     'www.thieme-connect.de', 'www.science.org', 'deepblue.lib.umich.edu', \n",
    "#     'bpb-us-e1.wpmucdn.com', 'www.researchgate.net', 'ieeexplore.ieee.org', 'zsp.com.pk', 'journals.biologists.com', \n",
    "#     'journals.aps.org', 'papers.ssrn.com', 'academic.oup.com', 'www.hifo.uzh.ch', \n",
    "#     'royalsocietypublishing.org', 'www.biorxiv.org', 'www.ingentaconnect.com', 'ujms.net', 'enpubs.faculty.ucdavis.edu', \n",
    "#     'psychiatryonline.org', 'n.neurology.org', 'www.annualreviews.org', 'ruor.uottawa.ca', \n",
    "#     'www.jstage.jst.go.jp', 'synapse.koreamed.org', 'journals.physiology.org', 'linkinghub.elsevier.com', \n",
    "#     'www.tandfonline.com', 'www.jneurosci.org', 'pubs.asahq.org', \n",
    "#     'thejns.org', 'biomedcentral.com', 'journals.sagepub.com', 'direct.mit.edu', \n",
    "#     'pubs.acs.org', 'aspetjournals.org', 'journals.lww.com', 'jnm.snmjournals.org'\n",
    "# ]\n",
    "\n",
    "# input_path = fpath.poten_litera_litera_db\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "# for website in pdf_source_set:\n",
    "#     for ind in df.index:\n",
    "#         if df.at[ind, \"PDF_SOURCE\"] != df.at[ind, \"PDF_SOURCE\"]:\n",
    "#             continue\n",
    "#         if website in df.at[ind, \"PDF_SOURCE\"]:\n",
    "#             print(\"# \" + website)\n",
    "#             print(\"\\\"\" + df.at[ind, \"PDF_URL\"] + \"\\\"\")\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_path = fpath.poten_litera_litera_db\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# print(df.shape)\n",
    "# print(df.head(5))\n",
    "# # (10776, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download pdfs and jsons, rename them to build a database\n",
    "# input_path = fpath.poten_litera_db\n",
    "# pdf_folder = fpath.pdf_folder\n",
    "# download_pdf(input_path, pdf_folder, 0, 10776)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # list the articles whose pdfs are not downloaded, and manually download them if possible\n",
    "# input_path = fpath.poten_litera_db\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# df.columns = df_col.db_columns\n",
    "\n",
    "# pdf_folder = fpath.pdf_folder\n",
    "\n",
    "# for ind in df.index:\n",
    "#     index = int(df.at[ind, \"INDEX\"])\n",
    "#     pdf_file_name = str(index) + \".pdf\"\n",
    "#     json_file_name = str(index) + \".json\"\n",
    "#     pdf_path = os.path.join(pdf_folder, pdf_file_name)\n",
    "#     json_path = os.path.join(pdf_folder, json_file_name)\n",
    "\n",
    "#     # if full text is not available/downloaded, print the info\n",
    "#     if (not os.path.exists(json_path)) and (not os.path.exists(pdf_path)):\n",
    "#         print(df.at[ind, \"INDEX\"], df.at[ind, \"DOI\"], df.at[ind, \"PMID\"], df.at[ind, \"PMCID\"])\n",
    "#         print(df.at[ind, \"TITLE\"])\n",
    "#         print(df.at[ind, \"FULL_TEXT_URL\"], df.at[ind, \"FULL_TEXT_SOURCE\"])\n",
    "#         print(df.at[ind, \"PDF_URL\"], df.at[ind, \"PDF_SOURCE\"])\n",
    "#         print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the downloaded pdfs and jsons\n",
    "def test_pdf(pdf_path):       \n",
    "    # opens the file for reading\n",
    "    with open(pdf_path, 'rb') as p:\n",
    "        txt = (p.readlines())\n",
    "\n",
    "    actual_line = len(txt)\n",
    "    \n",
    "    for i, x in enumerate(txt[::-1]):\n",
    "        if b'%%EOF' in x:\n",
    "            actual_line = len(txt)-i\n",
    "            # print(f'EOF found at line position {-i} = actual {actual_line}, with value {x}')\n",
    "            break\n",
    "    \n",
    "    if actual_line != len(txt):\n",
    "        # get the new list terminating correctly\n",
    "        txtx = txt[:actual_line]\n",
    "\n",
    "        # write to new pdf\n",
    "        with open(pdf_path, 'wb') as f:\n",
    "            f.writelines(txtx)\n",
    "        f.close()\n",
    "\n",
    "    fixed_pdf = PyPDF2.PdfReader(pdf_path)\n",
    "\n",
    "    page_max = len(fixed_pdf.pages)\n",
    "\n",
    "    if page_max < 5:\n",
    "        print(page_max)\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "# --------------------start of test code--------------------\n",
    "# input_path = fpath.poten_litera_db\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "# pdf_folder = fpath.pdf_folder\n",
    "\n",
    "# start = 0\n",
    "# end = 10776\n",
    "\n",
    "# for ind in range(start, end):\n",
    "    # index = str(int(df.at[ind, \"INDEX\"]))\n",
    "    # pdf_file_name = str(index) + \".pdf\"\n",
    "    # json_file_name = str(index) + \".json\"\n",
    "    # pdf_path = os.path.join(pdf_folder, pdf_file_name)\n",
    "    # json_path = os.path.join(pdf_folder, json_file_name)\n",
    "\n",
    "    # try:\n",
    "    #     if os.path.exists(json_path):\n",
    "    #         print(ind, index)\n",
    "    #         continue\n",
    "    #     elif os.path.exists(pdf_path):\n",
    "    #         if test_pdf(pdf_path):\n",
    "    #             print(ind, index)\n",
    "    #         else:\n",
    "    #             print(\"\\n\")\n",
    "    #             print(\"PDF LEGNTH < 3\")\n",
    "    #             print(df.at[ind, \"INDEX\"], df.at[ind, \"DOI\"], df.at[ind, \"PMID\"], df.at[ind, \"PMCID\"])\n",
    "    #             print(df.at[ind, \"FULL_TEXT_URL\"], df.at[ind, \"FULL_TEXT_SOURCE\"])\n",
    "    #             print(df.at[ind, \"PDF_URL\"], df.at[ind, \"PDF_SOURCE\"])\n",
    "    #             print(ind, index)\n",
    "    #             print(\"\\n\")\n",
    "    #     else:\n",
    "    #         print(\"\\n\")\n",
    "    #         print(\"PDF NOT AVAILABLE\")\n",
    "    #         print(df.at[ind, \"INDEX\"], df.at[ind, \"DOI\"], df.at[ind, \"PMID\"], df.at[ind, \"PMCID\"])\n",
    "    #         print(df.at[ind, \"FULL_TEXT_URL\"], df.at[ind, \"FULL_TEXT_SOURCE\"])\n",
    "    #         print(df.at[ind, \"PDF_URL\"], df.at[ind, \"PDF_SOURCE\"])\n",
    "    #         print(ind, index)\n",
    "    #         print(\"\\n\")\n",
    "    # except:\n",
    "    #     print(\"\\n\")\n",
    "    #     print(\"PDF Corrupted\")\n",
    "    #     print(df.at[ind, \"INDEX\"], df.at[ind, \"DOI\"], df.at[ind, \"PMID\"], df.at[ind, \"PMCID\"])\n",
    "    #     print(df.at[ind, \"FULL_TEXT_URL\"], df.at[ind, \"FULL_TEXT_SOURCE\"])\n",
    "    #     print(df.at[ind, \"PDF_URL\"], df.at[ind, \"PDF_SOURCE\"])\n",
    "    #     print(ind, index)\n",
    "    #     print(\"\\n\")\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # copy the relevant and not relevant pdfs and jsons from \"pdfs\" to repective folders \"relevant_pdfs\" and \"not_relevant_pdfs\"\n",
    "# test_path = fpath.poten_litera_testing_set_300_read_index_corrected\n",
    "# destination1 = \"/media/hou/DIDIHOU/relevant_pdfs\"\n",
    "# destination2 = \"/media/hou/DIDIHOU/not_relevant_pdfs\"\n",
    "\n",
    "# df_test = pd.read_csv(test_path, header=0, sep=',')\n",
    "# df_test.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANT\"]\n",
    "\n",
    "# for ind in df_test.index:\n",
    "#     index = df_test.at[ind, \"INDEX\"]\n",
    "#     if df_test.at[ind, \"RELEVANT\"] == \"YES\": # relevant\n",
    "#         flag = False\n",
    "#         print(ind, index)\n",
    "        \n",
    "#         json_path = os.path.join(fpath.pdf_folder, str(index) + \".json\")\n",
    "#         pdf_path = os.path.join(fpath.pdf_folder, str(index) + \".pdf\")\n",
    "        \n",
    "#         if os.path.exist(json_path):\n",
    "#             shutil.copy(json_path, destination1)\n",
    "#             flag = True\n",
    "#         if os.path.exist(pdf_path):\n",
    "#             shutil.copy(pdf_path, destination1)\n",
    "#             flag = True\n",
    "        \n",
    "#         if not flag:\n",
    "#             print(\"No file found for index: \", index)\n",
    "#     else: # not relevant\n",
    "#         json_path = os.path.join(fpath.pdf_folder, str(index) + \".json\")\n",
    "#         pdf_path = os.path.join(fpath.pdf_folder, str(index) + \".pdf\")\n",
    "\n",
    "#         if os.path.exist(json_path):\n",
    "#             shutil.copy(json_path, destination2)\n",
    "#         if os.path.exist(pdf_path):\n",
    "#             shutil.copy(pdf_path, destination2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract text to store to a text file, record the articles whose pdfs or jsons are not available\n",
    "# input_path = fpath.poten_litera_db\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# df.columns = df_col.db_columns\n",
    "\n",
    "# # poten_litera_pdf_not_available = fpath.poten_litera_pdf_not_available\n",
    "# # plib.clear_file(poten_litera_pdf_not_available)\n",
    "\n",
    "# pdf_folder = fpath.pdf_folder\n",
    "# text_folder = fpath.text_folder\n",
    "\n",
    "# for ind in df.index:\n",
    "#     time.sleep(1)\n",
    "#     index = int(df.at[ind, \"INDEX\"])\n",
    "#     pdf_file_name = str(index) + \".pdf\"\n",
    "#     json_file_name = str(index) + \".json\"\n",
    "#     pdf_path = os.path.join(pdf_folder, pdf_file_name)\n",
    "#     json_path = os.path.join(pdf_folder, json_file_name)\n",
    "#     text_path = os.path.join(text_folder, str(index) + \".txt\")\n",
    "\n",
    "#     if os.path.exists(json_path):\n",
    "#         # pass\n",
    "#         json2text(json_path, text_path)\n",
    "#     elif os.path.exists(pdf_path):\n",
    "#         # pass\n",
    "#         pdf2text(pdf_path, text_path)\n",
    "#     else:\n",
    "#         pass\n",
    "#         # selected_row = df.iloc[[ind]]\n",
    "#         # selected_row.to_csv(poten_litera_pdf_not_available, mode='a', header=False, index=False)\n",
    "#         # print(df.at[ind, \"INDEX\"], df.at[ind, \"DOI\"], df.at[ind, \"PMID\"], df.at[ind, \"PMCID\"])\n",
    "#         # print(df.at[ind, \"TITLE\"])\n",
    "#         # print(df.at[ind, \"FULL_TEXT_URL\"], df.at[ind, \"FULL_TEXT_SOURCE\"])\n",
    "#         # print(df.at[ind, \"PDF_URL\"], df.at[ind, \"PDF_SOURCE\"])\n",
    "#         # print(\"\\n\")\n",
    "\n",
    "#     print(ind, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text of length params.text_length_to_extract from the text files and store to a text file\n",
    "input_path = fpath.poten_litera_db\n",
    "df = pd.read_csv(input_path, header=None, sep=',')\n",
    "df.columns = df_col.db_columns\n",
    "\n",
    "pdf_folder = fpath.pdf_folder\n",
    "text_folder = fpath.text_folder\n",
    "processed_text_folder = fpath.processed_texts_of_length_500_folder\n",
    "not_recog_articles_folder = fpath.not_recog_articles_folder\n",
    "\n",
    "for ind in df.index:\n",
    "    # time.sleep(1)\n",
    "    index = int(df.at[ind, \"INDEX\"])\n",
    "    pdf_path = os.path.join(pdf_folder, str(index) + \".pdf\")\n",
    "    json_path = os.path.join(pdf_folder, str(index) + \".json\")\n",
    "    text_path = os.path.join(text_folder, str(index) + \".txt\")\n",
    "    text_processed_path = os.path.join(processed_text_folder, str(index) + \".txt\")\n",
    "\n",
    "    if os.path.exists(text_path):\n",
    "        # first we test if the length of the text is greater than the length to extract\n",
    "        with open(text_path, 'r', encoding=\"ascii\") as f:\n",
    "            text = f.read()\n",
    "        f.close()\n",
    "        \n",
    "        text_split = text.split()\n",
    "        \n",
    "        if len(text_split) > params.text_length_to_extract:\n",
    "            pass\n",
    "            # text_500 = ' '.join(word for word in text_split[:params.text_length_to_extract])\n",
    "        else: # if the length of the text is less than the length to extract, we extract the text from the pdf or json once again\n",
    "            if os.path.exists(pdf_path):\n",
    "                pdf2text(pdf_path, text_path)\n",
    "                # read from the text file again and check if the length is less than the length to extract\n",
    "                with open(text_path, 'r', encoding=\"ascii\") as f:\n",
    "                    text = f.read()\n",
    "                f.close()\n",
    "                text_split = text.split()\n",
    "                if len(text_split) > params.text_length_to_extract:\n",
    "                    pass\n",
    "                elif os.path.exists(json_path):\n",
    "                    json2text(json_path, text_path)\n",
    "                else:\n",
    "                    pass\n",
    "            elif os.path.exists(json_path):\n",
    "                json2text(json_path, text_path)\n",
    "            else:\n",
    "                raise Exception(\"No pdf or json file found for index: \", index)\n",
    "\n",
    "        # write the text to the processed text file\n",
    "        with open(text_path, 'r', encoding=\"ascii\") as f:\n",
    "            text = f.read()\n",
    "        f.close()\n",
    "        \n",
    "        text_split = text.split()\n",
    "        \n",
    "        if len(text_split) > params.text_length_to_extract:\n",
    "            text_500 = ' '.join(word for word in text_split[:params.text_length_to_extract])\n",
    "        else:\n",
    "            text_500 = text\n",
    "\n",
    "            # copy this file to a folder for manual check\n",
    "            shutil.copy(pdf_path, not_recog_articles_folder)\n",
    "            shutil.copy(json_path, not_recog_articles_folder)\n",
    "\n",
    "            print(df.at[ind, \"INDEX\"], df.at[ind, \"DOI\"], df.at[ind, \"PMID\"], df.at[ind, \"PMCID\"])\n",
    "            print(df.at[ind, \"TITLE\"])\n",
    "            print(df.at[ind, \"FULL_TEXT_URL\"], df.at[ind, \"FULL_TEXT_SOURCE\"])\n",
    "            print(df.at[ind, \"PDF_URL\"], df.at[ind, \"PDF_SOURCE\"])\n",
    "            print(\"\\n\")\n",
    "        \n",
    "        with open(text_processed_path, 'w', encoding=\"ascii\") as f:\n",
    "            f.write(text_500)\n",
    "        f.close()\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # print(ind, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of literatures whose pdfs or jsons are not available\n",
    "input_path = fpath.poten_litera_pdf_not_available\n",
    "df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# print(df.shape)\n",
    "# (406, 11)\n",
    "# print number of rows\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4> 4. Training and testing data set split </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # select 300 random papers from poten_litera_ids_ftl_filled_filtered for testing\n",
    "# source_path = fpath.poten_litera_ids_ftl_filled_filtered\n",
    "# output_path = fpath.poten_litera_testing_set_300\n",
    "\n",
    "# # clear the file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "# df = df.sample(n=300, random_state=1, axis='index', ignore_index=False)\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df.to_csv(output_path, header=True, index=False)\n",
    "# # --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_testing_set_300\n",
    "# df = pd.read_csv(source_path, header=0, sep=',')\n",
    "# print(df.shape)\n",
    "# # (300, 12)\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # process, correct the INDEX in potential_related_literature_testing_set_300_read.csv\n",
    "# input_path = fpath.poten_litera_testing_set_300_read\n",
    "# output_path = fpath.poten_litera_testing_set_300_read_index_corrected\n",
    "# # plib.clear_file(output_path)\n",
    "# db_path = fpath.poten_litera_db\n",
    "\n",
    "# df_input = pd.read_csv(input_path, header=0, sep=',')\n",
    "# df_input.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANCE\"]\n",
    "# df_db = pd.read_csv(db_path, header=None, sep=',')\n",
    "# df_db.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "# df_db = df_db.fillna(0)\n",
    "# df_db = df_db.astype({\"PMID\": int})\n",
    "# # print(df_input.shape)\n",
    "# # print(df_input.head(5))\n",
    "# # (300, 12)\n",
    "# # print(df_db.shape)\n",
    "# # print(df_db.head(10))\n",
    "# # (10776, 11)\n",
    "\n",
    "# for ind in df_input.index:\n",
    "#     index = df_input.at[ind, \"INDEX\"]\n",
    "#     doi = df_input.at[ind, \"DOI\"]\n",
    "#     pmid = df_input.at[ind, \"PMID\"]\n",
    "#     # print(pmid, df_db.at[ind, \"PMID\"])\n",
    "#     # print(pmid.type(), df_db.at[ind, \"PMID\"].type())\n",
    "#     pmcid = df_input.at[ind, \"PMCID\"]\n",
    "#     full_text_url = df_input.at[ind, \"FULL_TEXT_URL\"]\n",
    "#     full_text_source = df_input.at[ind, \"FULL_TEXT_SOURCE\"]\n",
    "#     title = df_input.at[ind, \"TITLE\"].lower()\n",
    "\n",
    "#     if doi == doi:\n",
    "#         try:\n",
    "#             index = df_db.loc[df_db[\"DOI\"] == doi, 'INDEX'].values[0]\n",
    "#             df_input.at[ind, \"INDEX\"] = index\n",
    "#         except:\n",
    "#             print(\"DOI not found in db:\", df_input.at[ind, \"INDEX\"], df_input.at[ind, \"RELEVANCE\"])\n",
    "#             df_input.drop(ind, inplace=True)\n",
    "#     elif pmid == pmid:\n",
    "#         try:\n",
    "#             index = df_db.loc[int(df_db[\"PMID\"])==int(pmid), 'INDEX'].values[0]\n",
    "#             df_input.at[ind, \"INDEX\"] = index\n",
    "#         except:\n",
    "#             print(\"PMID not found in db:\", df_input.at[ind, \"INDEX\"], df_input.at[ind, \"RELEVANCE\"])\n",
    "#             df_input.drop(ind, inplace=True)\n",
    "#     elif pmcid == pmcid:\n",
    "#         index = df_db.loc[df_db[\"PMCID\"] == pmcid, 'INDEX'].values[0]\n",
    "#         df_input.at[ind, \"INDEX\"] = index\n",
    "#     elif title.lower() == title.lower():\n",
    "#         index = df_db.loc[df_db[\"TITLE\"].str.lower() == title, 'INDEX'].values[0]\n",
    "#         df_input.at[ind, \"INDEX\"] = index\n",
    "#     else:\n",
    "#         print(\"ALL 4 identifiers and title are missing:\", df_input.at[ind, \"INDEX\"], df_input.at[ind, \"RELEVANCE\"])\n",
    "    \n",
    "# df_input.reset_index(drop=True, inplace=True)\n",
    "# df_input.to_csv(output_path, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check the corrected file\n",
    "# corrected = fpath.poten_litera_testing_set_300_read_index_corrected\n",
    "# df_input = pd.read_csv(corrected, header=0, sep=',')\n",
    "# print(df_input.shape)\n",
    "# # (292, 12)\n",
    "# print(df_input.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test if the result matches the db\n",
    "# result_path = fpath.poten_litera_testing_set_300_read_index_corrected\n",
    "# db_path = fpath.poten_litera_db\n",
    "\n",
    "# df_result= pd.read_csv(result_path, header=0, sep=',')\n",
    "# df_result.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANCE\"]\n",
    "# df_db = pd.read_csv(db_path, header=None, sep=',')\n",
    "# df_db.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "# # print(df_result.shape)\n",
    "# # print(df_result.head(5))\n",
    "# # (292, 12)\n",
    "# # print(df_db.shape)\n",
    "# # print(df_db.head(10))\n",
    "# # (10776, 11)\n",
    "\n",
    "# for ind in df_result.index:\n",
    "#     index = int(df_result.at[ind, \"INDEX\"])\n",
    "#     title = df_result.at[ind, \"TITLE\"].lower()\n",
    "#     # title = ''.join([char for char in df_result.at[ind, \"TITLE\"].lower() if re.match(r'[a-z\\s-]', char)])\n",
    "#     cleaned_title = re.sub(r'\\s+', ' ', title).strip().replace(\".\", \"\")\n",
    "#     title_db = df_db.loc[df_db[\"INDEX\"].astype(int) == index, 'TITLE'].values[0].lower()\n",
    "#     # title_db = ''.join([char for char in df_db.loc[df_db[\"INDEX\"].astype(int) == index, 'TITLE'].values[0].lower() if re.match(r'[a-z\\s-]', char)])\n",
    "#     cleaned_title_db = re.sub(r'\\s+', ' ', title_db).strip().replace(\".\", \"\")\n",
    "    \n",
    "#     if cleaned_title == cleaned_title_db:\n",
    "#         pass\n",
    "#     else:\n",
    "#         # pass\n",
    "#         print(index)\n",
    "#         print(cleaned_title)\n",
    "#         print(cleaned_title_db)\n",
    "#         print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # select another 708 random papers from poten_litera_db to form 1000 papers as training and testing set\n",
    "# db_path = fpath.poten_litera_db\n",
    "# test_300_path = fpath.poten_litera_testing_set_300_read_index_corrected\n",
    "# test_708_path = fpath.poten_litera_testing_set_708\n",
    "# plib.clear_file(test_708_path)\n",
    "\n",
    "# df_db = pd.read_csv(db_path, header=None, sep=',')\n",
    "# df_db.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "# df_300 = pd.read_csv(test_300_path, header=0, sep=',')\n",
    "# df_300.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANCE\"]\n",
    "\n",
    "# # Get the indices of the previously selected 300 rows\n",
    "# selected_indices = df_300['INDEX'].values\n",
    "# # print(selected_indices)\n",
    "\n",
    "# # Drop the previously selected 300 rows from the original dataframe\n",
    "# df_remaining = df_db[~df_db['INDEX'].isin(selected_indices)]\n",
    "\n",
    "# # Randomly sample 708 rows from the remaining rows\n",
    "# df_708 = df_remaining.sample(n=708, random_state=42)  # Change random_state if needed\n",
    "# df_708['RELEVANCE'] = np.nan\n",
    "# df_708.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# df_708.to_csv(test_708_path, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine and obtain 1000 papers as training-test-set\n",
    "# test_300_path = fpath.poten_litera_testing_set_300_read_index_corrected\n",
    "# test_708_path = fpath.poten_litera_testing_set_708\n",
    "# test_1000_path = fpath.poten_litera_testing_set_1000\n",
    "# plib.clear_file(test_1000_path)\n",
    "\n",
    "# df_300 = pd.read_csv(test_300_path, header=0, sep=',')\n",
    "# df_708 = pd.read_csv(test_708_path, header=0, sep=',')\n",
    "# df_1000 = pd.concat([df_300, df_708], axis=0)\n",
    "# df_1000.reset_index(drop=True, inplace=True)\n",
    "# df_1000.to_csv(test_1000_path, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if there're duplicates in the 1000 papers\n",
    "# test_1000_path = fpath.poten_litera_testing_set_1000\n",
    "# df_1000 = pd.read_csv(test_1000_path, header=0, sep=',')\n",
    "# df_1000.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANCE\"]\n",
    "# print(df_1000.shape)\n",
    "# # (1000, 12)\n",
    "\n",
    "# print(len(set(df_1000['INDEX'])))\n",
    "# # 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test if poten_litera_testing_set_1000 matches poten_litera_db\n",
    "# test_1000_path = fpath.poten_litera_testing_set_1000\n",
    "# db_path = fpath.poten_litera_db\n",
    "\n",
    "# df_result= pd.read_csv(test_1000_path, header=0, sep=',')\n",
    "# df_result.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANCE\"]\n",
    "# df_db = pd.read_csv(db_path, header=None, sep=',')\n",
    "# df_db.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "# # print(df_result.shape)\n",
    "# # print(df_result.head(5))\n",
    "# # # (1000, 12)\n",
    "# # print(df_db.shape)\n",
    "# # print(df_db.head(10))\n",
    "# # # (10776, 11)\n",
    "\n",
    "# for ind in df_result.index:\n",
    "#     index = int(df_result.at[ind, \"INDEX\"])\n",
    "#     title = df_result.at[ind, \"TITLE\"].lower()\n",
    "#     # title = ''.join([char for char in df_result.at[ind, \"TITLE\"].lower() if re.match(r'[a-z\\s-]', char)])\n",
    "#     cleaned_title = re.sub(r'\\s+', ' ', title).strip().replace(\".\", \"\")\n",
    "#     title_db = df_db.loc[df_db[\"INDEX\"].astype(int) == index, 'TITLE'].values[0].lower()\n",
    "#     # title_db = ''.join([char for char in df_db.loc[df_db[\"INDEX\"].astype(int) == index, 'TITLE'].values[0].lower() if re.match(r'[a-z\\s-]', char)])\n",
    "#     cleaned_title_db = re.sub(r'\\s+', ' ', title_db).strip().replace(\".\", \"\")\n",
    "    \n",
    "#     if cleaned_title == cleaned_title_db:\n",
    "#         pass\n",
    "#     else:\n",
    "#         # pass\n",
    "#         print(index)\n",
    "#         print(cleaned_title)\n",
    "#         print(cleaned_title_db)\n",
    "#         print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # draw the distribution of the sampled 1000 papers to see if it's actually randomly sampled\n",
    "# test_1000_path = fpath.poten_litera_testing_set_1000\n",
    "# df_1000 = pd.read_csv(test_1000_path, header=0, sep=',')\n",
    "# df_1000.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANCE\"]\n",
    "\n",
    "# index_list = df_1000['INDEX'].tolist()\n",
    "# index_list.sort()\n",
    "# # print(index_list)\n",
    "# print(len(index_list))\n",
    "\n",
    "# # draw the histogram\n",
    "# plt.hist(index_list, bins=10)\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.title(\"Distribution of the sampled 1000 papers\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Next step: automatic filtering </h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
