{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Searched literature data preprocessing </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pypdfium2 as pdfium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib\n",
    "import extract_info\n",
    "import parameters as params\n",
    "import download_and_process_pdf as dpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Parameters: </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns of potential_related_literature_combined.csv\n",
    "columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Predefined fucntions: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pubmed(source_path, output_path, start, end):\n",
    "    print(\"Starting preprocessing search results from PubMed...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, sep=',')\n",
    "    df = df[[\"DOI\", \"PMID\", \"PMCID\", \"Title\"]]\n",
    "    \n",
    "    for ind in range(start, end):\n",
    "        # sleep to avoid to be blocked\n",
    "        time.sleep(random.randint(1, 3))\n",
    "        \n",
    "        # request the webpage\n",
    "        # the columns PMID, Title don't contain np.nan\n",
    "        pmid = str(df[\"PMID\"][ind]).strip()\n",
    "        url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "        soup = plib.request_webpage(url)\n",
    "        # print(soup)\n",
    "        \n",
    "        # get pmcid\n",
    "        if df[\"PMCID\"][ind] != df[\"PMCID\"][ind]: # PMCID is np.nan\n",
    "            try:\n",
    "                pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                pmcid = np.nan\n",
    "        else: # PMCID is not np.nan\n",
    "            pmcid = str(df[\"PMCID\"][ind]).strip()\n",
    "        # print(pmcid)\n",
    "\n",
    "        # get doi\n",
    "        if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # DOI is np.nan\n",
    "            try:\n",
    "                doi = soup.find_all(\"span\", {\"class\": \"identifier doi\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                doi  = np.nan\n",
    "        else: # DOI is not np.nan\n",
    "            doi = str(df[\"DOI\"][ind]).strip()\n",
    "        # print(doi)\n",
    "\n",
    "        full_text_url = np.nan\n",
    "        pdf_url = np.nan\n",
    "        title = (df.at[ind, \"Title\"]).strip()\n",
    "        abstract = np.nan\n",
    "        keywords = np.nan\n",
    "        \n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"Title\": [title],\n",
    "            \"Abstract\": [abstract],\n",
    "            \"Keywords\": [keywords]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_pubmed\n",
    "# output_path = fpath.poten_litera_pubmed_processed\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, sep=',')\n",
    "# print(df.shape)\n",
    "# # (2612, 11)\n",
    "# df = df[[\"DOI\", \"PMID\", \"PMCID\", \"Title\"]]\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "\n",
    "# print(df[\"DOI\"].isnull().values.any())\n",
    "# print(df[\"PMID\"].isnull().values.any())\n",
    "# print(df[\"PMCID\"].isnull().values.any())\n",
    "# print(df[\"Title\"].isnull().values.any())\n",
    "# # True, False, True, Flase\n",
    "# # PMID, Title don't contain np.nan\n",
    "# # DOI, PMCID contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_pubmed(source_path, output_path, start, end)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_webofscience(source_path, output_path, start, end):\n",
    "    print(\"Starting preprocessing search results from Web of Science...\")\n",
    "    \n",
    "    df = pd.read_csv(source_path, sep=\",\")\n",
    "    df = df[[\"DOI\", \"Pubmed Id\", \"Article Title\", \"Abstract\", \"Author Keywords\", \"Keywords Plus\"]]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # sleep to avoid to be blocked\n",
    "        time.sleep(random.randint(1, 3))\n",
    "        \n",
    "        # the columns Article Title don't contain np.nan\n",
    "        # the columns DOI and PMID contain np.nan\n",
    "\n",
    "        # get pmid, doi\n",
    "        if df[\"Pubmed Id\"][ind] != df[\"Pubmed Id\"][ind]: # Pubmed Id is np.nan\n",
    "            if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # DOI is np.nan\n",
    "                doi = np.nan\n",
    "                pmid = np.nan\n",
    "            else: # DOI is not np.nan\n",
    "                doi = str(df[\"DOI\"][ind]).strip()\n",
    "                pmid = plib.doi2pmid(doi)\n",
    "        else: # Pubmed Id is not np.nan\n",
    "            pmid = str(int(df[\"Pubmed Id\"][ind])).strip()\n",
    "            if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # DOI is not np.nan\n",
    "                doi, a = plib.pmid2doi_pmcid(pmid)\n",
    "            else: # DOI is not np.nan\n",
    "                doi = str(df[\"DOI\"][ind]).strip()\n",
    "        \n",
    "        # get pmcid\n",
    "        if pmid != pmid: # pmid is np.nan\n",
    "            pmcid = np.nan\n",
    "        else: # pmid is not np.nan\n",
    "            # request the webpage\n",
    "            url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "            soup = plib.request_webpage(url)\n",
    "            # print(soup)\n",
    "\n",
    "            # get pmcid\n",
    "            try:\n",
    "                pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                pmcid = np.nan\n",
    "            # print(pmcid)\n",
    "            \n",
    "        full_text_url = np.nan\n",
    "        pdf_url = np.nan\n",
    "        title = str(df[\"Article Title\"][ind]).strip()\n",
    "        abstract = str(df[\"Abstract\"][ind]).strip()\n",
    "        keywords = str(df[\"Author Keywords\"][ind]).strip() + \"; \" + str(df[\"Keywords Plus\"][ind]).strip()\n",
    "\n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"Title\": [title],\n",
    "            \"Abstract\": [abstract],\n",
    "            \"Keywords\": [keywords]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# # source_path = fpath.poten_litera_wos\n",
    "# # output_path = fpath.poten_litera_wos_processed\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, sep=';')\n",
    "# df = df[[\"DOI\", \"Pubmed Id\", \"Article Title\", \"Abstract\", \"Author Keywords\", \"Keywords Plus\"]]\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "\n",
    "# print(df[\"DOI\"].isnull().values.any())\n",
    "# print(df[\"Pubmed Id\"].isnull().values.any())\n",
    "# print(df[\"Article Title\"].isnull().values.any())\n",
    "# print(df[\"Abstract\"].isnull().values.any())\n",
    "# print(df[\"Author Keywords\"].isnull().values.any())\n",
    "# print(df[\"Keywords Plus\"].isnull().values.any())\n",
    "# # True, True, False\n",
    "# # Article Title don't contain np.nan\n",
    "# # DOI, Pubmed Id contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code--------------------- \n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_webofscience(source_path, output_path, 0, 10)\n",
    "# ---------------------end of test code--------------------- \n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=';')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_eupmc(source_path, output_path, start, end):\n",
    "    print(\"Starting preprocessing search results from Europe PMC...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, sep=\",\")\n",
    "    df = df[[\"SOURCE\", \"DOI\", \"EXTERNAL_ID\", \"PMCID\", \"TITLE\"]]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # sleep to avoid to be blocked\n",
    "        time.sleep(random.randint(1, 3))\n",
    "\n",
    "        # get pmid, doi\n",
    "        # SOURCE = {'PMC', 'MED', 'ETH', 'PPR'}\n",
    "        if df[\"SOURCE\"][ind] != \"MED\": # SOURCE is not \"MED\" \n",
    "            if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # doi is np.nan\n",
    "                doi = np.nan\n",
    "                pmid = np.nan\n",
    "            else:\n",
    "                doi = str(df[\"DOI\"][ind]).strip()\n",
    "                pmid = plib.doi2pmid(doi)\n",
    "        else: # SOURCE is \"MED\"\n",
    "            # get doi, pmid\n",
    "            if df[\"EXTERNAL_ID\"][ind] != df[\"EXTERNAL_ID\"][ind]: # EXTERNAL_ID is np.nan\n",
    "                if df[\"DOI\"][ind] != df[\"DOI\"][ind](): # DOI is np.nan\n",
    "                    doi = np.nan\n",
    "                    pmid = np.nan\n",
    "                else: # DOI is not np.nan\n",
    "                    doi = str(df[\"DOI\"][ind]).strip()\n",
    "                    pmid = plib.doi2pmid(doi)\n",
    "            else: # EXTERNAL_ID is not np.nan\n",
    "                pmid = str(df[\"EXTERNAL_ID\"][ind]).strip()\n",
    "                if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # DOI is np.nan\n",
    "                    doi, a = plib.pmid2doi_pmcid(pmid)\n",
    "                else: # DOI is not np.nan\n",
    "                    doi = str(df[\"DOI\"][ind]).strip()\n",
    "                \n",
    "        # get pmcid\n",
    "        if pmid != pmid: # pmid is np.nan\n",
    "            pmcid = df[\"PMCID\"][ind]\n",
    "        else: # pmid is not np.nan\n",
    "            # request the webpage\n",
    "            url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "            soup = plib.request_webpage(url)\n",
    "            # print(soup)\n",
    "\n",
    "            # get pmcid\n",
    "            try:\n",
    "                pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                pmcid = np.nan\n",
    "            # print(pmcid)\n",
    "        \n",
    "        full_text_url = np.nan\n",
    "        pdf_url = np.nan\n",
    "        title = (df.at[ind, \"TITLE\"]).strip()\n",
    "        abstract = np.nan\n",
    "        keywords = np.nan\n",
    "        \n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"Title\": [title],\n",
    "            \"Abstract\": [abstract],\n",
    "            \"Keywords\": [keywords]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_eupmc\n",
    "# output_path = fpath.poten_litera_eupmc_processed\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, sep=',')\n",
    "# df = df[[\"SOURCE\", \"DOI\", \"EXTERNAL_ID\", \"PMCID\", \"TITLE\"]]\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "\n",
    "# col_one_list = set(df['SOURCE'].tolist())\n",
    "# print(col_one_list)\n",
    "# # ['PMC', 'MED', 'ETH', 'PPR']\n",
    "\n",
    "# print(df[\"SOURCE\"].isnull().values.any())\n",
    "# print(df[\"DOI\"].isnull().values.any())\n",
    "# print(df[\"EXTERNAL_ID\"].isnull().values.any())\n",
    "# print(df[\"PMCID\"].isnull().values.any())\n",
    "# print(df[\"TITLE\"].isnull().values.any())\n",
    "# # False, True, False, True, False\n",
    "# # SOURCE, EXTERNAL_ID, Title don't contain np.nan\n",
    "# # DOI, PMCID contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_eupmc(source_path, output_path, 0, 10)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_google_shcolar_step1(source_path, output_path, start, end):\n",
    "    print(\"Starting merging search results from Google Scholar...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, header=None, sep=',')\n",
    "    df.columns = [\"title\", \"url\", \"url_tag\", \"full_text_url\", \"full_text_tag\"]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # df[\"url_tag\"]: {'[CITATION][C]', '[HTML][HTML]', '[PDF][PDF]', '[BOOK][B]', nan}\n",
    "        # we don't need books, as they are not likely to include connecivity information\n",
    "        if df.at[ind, \"url_tag\"] == \"[BOOK][B]\":\n",
    "            continue\n",
    "        \n",
    "        # if url or title doesn't exsit AND full_text_url exists\n",
    "        if (df.at[ind, \"url\"] != df.at[ind, \"url\"] or df.at[ind, \"title\"] != df.at[ind, \"title\"]) and (df.at[ind, \"full_text_tag\"] == \"[PDF]\" or df.at[ind, \"full_text_tag\"] == \"[HTML]\"):\n",
    "            raise Exception(ind, \": url or title are both nan, but full_text_tag is [PDF] or [HTML]!\")\n",
    "\n",
    "        # if url or title doesn't exsit AND full_text_url doesn't exist\n",
    "        if (df.at[ind, \"url\"] != df.at[ind, \"url\"] or df.at[ind, \"title\"] != df.at[ind, \"title\"]):\n",
    "            continue \n",
    "        \n",
    "        title = str(df[\"title\"][ind]).strip()\n",
    "\n",
    "        # now every row has at least title and url\n",
    "        if df[\"url_tag\"][ind] == \"[PDF][PDF]\": # {'[CITATION][C]', '[HTML][HTML]', '[PDF][PDF]', nan}\n",
    "            if df[\"full_text_tag\"][ind] == \"[HTML]\": # {'[PDF]', '[HTML]', nan}\n",
    "                link = str(df[\"full_text_url\"][ind]).strip()\n",
    "                full_text_url, status_code  = plib.get_final_redirected_url(link)\n",
    "                if full_text_url == full_text_url:\n",
    "                    full_text_source = full_text_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                else:\n",
    "                    full_text_source = np.nan\n",
    "            else:\n",
    "                full_text_url = np.nan\n",
    "                full_text_source = np.nan\n",
    "            # get pdf_url, pdf_source\n",
    "            link = str(df[\"url\"][ind]).strip()\n",
    "            pdf_url, status_code = plib.get_final_redirected_url(link)\n",
    "        else: # {'[CITATION][C]', '[HTML][HTML]', nan}\n",
    "            link = str(df[\"url\"][ind]).strip()\n",
    "            full_text_url, status_code = plib.get_final_redirected_url(link)\n",
    "            if full_text_url == full_text_url:\n",
    "                full_text_source = full_text_url.split(\"://\")[1].split(\"/\")[0]\n",
    "            else:\n",
    "                full_text_source = np.nan\n",
    "            # get pdf_url, pdf_source\n",
    "            if df[\"full_text_tag\"][ind] == \"[PDF]\": # full_text_type = {'[HTML]', nan, '[PDF]'}\n",
    "                link = str(df[\"full_text_url\"][ind]).strip()\n",
    "                pdf_url, status_code  = plib.get_final_redirected_url(link)\n",
    "            else:\n",
    "                pdf_url = np.nan\n",
    "        \n",
    "        columns = [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\"]\n",
    "        row = {\n",
    "            \"Title\": [title],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"full_text_source\": [full_text_source],\n",
    "            \"pdf_url\": [pdf_url]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_gs\n",
    "# output_path = fpath.poten_litera_gs_processed_step1\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"title\", \"url\", \"url_tag\", \"full_text_url\", \"full_text_tag\"]\n",
    "# # print(df.head(3))\n",
    "# print(df.shape)\n",
    "# # (980, 5)\n",
    "\n",
    "# url_type = set(df['url_tag'].tolist())\n",
    "# print(url_type)\n",
    "# # {'[CITATION][C]', '[HTML][HTML]', '[PDF][PDF]', '[BOOK][B]', nan}\n",
    "# full_text_tag = set(df['full_text_tag'].tolist())\n",
    "# print(full_text_tag)\n",
    "# # {'[PDF]', '[HTML]', nan}\n",
    "# # ---------------------end of test code---------------------\n",
    "\n",
    "# # --------------------start of test code--------------------\n",
    "# # [\"title\", \"url\", \"url_tag\", \"full_text_url\", \"full_text_tag\"]\n",
    "# print(df[\"title\"].isnull().any().any())\n",
    "# print(df[\"url\"].isnull().any().any())\n",
    "# print(df[\"url_tag\"].isnull().any().any())\n",
    "# print(df[\"full_text_url\"].isnull().any().any())\n",
    "# print(df[\"full_text_tag\"].isnull().any().any())\n",
    "# # True, True, True, True, True\n",
    "# # title, url, url_tag, full_text_url, full_text_tag, all contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_google_shcolar_step1(source_path, output_path, 0, 1000)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_google_shcolar_step2(source_path, output_path, start, end):\n",
    "    print(\"Starting merging search results from Google Scholar...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, header=None, sep=',')\n",
    "    df.columns = [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\"]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # get doi from url\n",
    "        if df[\"full_text_url\"][ind] == df[\"full_text_url\"][ind]: # there's a full_text_url\n",
    "            url = str(df[\"full_text_url\"][ind]).strip()\n",
    "            source = url.split(\"://\")[1].split(\"/\")[0]\n",
    "            # check if the full_text_url is one of our websites\n",
    "            flag = False\n",
    "            for website in params.websites_gs:\n",
    "                if website in source:\n",
    "                    flag = True\n",
    "                    break\n",
    "            if not flag:\n",
    "                continue\n",
    "            info = extract_info.extract_info_from_webpage(url, params.websites_gs)\n",
    "            doi = info[\"doi\"]\n",
    "            pmid = info[\"pmid\"]\n",
    "            pmcid = info[\"pmcid\"]\n",
    "        else:\n",
    "            url = np.nan\n",
    "            doi = np.nan\n",
    "            pmid = np.nan\n",
    "            pmcid = np.nan\n",
    "        \n",
    "        full_text_url = url\n",
    "        pdf_url = df.at[ind, \"pdf_url\"]\n",
    "        title = df.at[ind, \"Title\"]\n",
    "        abstract = np.nan\n",
    "        keywords = np.nan\n",
    "\n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"Title\": [title],\n",
    "            \"Abstract\": [abstract],\n",
    "            \"Keywords\": [keywords]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(doi)\n",
    "        if doi != doi:\n",
    "            print([df[\"full_text_url\"][ind]])\n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_gs_processed_step1\n",
    "# output_path = fpath.poten_litera_gs_processed_step2\n",
    "# plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\"]\n",
    "# # print(df.head(3))\n",
    "# print(df.shape)\n",
    "# # (926, 4)\n",
    "# full_text_source = set(df['full_text_source'].tolist())\n",
    "# print(full_text_source)\n",
    "# # {'www.elibrary.ru', 'n.neurology.org', 'jnnp.bmj.com', 'anatomypubs.onlinelibrary.wiley.com', 'academic.oup.com', \n",
    "# #  'nyaspubs.onlinelibrary.wiley.com', 'cir.nii.ac.jp', 'link.springer.com', 'www.mdpi.com', 'pure.mpg.de', \n",
    "# #  'bmcneurosci.biomedcentral.com', 'elibrary.ru', 'journals.sagepub.com', 'tbiomed.biomedcentral.com', \n",
    "# #  'onlinelibrary.wiley.com', 'www.cambridge.org', 'wakespace.lib.wfu.edu', nan, 'www.cell.com', 'europepmc.org', \n",
    "# #  'var.scholarpedia.org', 'jpet.aspetjournals.org', 'journal.psych.ac.cn', 'www.biorxiv.org', 'ieeexplore.ieee.org', \n",
    "# #  'www.jstor.org', 'www.cabdirect.org', 'royalsocietypublishing.org', 'analyticalsciencejournals.onlinelibrary.wiley.com', \n",
    "# #  'open.bu.edu', 'journals.lww.com', 'www.eneuro.org', 'www.jstage.jst.go.jp', 'journals.plos.org', 'www.ncbi.nlm.nih.gov', \n",
    "# #  'www.liebertpub.com', 'neuro.psychiatryonline.org', 'www.sciencedirect.com', 'psycnet.apa.org', 'www.taylorfrancis.com', \n",
    "# #  'www.degruyter.com', 'www.nature.com', 'jamanetwork.com', 'karger.com', 'www.tandfonline.com', 'journals.physiology.org', \n",
    "# #  'movementdisorders.onlinelibrary.wiley.com', 'www.pnas.org', 'www.jneurosci.org', 'thejns.org', 'pascal-francis.inist.fr', \n",
    "# #  'physoc.onlinelibrary.wiley.com', 'agro.icm.edu.pl', 'elifesciences.org', 'www.frontiersin.org', 'escholarship.mcgill.ca', \n",
    "# #  'ajp.psychiatryonline.org', 'www.science.org', 'books.google.de'}\n",
    "\n",
    "# # {'elibrary.ru', 'neurology.org', 'bmj.com', 'wiley.com', 'oup.com', 'cir.nii.ac.jp', 'springer.com', 'mdpi.com', 'mpg.de', \n",
    "# #  'biomedcentral.com', 'sagepub.com', 'cambridge.org', 'wfu.edu', nan, 'cell.com', 'europepmc.org', 'scholarpedia.org', \n",
    "# #  'aspetjournals.org', 'psych.ac.cn', 'biorxiv.org', 'ieee.org', 'jstor.org', 'cabdirect.org', 'royalsocietypublishing.org', \n",
    "# #  'bu.edu', 'lww.com', 'eneuro.org', 'jst.go.jp', 'plos.org', 'ncbi.nlm.nih.gov', 'liebertpub.com', 'psychiatryonline.org', \n",
    "# #  'sciencedirect.com', 'psycnet.apa.org', 'taylorfrancis.com', 'degruyter.com', 'nature.com', 'jamanetwork.com', \n",
    "# #  'karger.com', 'www.tandfonline.com', 'physiology.org', 'www.pnas.org', 'jneurosci.org', 'thejns.org', \n",
    "# #  'pascal-francis.inist.fr', 'agro.icm.edu.pl', 'elifesciences.org', 'frontiersin.org', 'mcgill.ca', \n",
    "# #  'science.org', 'books.google.de'}\n",
    "\n",
    "# # websites_gs = {\n",
    "# #     'neurology.org', 'bmj.com', 'wiley.com', 'oup.com', 'springer.com', 'mdpi.com', \n",
    "# #     'biomedcentral.com', 'sagepub.com', 'cambridge.org', 'wfu.edu', 'cell.com', 'europepmc.org', \n",
    "# #     'aspetjournals.org', 'psych.ac.cn', 'biorxiv.org', 'ieee.org', 'jstor.org', 'royalsocietypublishing.org', \n",
    "# #     'bu.edu', 'lww.com', 'eneuro.org', 'jst.go.jp', 'plos.org', 'ncbi.nlm.nih.gov', 'liebertpub.com', \n",
    "# #     'psychiatryonline.org', 'sciencedirect.com', 'psycnet.apa.org', 'degruyter.com', 'nature.com', 'jamanetwork.com', \n",
    "# #     'karger.com', 'tandfonline.com', 'physiology.org', 'pnas.org', 'jneurosci.org', 'thejns.org', \n",
    "# #     'agro.icm.edu.pl', 'elifesciences.org', 'frontiersin.org', 'science.org'}\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# # [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\"]\n",
    "# print(df[\"Title\"].isnull().any().any())\n",
    "# print(df[\"full_text_url\"].isnull().any().any())\n",
    "# print(df[\"full_text_source\"].isnull().any().any())\n",
    "# print(df[\"pdf_url\"].isnull().any().any())\n",
    "# # False, True, True, True\n",
    "# # full_text_url, full_text_source, pdf_url contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_google_shcolar_step2(source_path, output_path, 0, 905)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_seed_paper_spanning(source_path, output_path):\n",
    "    print(\"Starting preprocessing search results from spanning citations of seed paper...\")\n",
    "    return True\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cocomac_paper(source_path, output_path):\n",
    "    print(\"Starting preprocessing search results from CoCoMac papers...\")\n",
    "    return True\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(input, output_path):\n",
    "    # combine all results\n",
    "    df = pd.DataFrame()\n",
    "    for search_result in input:\n",
    "        df_single = pd.read_csv(search_result, header=None, sep = \",\")\n",
    "        df = pd.concat([df, df_single], ignore_index=True, sort=False)\n",
    "    \n",
    "    df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.to_csv(output_path, header=False, index=False)\n",
    "# --------------------start of test code--------------------\n",
    "# gos = fpath.poten_litera_gs_processed_step2\n",
    "# wos = fpath.poten_litera_wos_processed\n",
    "# pubmed = fpath.poten_litera_pubmed_processed\n",
    "# eupmc = fpath.poten_litera_eupmc_processed\n",
    "# input = [gos, wos, pubmed, eupmc]\n",
    "# output_path = fpath.poten_litera_combined\n",
    "# # plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# combine(input, output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "# # (14627, 8)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_in_identifiers(input_path, output_path, start, end):\n",
    "    df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "    df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "    \n",
    "    # fill in missing identifiers\n",
    "    for ind in range(start, end):\n",
    "        # if all 3 identifiers are missing, and full_text_url and pdf_url are missing, skip\n",
    "        if df.at[ind, \"DOI\"] != df.at[ind, \"DOI\"] and df.at[ind, \"PMID\"] != df.at[ind, \"PMID\"] and df.at[ind, \"PMCID\"] != df.at[ind, \"PMCID\"] and df.at[ind, \"full_text_url\"] != df.at[ind, \"full_text_url\"] and df.at[ind, \"pdf_url\"] != df.at[ind, \"pdf_url\"]:\n",
    "            continue\n",
    "        \n",
    "        # initialzie\n",
    "        doi = np.nan\n",
    "        pmid = np.nan\n",
    "        pmcid = np.nan\n",
    "        full_text_url = df.at[ind, \"full_text_url\"]\n",
    "        pdf_url = df.at[ind, \"pdf_url\"]\n",
    "        title = df.at[ind, \"Title\"]\n",
    "        abstract = df.at[ind, \"Abstract\"]\n",
    "        keywords = df.at[ind, \"Keywords\"]\n",
    "\n",
    "        # if all 3 identifiers are missing\n",
    "        if df.at[ind, \"DOI\"] != df.at[ind, \"DOI\"] and df.at[ind, \"PMID\"] != df.at[ind, \"PMID\"] and df.at[ind, \"PMCID\"] != df.at[ind, \"PMCID\"]:\n",
    "            columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "            row = {\n",
    "                \"DOI\": [doi],\n",
    "                \"PMID\": [pmid],\n",
    "                \"PMCID\": [pmcid],\n",
    "                \"full_text_url\": [full_text_url],\n",
    "                \"pdf_url\": [pdf_url],\n",
    "                \"Title\": [title],\n",
    "                \"Abstract\": [abstract],\n",
    "                \"Keywords\": [keywords]\n",
    "            }\n",
    "\n",
    "            if not plib.add_row_to_csv(output_path, row, columns):\n",
    "                print(\"Error detected when adding a row to csv!\")\n",
    "\n",
    "            print(ind)\n",
    "            continue\n",
    "        \n",
    "        # we have at least one of the 3 identifiers\n",
    "        # doi, pmid\n",
    "        if df[\"DOI\"][ind] == df[\"DOI\"][ind]: # DOI -> PMID\n",
    "            doi = str(df[\"DOI\"][ind]).strip().lower()\n",
    "            # print(doi)\n",
    "            if df[\"PMID\"][ind] == df[\"PMID\"][ind]:\n",
    "                pmid = str(df[\"PMID\"][ind]).strip()\n",
    "                # print(pmid)\n",
    "            else:\n",
    "                pmid = plib.doi2pmid(doi)\n",
    "                # print(pmid)\n",
    "                if pmid != pmid:\n",
    "                    pmid_cadidate = plib.title2pmid(title)\n",
    "                    # print(pmid_cadidate)\n",
    "        elif df[\"PMID\"][ind] == df[\"PMID\"][ind]: # PMID -> DOI\n",
    "            pmid = str(int(df[\"PMID\"][ind])).strip()\n",
    "            # print(pmid)\n",
    "            doi, pmcid = plib.pmid2doi_pmcid(pmid)\n",
    "            # print(doi)\n",
    "        elif df[\"PMCID\"][ind] == df[\"PMCID\"][ind]: # PMCID -> DOI, PMID\n",
    "            pmcid = str(df[\"PMCID\"][ind]).strip()\n",
    "            try:\n",
    "                doi, pmid = plib.pmcid2doi_pmid(pmcid)\n",
    "            except:\n",
    "                doi = np.nan\n",
    "                pmid = np.nan\n",
    "            # print(doi)\n",
    "            # print(pmid)\n",
    "        else:\n",
    "            doi = np.nan\n",
    "            pmid = np.nan\n",
    "        # print(doi)\n",
    "        # print(pmid)\n",
    "        \n",
    "        # pmcid\n",
    "        if df[\"PMCID\"][ind] == df[\"PMCID\"][ind]:\n",
    "            pmcid = str(df[\"PMCID\"][ind]).strip()\n",
    "        elif pmid == pmid:\n",
    "            a, pmcid = plib.pmid2doi_pmcid(pmid)\n",
    "        else:\n",
    "            pmcid = np.nan\n",
    "        # print(pmcid)\n",
    "    \n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"Title\": [title],\n",
    "            \"Abstract\": [abstract],\n",
    "            \"Keywords\": [keywords]\n",
    "        }\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "\n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# input_path = fpath.poten_litera_combined\n",
    "# # output_path = fpath.poten_litera_filled\n",
    "# # plib.clear_file(output_path)\n",
    "# df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "# print(df.shape)\n",
    "# df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "# print(df[\"DOI\"].isnull().any().any())\n",
    "# print(df[\"PMID\"].isnull().any().any())\n",
    "# print(df[\"PMCID\"].isnull().any().any())\n",
    "# print(df[\"full_text_url\"].isnull().any().any())\n",
    "# print(df[\"pdf_url\"].isnull().any().any())\n",
    "# print(df[\"Title\"].isnull().any().any())\n",
    "# print(df[\"Abstract\"].isnull().any().any())\n",
    "# print(df[\"Keywords\"].isnull().any().any())\n",
    "# True, True, True, True, True, False, True, True\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# fill_in_identifiers(input_path, output_path, 0, 14690)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_remove_dupli(input_path, output_path, columns, identifiers): \n",
    "    df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "    df.columns = columns\n",
    "\n",
    "    # remove all duplicates\n",
    "    for identifier in identifiers:\n",
    "        remove_dup_by = identifier\n",
    "        df = df[df[remove_dup_by].isnull() | ~df[df[remove_dup_by].notnull()].duplicated(subset=remove_dup_by, keep='first')]\n",
    "        # df = df.drop_duplicates(subset=['DOI'])\n",
    "        # df = df.drop_duplicates(subset=['PMID'])\n",
    "        # df = df.drop_duplicates(subset=['PMCID'])\n",
    "\n",
    "    # reset index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df.to_csv(output_path, header=False, index=False)\n",
    "    print(\"Duplication in the potential related literature removed.\")\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_ids_filled\n",
    "# output_path = fpath.poten_litra_filtered\n",
    "# plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# merge all search results\n",
    "# identifiers = [\"DOI\", \"PMID\", \"PMCID\"]\n",
    "# columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "# merge_remove_dupli(source_path, output_path, identifiers)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_text_url_filling(input_path, output_path, start, end):\n",
    "    df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "    df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "    \n",
    "    for ind in range(start, end):\n",
    "        time.sleep(random.randint(1, 3))\n",
    "\n",
    "        # initialize\n",
    "        doi = df.at[ind, \"DOI\"]\n",
    "        pmid = df.at[ind, \"PMID\"]\n",
    "        pmcid = df.at[ind, \"PMCID\"]\n",
    "        full_text_url = np.nan\n",
    "        pdf_url = df.at[ind, \"pdf_url\"]\n",
    "        title = df.at[ind, \"Title\"]\n",
    "        abstract = df.at[ind, \"Abstract\"]\n",
    "        keywords = df.at[ind, \"Keywords\"]\n",
    "\n",
    "        # get full text link from pmcid\n",
    "        if full_text_url!= full_text_url and pmcid == pmcid:\n",
    "            url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + str(pmcid).strip() + \"/\"\n",
    "            full_text_url, status_code = plib.get_final_redirected_url(url)\n",
    "            if status_code == 200: # some papers are assined pmcid, but the pmc wepage isn't avaiable until 2024\n",
    "                full_text_url = full_text_url\n",
    "            else:\n",
    "                print(status_code, \"Error when trying to get final redirected url from\", url)\n",
    "                full_text_url = np.nan\n",
    "        \n",
    "        # get full text url from doi\n",
    "        if full_text_url != full_text_url and doi == doi:\n",
    "            url = \"https://doi.org/\" + str(doi).strip().lower()\n",
    "            full_text_url, status_code = plib.get_final_redirected_url(url)\n",
    "            if status_code == 200 or status_code == 403:\n",
    "                full_text_url = full_text_url\n",
    "            else:\n",
    "                print(status_code, \"Error when trying to get final redirected url from\", url)\n",
    "                full_text_url = np.nan\n",
    "                \n",
    "        # get full text url from pmid\n",
    "        if full_text_url != full_text_url and pmid == pmid:\n",
    "            url = \"https://pubmed.ncbi.nlm.nih.gov/\" + str(int(df.at[ind, \"PMID\"])).strip() + \"/\"\n",
    "            try:\n",
    "                soup = plib.request_webpage(url)\n",
    "                link = soup.find(\"div\", {\"class\": \"full-text-links-list\"}).find(\"a\", {\"class\": \"link-item pmc\"})[\"href\"]\n",
    "                full_text_url, status_code = plib.get_final_redirected_url(link)\n",
    "                if status_code == 200:\n",
    "                    full_text_url = full_text_url\n",
    "                else:\n",
    "                    print(status_code, \"Error when trying to get final redirected url from\", url)\n",
    "                    full_text_url = np.nan\n",
    "                    raise Exception()\n",
    "            except:\n",
    "                try:\n",
    "                    soup = plib.request_webpage(url)\n",
    "                    link = soup.find(\"div\", {\"class\": \"full-text-links-list\"}).find(\"a\", {\"class\": \"link-item dialog-focus\"})[\"href\"]\n",
    "                    full_text_url, status_code = plib.get_final_redirected_url(link)\n",
    "                    if status_code == 200 or status_code == 403:\n",
    "                        full_text_url = full_text_url\n",
    "                    else:\n",
    "                        print(status_code, \"Error when trying to get final redirected url from\", url)\n",
    "                        full_text_url = np.nan\n",
    "                        raise Exception()\n",
    "                except:\n",
    "                    full_text_url = np.nan\n",
    "               \n",
    "        # get full text url from df.at[ind, \"full_text_url\"] \n",
    "        if full_text_url != full_text_url and df.at[ind, \"full_text_url\"] == df.at[ind, \"full_text_url\"]:\n",
    "            full_text_url, status_code = plib.get_final_redirected_url(df.at[ind, \"full_text_url\"])\n",
    "            if status_code == 200 or status_code == 403:\n",
    "                full_text_url = full_text_url\n",
    "            else:\n",
    "                print(status_code, \"Error when trying to get final redirected url from\", df.at[ind, \"full_text_url\"])\n",
    "                full_text_url = np.nan                  \n",
    "        \n",
    "        # get full text url from pmid\n",
    "        if full_text_url != full_text_url and pmid == pmid:\n",
    "            url = \"https://pubmed.ncbi.nlm.nih.gov/\" + str(int(df.at[ind, \"PMID\"])).strip() + \"/\"\n",
    "            full_text_url, status_code = plib.get_final_redirected_url(url)\n",
    "            if status_code == 200:\n",
    "                full_text_url = full_text_url\n",
    "            else:\n",
    "                print(status_code, \"Error when trying to get final redirected url from\", url)\n",
    "                full_text_url = np.nan  \n",
    "        \n",
    "        # get pdf url, pdf source\n",
    "        if pdf_url == pdf_url:\n",
    "            try:\n",
    "                pdf_url, status_code = plib.get_final_redirected_url(pdf_url)\n",
    "                if pdf_url == pdf_url:\n",
    "                    pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                else:\n",
    "                    pdf_source = np.nan\n",
    "            except:\n",
    "                print(\"error when getting final redirected url from: \", pdf_url)\n",
    "                pdf_url = np.nan\n",
    "                pdf_source = np.nan\n",
    "        else:\n",
    "            pdf_source = np.nan\n",
    "\n",
    "        # get full text source\n",
    "        if full_text_url == full_text_url:\n",
    "            full_text_source = full_text_url.split(\"://\")[1].split(\"/\")[0]\n",
    "        else:\n",
    "            full_text_source = np.nan\n",
    "        \n",
    "        # make sure doi is in lower case\n",
    "        if doi == doi:\n",
    "            doi = doi.lower()\n",
    "\n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"full_text_source\": [full_text_source],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"pdf_source\": [pdf_source],\n",
    "            \"Title\": [title],\n",
    "            \"Abstract\": [abstract],\n",
    "            \"Keywords\": [keywords]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_filling(input_path, output_path, start, end):\n",
    "    # scan each row in the potential related literature and extract information\n",
    "    df = pd.read_csv(input_path, header=None, sep=\",\")\n",
    "    df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # info = {\n",
    "        #     \"doi\": np.nan,\n",
    "        #     \"pmid\": np.nan,\n",
    "        #     \"pmcid\": np.nan,\n",
    "        #     \"title\": np.nan,\n",
    "        #     \"abstract\": np.nan,\n",
    "        #     \"keywords\": np.nan,\n",
    "        #     \"pdf_link\": np.nan\n",
    "        # }\n",
    "\n",
    "        # initialzie\n",
    "        index = df.at[ind, \"INDEX\"]\n",
    "        doi = df.at[ind, \"DOI\"]\n",
    "        pmid = df.at[ind, \"PMID\"]\n",
    "        pmcid = df.at[ind, \"PMCID\"]\n",
    "        full_text_url = df.at[ind, \"FULL_TEXT_URL\"]\n",
    "        full_text_source = df.at[ind, \"FULL_TEXT_SOURCE\"]\n",
    "        pdf_url = np.nan\n",
    "        pdf_source = np.nan\n",
    "        title = df.at[ind, \"TITLE\"]\n",
    "        abstract = df.at[ind, \"ABSTRACT\"]\n",
    "        keywords = df.at[ind, \"KEYWORDS\"]\n",
    "\n",
    "        if full_text_url != full_text_url: # full text url not found\n",
    "            if df.at[ind, \"PDF_URL\"] == df.at[ind, \"PDF_URL\"]:\n",
    "                url = str(df.at[ind, \"PDF_URL\"]).strip()\n",
    "                url1, status_code = plib.get_final_redirected_url(url)\n",
    "                if status_code == 200:\n",
    "                    pdf_url = url\n",
    "                    pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                elif status_code == 403:\n",
    "                    # print(status_code, \"when getting final redirected url: \", url)\n",
    "                    pdf_url = url\n",
    "                    pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                else:\n",
    "                    print(status_code, \"when getting final redirected url: \", url)\n",
    "                    pdf_url = np.nan\n",
    "                    pdf_source = np.nan  \n",
    "            else:\n",
    "                print(\"full text url and pdf url are not available!\")\n",
    "                pdf_url = np.nan\n",
    "                pdf_source = np.nan\n",
    "        elif ('.pdf' in full_text_url and full_text_url.split('.pdf')[1] == '') or ('.PDF' in full_text_url and full_text_url.split('.PDF')[1] == ''):\n",
    "            print(\"full text url is a pdf file: \", full_text_url)\n",
    "            pdf_url = full_text_url\n",
    "            pdf_source = full_text_source\n",
    "            full_text_url = np.nan\n",
    "            full_text_source = np.nan\n",
    "        else: # full text url found\n",
    "            flag = False\n",
    "            for website in params.websites:\n",
    "                if website in full_text_source:\n",
    "                    flag = True\n",
    "                    break\n",
    "            if not flag:\n",
    "                continue\n",
    "\n",
    "            url = str(full_text_url).strip()\n",
    "            try:\n",
    "                info = extract_info.extract_info_from_webpage(url, params.websites)\n",
    "            except:\n",
    "                raise Exception(\"Error! Cannot extract information from the webpage: \", url)\n",
    "            \n",
    "            # doi\n",
    "            if info['doi'] == info['doi'] and doi == doi and info['doi'] != doi:\n",
    "                print(doi)\n",
    "                print(info['doi'])\n",
    "\n",
    "            if info['doi'] == info['doi']:\n",
    "                doi = info['doi'].lower()\n",
    "            else:\n",
    "                doi = doi\n",
    "            \n",
    "            # pmid\n",
    "            if info['pmid'] == info['pmid'] and df.at[ind, \"PMID\"] == df.at[ind, \"PMID\"] and str(int(info['pmid'])) != str(int(df.at[ind, \"PMID\"])):\n",
    "                print(str(int(df.at[ind, \"PMID\"]))) \n",
    "                print(str(int(info['pmid'])))      \n",
    "\n",
    "            if info['pmid'] == info['pmid']:\n",
    "                pmid = str(int(info['pmid']))\n",
    "            elif pmid == pmid:\n",
    "                pmid = str(int(pmid)).strip()\n",
    "            else:\n",
    "                pmid = np.nan\n",
    "            \n",
    "            # pmcid\n",
    "            if info['pmcid'] == info['pmcid'] and pmcid == pmcid and info['pmcid'] != pmcid:\n",
    "                print(pmcid)\n",
    "                print(info['pmcid'])\n",
    "\n",
    "            if info['pmcid'] == info['pmcid']:\n",
    "                pmcid = info['pmcid']\n",
    "            else:\n",
    "                pmcid = df.at[ind, \"PMCID\"]\n",
    "            \n",
    "            # full_text_url, full_text_surce\n",
    "            if full_text_url == full_text_url:\n",
    "                full_text_source = full_text_url.split(\"://\")[1].split(\"/\")[0]\n",
    "            else:\n",
    "                print(\"full text url is not available\")\n",
    "                full_text_url = np.nan\n",
    "                full_text_source = np.nan\n",
    "\n",
    "            if pdf_url != pdf_url and info['pdf_link'] == info['pdf_link']:\n",
    "                pdf_url = str(info['pdf_link']).strip()\n",
    "                pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                # try:\n",
    "                #     pdf_url, status_code = plib.get_final_redirected_url(url)\n",
    "                #     if status_code == 200:\n",
    "                #         pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                #     elif status_code == 403:\n",
    "                #         # print(status_code, \"when getting final redirected url: \", url)\n",
    "                #         pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                #     else:\n",
    "                #         print(status_code, \"when getting final redirected url: \", url)\n",
    "                #         pdf_url = np.nan\n",
    "                #         pdf_source = np.nan\n",
    "                # except:\n",
    "                #     pdf_url = np.nan\n",
    "                #     pdf_source = np.nan \n",
    "                    \n",
    "            if pdf_url != pdf_url and df.at[ind, \"PDF_URL\"] == df.at[ind, \"PDF_URL\"]:\n",
    "                print(\"PDF_URL not extracted from info: , but existed already: \", df.at[ind, \"PDF_URL\"])\n",
    "                pdf_url = df.at[ind, \"PDF_URL\"]\n",
    "                pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "            \n",
    "            if pdf_url != pdf_url:\n",
    "                if full_text_url == full_text_url and full_text_url.split(\"://\")[1].split(\"/\")[0] == 'www.ncbi.nlm.nih.gov':\n",
    "                    if doi == doi:\n",
    "                        url = \"https://doi.org/\" + doi\n",
    "                        url, status_code = plib.get_final_redirected_url(url)\n",
    "                        info = extract_info.extract_info_from_webpage(url, params.websites)\n",
    "                        full_text_url = url\n",
    "                        full_text_source = url.split(\"://\")[1].split(\"/\")[0]\n",
    "                        pdf_url = info[\"pdf_link\"]\n",
    "                        pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                    else:\n",
    "                        pdf_url = np.nan\n",
    "                        pdf_source = np.nan\n",
    "                else:  \n",
    "                    print(\"PDF_URL not found for: \", doi, pmid, pmcid, full_text_url)\n",
    "                    pdf_url = np.nan\n",
    "                    pdf_source = np.nan\n",
    "                \n",
    "            # title\n",
    "            if info['title'] == info['title']:\n",
    "                title = info['title']\n",
    "                title = title.replace(\";\", \",\")\n",
    "            else:\n",
    "                title = title\n",
    "            \n",
    "            # abstract\n",
    "            if info['abstract'] == info['abstract']:\n",
    "                abstract = info['abstract']\n",
    "                abstract = ''.join(e for e in abstract if (e.isalpha() or e == \" \" or e == \"-\"))\n",
    "            else:\n",
    "                abstract = abstract\n",
    "            \n",
    "            # keywords\n",
    "            if info['keywords'] == info['keywords']:\n",
    "                keywords = info['keywords']\n",
    "                keywords = keywords.replace(\";\", \",\")\n",
    "                keywords = ''.join(e for e in keywords if (e.isalpha() or e == \" \" or e == \"-\" or e == \",\"))\n",
    "            else:\n",
    "                keywords = keywords\n",
    "        \n",
    "        columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "        \n",
    "        row = {\n",
    "            \"INDEX\": [index],\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"FULL_TEXT_URL\": [full_text_url],\n",
    "            \"FULL_TEXT_SOURCE\": [full_text_source],\n",
    "            \"PDF_URL\": [pdf_url],\n",
    "            \"PDF_SOURCE\": [pdf_source],\n",
    "            \"TITLE\": [title],\n",
    "            \"ABSTRACT\": [abstract],\n",
    "            \"KEYWORDS\": [keywords]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# input_path = fpath.poten_litera_ids_ftl_filled\n",
    "# output_path = fpath.poten_litera_litera_db\n",
    "\n",
    "# # clear file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# info_filling(input_path, output_path)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(input_path, pdf_folder, start, end):\n",
    "    df = pd.read_csv(input_path, header=None, sep=',')\n",
    "    df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # time.sleep(3)\n",
    "        \n",
    "        # the flag to indicate whether the pdf is downloaded successfully, if not, save the row to poten_litera_pdf_not_available.csv\n",
    "        flag = False\n",
    "\n",
    "        pdf_url = df.at[ind, \"PDF_URL\"]\n",
    "\n",
    "        # pdf_url not found\n",
    "        if pdf_url != pdf_url:\n",
    "            flag = False\n",
    "        # pdf_url found\n",
    "        else:\n",
    "            doi = df.at[ind, \"DOI\"]\n",
    "            if dpp.download_and_rename_pdf(pdf_url, doi, df.at[ind, \"INDEX\"], pdf_folder):\n",
    "                flag = True\n",
    "            else:\n",
    "                print(df.at[ind, \"FULL_TEXT_URL\"])\n",
    "                flag = False\n",
    "\n",
    "        # # pdf_url not found\n",
    "        # if pdf_url != \"://linkinghub.elsevier.com/\":\n",
    "        #     continue\n",
    "        # # pdf_url found\n",
    "        # else:\n",
    "        #     doi = df.at[ind, \"DOI\"]\n",
    "        #     if dpp.download_and_rename_pdf(pdf_url, doi, df.at[ind, \"INDEX\"], pdf_folder):\n",
    "        #         flag = True\n",
    "        #     else:\n",
    "        #         print(df.at[ind, \"FULL_TEXT_URL\"])\n",
    "        #         flag = False\n",
    "        \n",
    "        if not flag:\n",
    "            print(\"PDF_URL not found or PDF not successfully downloaded for: \")\n",
    "            print(\"\\n\")\n",
    "            print(df.at[ind, \"INDEX\"], df.at[ind, \"DOI\"], df.at[ind, \"PMID\"], df.at[ind, \"PMCID\"], df.at[ind, \"FULL_TEXT_URL\"], df.at[ind, \"FULL_TEXT_SOURCE\"], df.at[ind, \"PDF_URL\"], df.at[ind, \"PDF_SOURCE\"], df.at[ind, \"TITLE\"], df.at[ind, \"ABSTRACT\"], df.at[ind, \"KEYWORDS\"])\n",
    "            # print(\"\\n\")\n",
    "\n",
    "        line_number_in_csv = ind + 1\n",
    "        print(ind, \" Line number:\", line_number_in_csv, \" INDEX:\", int(df.at[ind, \"INDEX\"]))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf2text(pdf_path, text_path): \n",
    "    try:   \n",
    "        # # creating a pdf reader object\n",
    "        # reader = PyPDF2.PdfReader(pdf_path)\n",
    "        \n",
    "        # # printing number of pages in pdf file\n",
    "        # page_max = len(reader.pages)\n",
    "        # # print(page_max)\n",
    "        \n",
    "        # # getting a specific page from the pdf file\n",
    "        # text = \"\"\n",
    "        \n",
    "        # for i in range(0, page_max):\n",
    "        #     page = reader.pages[i]\n",
    "        #     text = text + \" \".join(page.extract_text().splitlines())\n",
    "\n",
    "        # with open(text_path, \"w\") as f:\n",
    "        #     f.write(text)\n",
    "        # f.close()\n",
    "\n",
    "        text = \"\"\n",
    "        pdf = pdfium.PdfDocument(pdf_path)\n",
    "        n_pages = len(pdf)\n",
    "        for i in range(n_pages):\n",
    "            page = pdf[i]\n",
    "            textpage = page.get_textpage()\n",
    "            text += textpage.get_text_range() + \" \"\n",
    "            # text += textpage.get_text()\n",
    "            # text += \"\\n\"\n",
    "            [g.close() for g in (textpage, page)]\n",
    "        pdf.close()\n",
    "\n",
    "        # remove non-ASCII characters from the string\n",
    "        text = text.encode('ascii', 'ignore').decode()\n",
    "        text = text.replace(\"\\n\", \" \").replace(\"\\r\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
    "        # remove extra spaces\n",
    "        text = re.sub(' +', ' ', text)\n",
    "\n",
    "        with open(text_path, \"w\") as f:\n",
    "            f.write(text)\n",
    "        f.close()\n",
    "    except:\n",
    "        print(\"ERROR! ERROR!\")\n",
    "        print(\"ERROR when converting pdf to text for: \", pdf_path)\n",
    "# --------------------start of test code--------------------\n",
    "# index = 0\n",
    "# pdf_folder = fpath.pdf_folder\n",
    "# text_folder = fpath.text_folder\n",
    "\n",
    "# pdf_file_name = str(index) + \".pdf\"\n",
    "# pdf_path = os.path.join(pdf_folder, pdf_file_name)\n",
    "# text_path = os.path.join(text_folder, pdf_file_name.split(\".pdf\")[0] + \".txt\")\n",
    "# # print(pdf_path)\n",
    "# # print(text_path)\n",
    "# pdf2text(pdf_path, text_path)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json2text(json_path, text_path):    \n",
    "    with open(json_path, \"r\") as f:\n",
    "        json_file = json.load(f)\n",
    "        text = json_file[\"full-text-retrieval-response\"][\"originalText\"]\n",
    "    f.close()\n",
    "\n",
    "    title = json_file[\"full-text-retrieval-response\"][\"coredata\"][\"dc:title\"]\n",
    "\n",
    "    text = title + \" \" + text.split(title)[1]\n",
    "    text = re.sub(' +', ' ', text)\n",
    "\n",
    "    with open(text_path, \"w\") as f:\n",
    "        f.write(text)\n",
    "    f.close()\n",
    "# --------------------start of test code--------------------\n",
    "# index = 1\n",
    "# pdf_folder = fpath.pdf_folder\n",
    "# text_folder = fpath.text_folder\n",
    "# start_page = 0\n",
    "# text_length_to_extract = 1000\n",
    "\n",
    "# json_file_name = str(index) + \".json\"\n",
    "# json_path = os.path.join(pdf_folder, json_file_name)\n",
    "# text_path = os.path.join(text_folder, str(index) + \".txt\")\n",
    "# # print(pdf_path)\n",
    "# # print(text_path)\n",
    "# json2text(json_path, text_path, text_length_to_extract)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Main program: </h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 1. Preprocess and combine search results </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from PubMed\n",
    "\n",
    "# source_path = fpath.poten_litera_pubmed\n",
    "# output_path = fpath.poten_litera_pubmed_processed\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from PubMed\n",
    "# # 2612 results\n",
    "# preprocess_pubmed(source_path, output_path, 0, 2612)\n",
    "# print(\"preprocessing results from PubMed succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from PubMed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine search results from Web of Science\n",
    "# # clear the file\n",
    "# plib.clear_file(fpath.poten_litera_wos)\n",
    "\n",
    "# # combine the 2 files of search results from web of science\n",
    "# source_path_1 = fpath.poten_litera_wos_1\n",
    "# source_path_2 = fpath.poten_litera_wos_2\n",
    "# df_1 = pd.read_csv(source_path_1, sep=',')\n",
    "# df_2 = pd.read_csv(source_path_2, sep=',')\n",
    "# df_1.to_csv(fpath.poten_litera_wos, header=True, index=False, sep=\",\")\n",
    "# df_2.to_csv(fpath.poten_litera_wos, mode=\"a\", header=False, index=False, sep=\",\")\n",
    "# # --------------------start of test code--------------------\n",
    "# df = pd.read_csv(fpath.poten_litera_wos, sep=',')\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "# # (1993, 72)\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from Web of Science\n",
    "\n",
    "# source_path = fpath.poten_litera_wos\n",
    "# output_path = fpath.poten_litera_wos_processed\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from Web of Science\n",
    "# # 1993 results\n",
    "# preprocess_webofscience(source_path, output_path, 0, 1993)\n",
    "# print(\"preprocessing results from Web of Science succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from Web of Science!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from Europe PMC\n",
    "\n",
    "# source_path = fpath.poten_litera_eupmc\n",
    "# output_path = fpath.poten_litera_eupmc_processed\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from Europe PMC\n",
    "# preprocess_eupmc(source_path, output_path, 2980, 9178)\n",
    "# # 9178 results\n",
    "# print(\"preprocessing results from Europe PMC succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from Europe PMC!\")\n",
    "\n",
    "# # 2980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from Google Scholar step 1\n",
    "\n",
    "# source_path = fpath.poten_litera_gs\n",
    "# # 980 results\n",
    "# output_path = fpath.poten_litera_gs_processed_step1\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from Google Scholar\n",
    "# preprocess_google_shcolar_step1(source_path, output_path, 0, 980)\n",
    "# # 926 results\n",
    "# print(\"step 1 of preprocessing results from Google Scholar succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from Google Scholar step 1!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reset index for poten_litera_gs_processed_step1\n",
    "# input_path = fpath.poten_litera_gs_processed_step1\n",
    "# output_path = fpath.poten_litera_gs_processed_step1\n",
    "# df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df.to_csv(output_path, header=False, index=False)\n",
    "\n",
    "# input_path = fpath.poten_litera_gs_processed_step1\n",
    "# df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "# print(df.shape)\n",
    "# # (926, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from Google Scholar step 2\n",
    "\n",
    "# source_path = fpath.poten_litera_gs_processed_step1\n",
    "# # (926, 4)\n",
    "# output_path = fpath.poten_litera_gs_processed_step2\n",
    "\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from Google Scholar\n",
    "# preprocess_google_shcolar_step2(source_path, output_path, 0, 926)\n",
    "# print(\"step 2 of preprocessing results from Google Scholar succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from Google Scholar step 2!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reset index for poten_litera_gs_processed_step2\n",
    "# input_path = fpath.poten_litera_gs_processed_step2\n",
    "# output_path = fpath.poten_litera_gs_processed_step2\n",
    "# df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df.to_csv(output_path, header=False, index=False)\n",
    "\n",
    "# input_path = fpath.poten_litera_gs_processed_step2\n",
    "# df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "# print(df.shape)\n",
    "# # (926, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from spanning citations of seed paper\n",
    "\n",
    "# preprocess_seed_paper_spanning(source_path, output_path, columns):\n",
    "# print(\"preprocessing results from spanning citations of seed papers succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from spanning citations of seed papers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from CoCoMac papers\n",
    "\n",
    "# preprocess_cocomac_paper(source_path, output_path, columns)\n",
    "# print(\"preprocessing results from CoCoMac papers succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from CoCoMac papers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # take a look at all the preprossed search results\n",
    "# gos = fpath.poten_litera_gs_processed_step2\n",
    "# wos = fpath.poten_litera_wos_processed\n",
    "# pubmed = fpath.poten_litera_pubmed_processed\n",
    "# eupmc = fpath.poten_litera_eupmc_processed\n",
    "\n",
    "# df_gs = pd.read_csv(gos, header=None, sep=',')\n",
    "# print(df_gs.shape)\n",
    "# # (907, 6)\n",
    "# df_wos = pd.read_csv(wos, header=None, sep=',')\n",
    "# print(df_wos.shape)\n",
    "# # (1993, 8)\n",
    "# df_pubmed = pd.read_csv(pubmed, header=None, sep=',')\n",
    "# print(df_pubmed.shape)\n",
    "# # (2612, 8)\n",
    "# df_eupmc = pd.read_csv(eupmc, header=None, sep=',')\n",
    "# print(df_eupmc.shape)\n",
    "# # (9178, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine all search results\n",
    "\n",
    "# gos = fpath.poten_litera_gs_processed_step2\n",
    "# wos = fpath.poten_litera_wos_processed\n",
    "# pubmed = fpath.poten_litera_pubmed_processed\n",
    "# eupmc = fpath.poten_litera_eupmc_processed\n",
    "# input = [gos, wos, pubmed, eupmc]\n",
    "# output_path = fpath.poten_litera_combined\n",
    "\n",
    "# # clear the file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# combine(input, output_path)\n",
    "# # (14627, 8)\n",
    "# print(\"Combining all search results succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when combining all search results!\")\n",
    "\n",
    "# df_combined = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df_combined.shape)\n",
    "# # (14690, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fill in missing identifiers\n",
    "# input_path = fpath.poten_litera_combined\n",
    "# output_path = fpath.poten_litera_ids_filled\n",
    "\n",
    "# # clear file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# fill_in_identifiers(input_path, output_path, 0, 14690)\n",
    "# print(\"Filling in missing elements succeeded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check the missing elements in the combined search results\n",
    "# source_path = fpath.poten_litera_ids_filled\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "# # for ind in df.index:\n",
    "# #     if df.at[ind, \"DOI\"] != df.at[ind, \"DOI\"] and df.at[ind, \"PMID\"] != df.at[ind, \"PMID\"] and df.at[ind, \"PMCID\"] != df.at[ind, \"PMCID\"] and df.at[ind, \"full_text_url\"] != df.at[ind, \"full_text_url\"] and df.at[ind, \"pdf_url\"] != df.at[ind, \"pdf_url\"]:\n",
    "# #         print(ind)\n",
    "# #         print(df.at[ind, \"Title\"])\n",
    "# #         print(df.at[ind, \"full_text_url\"])\n",
    "# #         print(df.at[ind, \"pdf_url\"])\n",
    "\n",
    "# # for ind in df.index:\n",
    "# #     if df.at[ind, \"DOI\"] != df.at[ind, \"DOI\"] and df.at[ind, \"PMID\"] != df.at[ind, \"PMID\"] and df.at[ind, \"PMCID\"] != df.at[ind, \"PMCID\"] and df.at[ind, \"full_text_url\"] != df.at[ind, \"full_text_url\"]:\n",
    "# #         print(ind)\n",
    "# #         print(df.at[ind, \"Title\"])\n",
    "# #         print(df.at[ind, \"full_text_url\"])\n",
    "# #         print(df.at[ind, \"pdf_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge all search results and remove duplication by identifiers\n",
    "\n",
    "# source_path = fpath.poten_litera_ids_filled\n",
    "# output_path = fpath.poten_litra_filtered\n",
    "\n",
    "# # clear the file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# # merge all search results\n",
    "# identifiers = [\"DOI\", \"PMID\", \"PMCID\"]\n",
    "# columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "# merge_remove_dupli(source_path, output_path, columns, identifiers)\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litra_filtered\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# print(df.shape)\n",
    "# # (10982, 8)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2. Filling in missing information and construct database </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fill in full_text_url\n",
    "\n",
    "# source_path = fpath.poten_litra_filtered\n",
    "# output_path = fpath.poten_litera_ids_ftl_filled\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # merge all search results\n",
    "# full_text_url_filling(source_path, output_path, 0, 10980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove duplications and reset the index and add index column\n",
    "# source_path = fpath.poten_litera_ids_ftl_filled\n",
    "# output_path = fpath.poten_litera_ids_ftl_filled_filtered\n",
    "\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "# identifiers = [\"DOI\", \"PMID\", \"PMCID\"]\n",
    "# # merge_remove_dupli(source_path, output_path, columns, identifiers)\n",
    "\n",
    "# for identifier in identifiers:\n",
    "#     remove_dup_by = identifier\n",
    "#     df = df[df[remove_dup_by].isnull() | ~df[df[remove_dup_by].notnull()].duplicated(subset=remove_dup_by, keep='last')]\n",
    "\n",
    "# # reset index\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# df.to_csv(output_path, header=False, index=True)\n",
    "# print(\"Duplication in the potential related literature removed.\")\n",
    "# # --------------------start of test code--------------------\n",
    "# output_path = fpath.poten_litera_ids_ftl_filled_filtered\n",
    "# df_output = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df_output.head(5))\n",
    "# print(df_output.shape)\n",
    "# # (10980, 11)\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check the missing elements\n",
    "# source_path = fpath.poten_litera_ids_ftl_filled_filtered\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "\n",
    "# # # if all 3 identifiers are missing, and full_text_url and pdf_url are missing\n",
    "# # for ind in df.index:\n",
    "# #     if df.at[ind, \"DOI\"] != df.at[ind, \"DOI\"] and df.at[ind, \"PMID\"] != df.at[ind, \"PMID\"] and df.at[ind, \"PMCID\"] != df.at[ind, \"PMCID\"] and df.at[ind, \"full_text_url\"] != df.at[ind, \"full_text_url\"] and df.at[ind, \"pdf_url\"] != df.at[ind, \"pdf_url\"]:\n",
    "# #         print(ind)\n",
    "# #         print(df.at[ind, \"Title\"])\n",
    "# #         print(df.at[ind, \"full_text_url\"])\n",
    "# #         print(df.at[ind, \"pdf_url\"])\n",
    "# # # None\n",
    "\n",
    "# # if all 3 identifiers are missing and full_text_url is missing, but pdf_url is available\n",
    "# for ind in df.index:\n",
    "#     if df.at[ind, \"DOI\"] != df.at[ind, \"DOI\"] and df.at[ind, \"PMID\"] != df.at[ind, \"PMID\"] and df.at[ind, \"PMCID\"] != df.at[ind, \"PMCID\"] and df.at[ind, \"full_text_url\"] != df.at[ind, \"full_text_url\"]:\n",
    "#         print(ind)\n",
    "#         print(df.at[ind, \"Title\"])\n",
    "#         print(df.at[ind, \"full_text_url\"])\n",
    "#         print(df.at[ind, \"pdf_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check all possible full_text_source\n",
    "# input_path = fpath.poten_litera_ids_ftl_filled_filtered\n",
    "# df = pd.read_csv(input_path, header=None, sep=\",\")\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "# print(df.shape)\n",
    "# # (10980, 11)\n",
    "\n",
    "# # [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "# print(df[\"FULL_TEXT_URL\"].isnull().any().any()) # True\n",
    "# print(df[\"FULL_TEXT_SOURCE\"].isnull().any().any()) # True\n",
    "# print(df[\"TITLE\"].isnull().any().any()) # False\n",
    "\n",
    "# print(df[\"INDEX\"].dtypes) # int64\n",
    "# print(df[\"DOI\"].dtypes) # object\n",
    "# print(df[\"PMID\"].dtypes) # float64\n",
    "# print(df[\"PMCID\"].dtypes) # object\n",
    "# print(df[\"FULL_TEXT_URL\"].dtypes) # object\n",
    "# print(df[\"FULL_TEXT_SOURCE\"].dtypes) # object\n",
    "# print(df[\"PDF_URL\"].dtypes) # object\n",
    "# print(df[\"PDF_SOURCE\"].dtypes) # object\n",
    "# print(df[\"TITLE\"].dtypes) # object\n",
    "# print(df[\"ABSTRACT\"].dtypes) # object\n",
    "# print(df[\"KEYWORDS\"].dtypes) # object\n",
    "\n",
    "# full_text_source_dict = set(df['FULL_TEXT_SOURCE'].tolist())\n",
    "# print(full_text_source_dict)\n",
    "# full_text_source_dict = {'pharmrev.aspetjournals.org', 'www.ingentaconnect.com', 'pubs.aip.org', 'journal.psych.ac.cn', 'iovs.arvojournals.org', \n",
    "# 'linkinghub.elsevier.com', 'www.architalbiol.org', 'open.bu.edu', 'psycnet.apa.org:443', 'jamanetwork.com', \n",
    "# 'link.springer.com', 'europepmc.org', 'www.ncbi.nlm.nih.gov', 'thejns.org', 'ujms.net', 'jpet.aspetjournals.org', \n",
    "# 'journals.biologists.com', 'www.thieme-connect.de', 'academic.oup.com', 'direct.mit.edu', 'ajp.psychiatryonline.org', \n",
    "# 'journals.lww.com', 'wakespace.lib.wfu.edu', 'www.ahajournals.org', 'symposium.cshlp.org', 'www.microbiologyresearch.org', \n",
    "# 'journals.aps.org', 'www.cambridge.org', 'www.imrpress.com', 'www.jstor.org', 'www.researchsquare.com', 'www.science.org', \n",
    "# 'content.iospress.com:443', 'n.neurology.org', 'royalsocietypublishing.org', 'ieeexplore.ieee.org', 'neuro.psychiatryonline.org', \n",
    "# 'pubmed.ncbi.nlm.nih.gov', 'analyticalsciencejournals.onlinelibrary.wiley.com', 'www.rbojournal.org', 'papers.ssrn.com', \n",
    "# 'www.worldscientific.com', 'www.jstage.jst.go.jp', 'webview.isho.jp', 'www.degruyter.com', 'www.taylorfrancis.com', \n",
    "# 'www.biorxiv.org', 'nan', 'www.liebertpub.com', 'opg.optica.org', 'jnm.snmjournals.org', 'neurologia.com', 'www.nature.com', \n",
    "# 'karger.com', 'www.tandfonline.com', 'onlinelibrary.wiley.com', 'www.ajtmh.org', 'pubs.acs.org', 'www.annualreviews.org', \n",
    "# 'journals.physiology.org', 'journals.sagepub.com', 'pubs.asahq.org', 'nrc-prod.literatumonline.com'}\n",
    "\n",
    "# # dict2 = {}\n",
    "\n",
    "# # print(full_text_source_dict == dict2) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # websites_hosts\n",
    "# # {'pharmrev.aspetjournals.org', 'www.ingentaconnect.com', 'pubs.aip.org', 'journal.psych.ac.cn', 'iovs.arvojournals.org', \n",
    "# # 'linkinghub.elsevier.com', 'www.architalbiol.org', 'open.bu.edu', 'psycnet.apa.org:443', 'jamanetwork.com', \n",
    "# # 'link.springer.com', 'europepmc.org', 'www.ncbi.nlm.nih.gov', 'thejns.org', 'ujms.net', 'jpet.aspetjournals.org', \n",
    "# # 'journals.biologists.com', 'www.thieme-connect.de', 'academic.oup.com', 'direct.mit.edu', 'ajp.psychiatryonline.org', \n",
    "# # 'journals.lww.com', 'wakespace.lib.wfu.edu', 'www.ahajournals.org', 'symposium.cshlp.org', 'www.microbiologyresearch.org', \n",
    "# # 'journals.aps.org', 'www.cambridge.org', 'www.imrpress.com', 'www.jstor.org', 'www.researchsquare.com', 'www.science.org', \n",
    "# # 'content.iospress.com:443', 'n.neurology.org', 'royalsocietypublishing.org', 'ieeexplore.ieee.org', 'neuro.psychiatryonline.org', \n",
    "# # 'pubmed.ncbi.nlm.nih.gov', 'analyticalsciencejournals.onlinelibrary.wiley.com', 'www.rbojournal.org', 'papers.ssrn.com', \n",
    "# # 'www.worldscientific.com', 'www.jstage.jst.go.jp', 'webview.isho.jp', 'www.degruyter.com', 'www.taylorfrancis.com', \n",
    "# # 'www.biorxiv.org', 'nan', 'www.liebertpub.com', 'opg.optica.org', 'jnm.snmjournals.org', 'neurologia.com', 'www.nature.com', \n",
    "# # 'karger.com', 'www.tandfonline.com', 'onlinelibrary.wiley.com', 'www.ajtmh.org', 'pubs.acs.org', 'www.annualreviews.org', \n",
    "# # 'journals.physiology.org', 'journals.sagepub.com', 'pubs.asahq.org', 'nrc-prod.literatumonline.com'}\n",
    "\n",
    "# websites_hosts = [\n",
    "#     'aspetjournals.org', 'www.ingentaconnect.com', 'pubs.aip.org', 'journal.psych.ac.cn', 'iovs.arvojournals.org', \n",
    "#     'linkinghub.elsevier.com', 'www.architalbiol.org', 'open.bu.edu', 'psycnet.apa.org', 'jamanetwork.com', \n",
    "#     'link.springer.com', 'europepmc.org', 'www.ncbi.nlm.nih.gov', 'thejns.org', 'ujms.net',\n",
    "#     'journals.biologists.com', 'www.thieme-connect.de', 'academic.oup.com', 'direct.mit.edu', 'psychiatryonline.org', \n",
    "#     'journals.lww.com', 'wakespace.lib.wfu.edu', 'www.ahajournals.org', 'symposium.cshlp.org', 'www.microbiologyresearch.org', \n",
    "#     'journals.aps.org', 'www.cambridge.org', 'www.imrpress.com', 'www.jstor.org', 'www.researchsquare.com', 'www.science.org', \n",
    "#     'content.iospress.com', 'neurology.org', 'royalsocietypublishing.org', 'ieeexplore.ieee.org', \n",
    "#     'pubmed.ncbi.nlm.nih.gov', 'wiley.com', \n",
    "#     'www.rbojournal.org', 'papers.ssrn.com', 'www.worldscientific.com', 'www.jstage.jst.go.jp', 'webview.isho.jp', \n",
    "#     'www.degruyter.com', 'www.taylorfrancis.com', 'www.biorxiv.org', 'nan', 'www.liebertpub.com', 'opg.optica.org', \n",
    "#     'jnm.snmjournals.org', 'neurologia.com', 'www.nature.com', 'karger.com', 'www.tandfonline.com',\n",
    "#     'www.ajtmh.org', 'pubs.acs.org', 'www.annualreviews.org', 'journals.physiology.org', 'journals.sagepub.com', \n",
    "#     'pubs.asahq.org', 'literatumonline.com'\n",
    "# ]\n",
    "\n",
    "# # --------------------start of test code--------------------\n",
    "# if len(websites_hosts) == len(set(websites_hosts)):\n",
    "#     print(\"There are no duplicates in the list.\")\n",
    "# else:\n",
    "#     print(\"There are duplicates in the list.\")\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sort the websites by the number of articles they have\n",
    "# input_path = fpath.poten_litera_ids_ftl_filled_filtered\n",
    "# df = pd.read_csv(input_path, header=None, sep=\",\")\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "# func_dict = {website: 0 for website in websites_hosts}\n",
    "# # print(func_dict)\n",
    "\n",
    "# for ind in df.index:\n",
    "#     if df.at[ind, \"FULL_TEXT_SOURCE\"] != df.at[ind, \"FULL_TEXT_SOURCE\"]:\n",
    "#         func_dict[\"nan\"] += 1\n",
    "#         continue\n",
    "#     for website in websites_hosts:\n",
    "#         if website in df.at[ind, \"FULL_TEXT_SOURCE\"]:\n",
    "#             func_dict[website] += 1\n",
    "#             break\n",
    "\n",
    "# # Sort dictionary by values\n",
    "# sorted_dict = dict(sorted(func_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "# print(sorted_dict)\n",
    "# # {'www.ncbi.nlm.nih.gov': 7913, 'linkinghub.elsevier.com': 1023, 'wiley.com': 701, 'link.springer.com': 288, \n",
    "# # 'journals.physiology.org': 200, 'academic.oup.com': 152, 'pubmed.ncbi.nlm.nih.gov': 147, 'www.cambridge.org': 74, \n",
    "# # 'karger.com': 54, 'journals.lww.com': 49, 'www.nature.com': 45, 'nan': 38, 'www.science.org': 30, \n",
    "# # 'www.tandfonline.com': 29, 'journals.sagepub.com': 21, 'jamanetwork.com': 20, 'neurology.org': 16, \n",
    "# # 'www.biorxiv.org': 15, 'europepmc.org': 14, 'iovs.arvojournals.org': 13, 'royalsocietypublishing.org': 13, \n",
    "# # 'psycnet.apa.org': 12, 'psychiatryonline.org': 12, 'direct.mit.edu': 11, 'www.jstage.jst.go.jp': 11, \n",
    "# # 'thejns.org': 8, 'www.annualreviews.org': 8, 'aspetjournals.org': 7, 'jnm.snmjournals.org': 7, \n",
    "# # 'www.architalbiol.org': 4, 'www.ahajournals.org': 4, 'content.iospress.com': 3, 'www.worldscientific.com': 3, \n",
    "# # 'www.liebertpub.com': 3, 'pubs.acs.org': 3, 'www.thieme-connect.de': 2, 'opg.optica.org': 2, \n",
    "# # 'neurologia.com': 2, 'pubs.asahq.org': 2, 'www.ingentaconnect.com': 1, 'pubs.aip.org': 1, 'journal.psych.ac.cn': 1, \n",
    "# # 'open.bu.edu': 1, 'ujms.net': 1, 'journals.biologists.com': 1, 'wakespace.lib.wfu.edu': 1, \n",
    "# # 'symposium.cshlp.org': 1, 'www.microbiologyresearch.org': 1, 'journals.aps.org': 1, 'www.imrpress.com': 1, \n",
    "# # 'www.jstor.org': 1, 'www.researchsquare.com': 1, 'ieeexplore.ieee.org': 1, 'www.rbojournal.org': 1, \n",
    "# # 'papers.ssrn.com': 1, 'webview.isho.jp': 1, 'www.degruyter.com': 1, 'www.taylorfrancis.com': 1, \n",
    "# # 'www.ajtmh.org': 1, 'literatumonline.com': 1}\n",
    "\n",
    "# non_zero_keys = [key for key, value in sorted_dict.items() if value != 0]\n",
    "# print(non_zero_keys)\n",
    "# # ['www.ncbi.nlm.nih.gov', 'linkinghub.elsevier.com', 'wiley.com', 'link.springer.com', 'journals.physiology.org', \n",
    "# # 'academic.oup.com', 'pubmed.ncbi.nlm.nih.gov', 'www.cambridge.org', 'karger.com', 'journals.lww.com', \n",
    "# # 'www.nature.com', 'nan', 'www.science.org', 'www.tandfonline.com', 'journals.sagepub.com', 'jamanetwork.com', \n",
    "# # 'neurology.org', 'www.biorxiv.org', 'europepmc.org', 'iovs.arvojournals.org', 'royalsocietypublishing.org', \n",
    "# # 'psycnet.apa.org', 'psychiatryonline.org', 'direct.mit.edu', 'www.jstage.jst.go.jp', 'thejns.org', \n",
    "# # 'www.annualreviews.org', 'aspetjournals.org', 'jnm.snmjournals.org', 'www.architalbiol.org', 'www.ahajournals.org', \n",
    "# # 'content.iospress.com', 'www.worldscientific.com', 'www.liebertpub.com', 'pubs.acs.org', 'www.thieme-connect.de', \n",
    "# # 'opg.optica.org', 'neurologia.com', 'pubs.asahq.org', 'www.ingentaconnect.com', 'pubs.aip.org', 'journal.psych.ac.cn', \n",
    "# # 'open.bu.edu', 'ujms.net', 'journals.biologists.com', 'wakespace.lib.wfu.edu', 'symposium.cshlp.org', \n",
    "# # 'www.microbiologyresearch.org', 'journals.aps.org', 'www.imrpress.com', 'www.jstor.org', 'www.researchsquare.com', \n",
    "# # 'ieeexplore.ieee.org', 'www.rbojournal.org', 'papers.ssrn.com', 'webview.isho.jp', 'www.degruyter.com', \n",
    "# # 'www.taylorfrancis.com', 'www.ajtmh.org', 'literatumonline.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # websites\n",
    "# websites = [\n",
    "#     'www.ncbi.nlm.nih.gov', 'linkinghub.elsevier.com', 'wiley.com', 'link.springer.com', 'journals.physiology.org', \n",
    "#     'academic.oup.com', 'pubmed.ncbi.nlm.nih.gov', 'www.cambridge.org', 'karger.com', 'journals.lww.com', \n",
    "#     'www.nature.com', 'nan', 'www.science.org', 'www.tandfonline.com', 'journals.sagepub.com', 'jamanetwork.com', \n",
    "#     'neurology.org', 'www.biorxiv.org', 'europepmc.org', 'iovs.arvojournals.org', 'royalsocietypublishing.org', \n",
    "#     'psycnet.apa.org', 'psychiatryonline.org', 'direct.mit.edu', 'www.jstage.jst.go.jp', 'thejns.org', \n",
    "#     'www.annualreviews.org', 'aspetjournals.org', 'jnm.snmjournals.org', 'www.architalbiol.org', 'www.ahajournals.org', \n",
    "#     'content.iospress.com', 'www.worldscientific.com', 'www.liebertpub.com', 'pubs.acs.org', 'www.thieme-connect.de', \n",
    "#     'opg.optica.org', 'neurologia.com', 'pubs.asahq.org', 'www.ingentaconnect.com', 'pubs.aip.org', 'journal.psych.ac.cn', \n",
    "#     'open.bu.edu', 'ujms.net', 'journals.biologists.com', 'wakespace.lib.wfu.edu', 'symposium.cshlp.org', \n",
    "#     'www.microbiologyresearch.org', 'journals.aps.org', 'www.imrpress.com', 'www.jstor.org', 'www.researchsquare.com', \n",
    "#     'ieeexplore.ieee.org', 'www.rbojournal.org', 'papers.ssrn.com', 'webview.isho.jp', 'www.degruyter.com', \n",
    "#     'www.taylorfrancis.com', 'www.ajtmh.org', 'literatumonline.com'\n",
    "# ]\n",
    "# # --------------------start of test code--------------------\n",
    "# # if len(websites) == len(websites_hosts):\n",
    "# #     print('The number of websites is correct')\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # websites to remove\n",
    "\n",
    "# # Reasons to move literature from these websites:\n",
    "# # 1. nan\n",
    "# # 2. full text not available\n",
    "# # 3. non-English\n",
    "# # 4. theses or dissertations\n",
    "\n",
    "# # 'pubmed.ncbi.nlm.nih.gov', \n",
    "# # https://pubmed.ncbi.nlm.nih.gov/4984008/\n",
    "# # full text not available\n",
    "# # 'nan', \n",
    "# # full text not available\n",
    "# # 'psycnet.apa.org', \n",
    "# # https://psycnet.apa.org/doiLanding?doi=10.1037%2F0735-7044.112.3.719\n",
    "# # full text not available\n",
    "# # 'content.iospress.com', \n",
    "# # https://content.iospress.com:443/articles/restorative-neurology-and-neuroscience/rnn140440\n",
    "# # full text not available\n",
    "# # 'www.worldscientific.com', \n",
    "# # https://www.worldscientific.com/doi/abs/10.1142/S0192415X06004296\n",
    "# # full text not available\n",
    "# # 'www.liebertpub.com', \n",
    "# # https://www.liebertpub.com/doi/10.1089/hum.2006.17.291\n",
    "# # full text not available\n",
    "# # 'opg.optica.org', \n",
    "# # https://opg.optica.org/josaa/viewmedia.cfm?uri=josaa-3-10-1726&seq=0\n",
    "# # full text not available\n",
    "# # 'neurologia.com', \n",
    "# # https://neurologia.com/articulo/99529\n",
    "# # non-English\n",
    "# # 'pubs.aip.org', \n",
    "# # https://pubs.aip.org/asa/jasa/article-abstract/104/5/2935/560140/Click-train-encoding-in-primary-auditory-cortex-of?redirectedFrom=fulltext\n",
    "# # full text not available\n",
    "# # 'journal.psych.ac.cn', \n",
    "# # https://journal.psych.ac.cn/adps/EN/abstract/abstract3663.shtml\n",
    "# # full text not available\n",
    "# # 'open.bu.edu', \n",
    "# # https://open.bu.edu/handle/2144/12127\n",
    "# # theses or dissertations\n",
    "# # 'wakespace.lib.wfu.edu', \n",
    "# # https://wakespace.lib.wfu.edu/handle/10339/37434\n",
    "# # theses or dissertations, full text not available\n",
    "# # 'symposium.cshlp.org', \n",
    "# # https://symposium.cshlp.org/content/61/39.long\n",
    "# # full text not available\n",
    "# # 'www.jstor.org', \n",
    "# # https://www.jstor.org/stable/82698\n",
    "# # full text not available\n",
    "# # 'www.rbojournal.org', \n",
    "# # https://www.rbojournal.org/article/influencia-da-thalamosinusotomia-sob-diferentes-pressoes-intra-oculares-constantes-na-facilidade-de-drenagem-do-humor-aquoso-em-olhos-de-porco/\n",
    "# # non-English\n",
    "# # 'webview.isho.jp', \n",
    "# # https://webview.isho.jp/openurl?rft.genre=article&rft.issn=1881-6096&rft.volume=63&rft.issue=5&rft.spage=473\n",
    "# # non-English\n",
    "# # 'www.degruyter.com', \n",
    "# # https://www.degruyter.com/document/doi/10.1515/REVNEURO.1998.9.4.291/html\n",
    "# # full text not available\n",
    "# # 'www.taylorfrancis.com', \n",
    "# # https://www.taylorfrancis.com/books/edit/10.4324/9780203449226/attention-action-glyn-humphreys-jane-riddoch,www.taylorfrancis.com\n",
    "# # book\n",
    "# # 'www.ajtmh.org', \n",
    "# # https://www.ajtmh.org/view/journals/tpmd/60/3/article-p338.xml\n",
    "# # full text not available\n",
    "# # 'literatumonline.com'\n",
    "# # https://nrc-prod.literatumonline.com/doi/10.1139/y94-079\n",
    "# # full text not available\n",
    "# # www.architalbiol.org\n",
    "# # https://www.architalbiol.org/index.php/aib/article/view/122237/\n",
    "# # full text not available \n",
    "# websites_to_remove = [\n",
    "#     'pubmed.ncbi.nlm.nih.gov', 'nan', 'psycnet.apa.org', 'content.iospress.com', 'www.worldscientific.com', 'www.liebertpub.com', \n",
    "#     'opg.optica.org', 'neurologia.com', 'pubs.aip.org', 'journal.psych.ac.cn', 'open.bu.edu', 'wakespace.lib.wfu.edu', \n",
    "#     'symposium.cshlp.org', 'www.jstor.org', 'www.rbojournal.org', 'webview.isho.jp', 'www.degruyter.com', 'www.taylorfrancis.com', \n",
    "#     'www.ajtmh.org', 'literatumonline.com', 'www.architalbiol.org'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# websites = [i for i in websites if i not in websites_to_remove]\n",
    "# print(websites)\n",
    "# # ['www.ncbi.nlm.nih.gov', 'linkinghub.elsevier.com', 'wiley.com', 'link.springer.com', 'journals.physiology.org', \n",
    "# # 'academic.oup.com', 'www.cambridge.org', 'karger.com', 'journals.lww.com', 'www.nature.com', 'www.science.org', \n",
    "# # 'www.tandfonline.com', 'journals.sagepub.com', 'jamanetwork.com', 'neurology.org', 'www.biorxiv.org', \n",
    "# # 'europepmc.org', 'iovs.arvojournals.org', 'royalsocietypublishing.org', 'psychiatryonline.org', 'direct.mit.edu', \n",
    "# # 'www.jstage.jst.go.jp', 'thejns.org', 'www.annualreviews.org', 'aspetjournals.org', 'jnm.snmjournals.org', \n",
    "# # 'www.ahajournals.org', 'pubs.acs.org', 'www.thieme-connect.de', 'pubs.asahq.org', \n",
    "# # 'www.ingentaconnect.com', 'ujms.net', 'journals.biologists.com', 'www.microbiologyresearch.org', \n",
    "# # 'journals.aps.org', 'www.imrpress.com', 'www.researchsquare.com', 'ieeexplore.ieee.org', 'papers.ssrn.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract and filling info to construct litera_db\n",
    "# input_path = fpath.poten_litera_ids_ftl_filled_filtered\n",
    "# output_path = fpath.poten_litera_litera_db\n",
    "\n",
    "# # clear file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# info_filling(input_path, output_path, 0, 10980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # process the resuls: remove duplicates and reset index\n",
    "# input_path = fpath.poten_litera_litera_db\n",
    "# output_path = fpath.poten_litera_litera_db\n",
    "\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "# identifiers = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\"]\n",
    "\n",
    "# for identifier in identifiers:\n",
    "#     remove_dup_by = identifier\n",
    "#     df = df[df[remove_dup_by].isnull() | ~df[df[remove_dup_by].notnull()].duplicated(subset=remove_dup_by, keep='last')]\n",
    "\n",
    "# # reset index\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# df.to_csv(output_path, header=False, index=False)\n",
    "# print(\"Duplication removed.\")\n",
    "\n",
    "# input_path = fpath.poten_litera_litera_db\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# print(df.shape)\n",
    "# # (10776, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # process the db so that no text contains no \",\" and are identified as seperators and if keywords starts with \"nan\", remove it\n",
    "# input_path = fpath.poten_litera_db\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "# for ind in df.index:\n",
    "#     if df.at[ind, \"TITLE\"] == df.at[ind, \"TITLE\"]:\n",
    "#         df.at[ind, \"TITLE\"] = df.at[ind, \"TITLE\"].replace(\",\", \";\").strip()\n",
    "#     if df.at[ind, \"ABSTRACT\"] == df.at[ind, \"ABSTRACT\"]:\n",
    "#         df.at[ind, \"ABSTRACT\"] = df.at[ind, \"ABSTRACT\"].replace(\",\", \";\").strip()\n",
    "#     if df.at[ind, \"KEYWORDS\"] == df.at[ind, \"KEYWORDS\"]:\n",
    "#         k = df.at[ind, \"KEYWORDS\"].replace(\",\", \";\").strip()\n",
    "#         if k.startswith(\"nan\"):\n",
    "#             k = k.split(\"nan;\")[1].strip()\n",
    "#         df.at[ind, \"KEYWORDS\"] = k\n",
    "\n",
    "# df.to_csv(input_path, header=False, index=False)\n",
    "\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# print(df.shape)\n",
    "# print(df.head(5))\n",
    "# # (10776, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 3. Download full text (pdf/json) and extract text </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # list all pdf source\n",
    "# input_path = fpath.poten_litera_litera_db\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "# pdf_source_set = set(df['PDF_SOURCE'].tolist())\n",
    "# print(pdf_source_set)\n",
    "# # {'www.ahajournals.org', 'anatomypubs.onlinelibrary.wiley.com', 'citeseerx.ist.psu.edu', 'www.nature.com', \n",
    "# # 'iovs.arvojournals.org', 'www.microbiologyresearch.org', 'nyaspubs.onlinelibrary.wiley.com', 'ahuman.org', \n",
    "# # 'karger.com', 'www.imrpress.com', 'www.researchsquare.com', 'link.springer.com', 'www.ijpp.com', \n",
    "# # 'europepmc.org', nan, 'www.cell.com', 'www.bu.edu', 'www.ncbi.nlm.nih.gov', 'jamanetwork.com', \n",
    "# # 'www.thieme-connect.de', 'www.science.org', 'physoc.onlinelibrary.wiley.com', 'deepblue.lib.umich.edu', \n",
    "# # 'bpb-us-e1.wpmucdn.com', 'www.researchgate.net', 'ieeexplore.ieee.org', 'zsp.com.pk', 'journals.biologists.com', \n",
    "# # 'journals.aps.org', 'papers.ssrn.com', 'academic.oup.com', 'onlinelibrary.wiley.com', 'www.hifo.uzh.ch', \n",
    "# # 'royalsocietypublishing.org', 'www.biorxiv.org', 'www.ingentaconnect.com', 'ujms.net', 'enpubs.faculty.ucdavis.edu', \n",
    "# # 'ajp.psychiatryonline.org', 'n.neurology.org', 'www.annualreviews.org', 'ruor.uottawa.ca', 'neuro.psychiatryonline.org', \n",
    "# # 'www.jstage.jst.go.jp', 'synapse.koreamed.org', 'journals.physiology.org', 'linkinghub.elsevier.com', \n",
    "# # 'www.tandfonline.com', 'www.jneurosci.org', 'analyticalsciencejournals.onlinelibrary.wiley.com', 'pubs.asahq.org', \n",
    "# # 'thejns.org', 'biomedical-engineering-online.biomedcentral.com', 'journals.sagepub.com', 'direct.mit.edu', \n",
    "# # 'pubs.acs.org', 'pharmrev.aspetjournals.org', 'journals.lww.com', 'jnm.snmjournals.org', 'jpet.aspetjournals.org', \n",
    "# # 'movementdisorders.onlinelibrary.wiley.com'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # list the pdf_source and prepare the test code for downloading the pdfs\n",
    "# pdf_source_set = [\n",
    "#     'www.ahajournals.org', 'wiley.com', 'citeseerx.ist.psu.edu', 'www.nature.com', \n",
    "#     'iovs.arvojournals.org', 'www.microbiologyresearch.org', 'ahuman.org', \n",
    "#     'karger.com', 'www.imrpress.com', 'www.researchsquare.com', 'link.springer.com', 'www.ijpp.com', \n",
    "#     'europepmc.org', 'www.cell.com', 'www.bu.edu', 'www.ncbi.nlm.nih.gov', 'jamanetwork.com', \n",
    "#     'www.thieme-connect.de', 'www.science.org', 'deepblue.lib.umich.edu', \n",
    "#     'bpb-us-e1.wpmucdn.com', 'www.researchgate.net', 'ieeexplore.ieee.org', 'zsp.com.pk', 'journals.biologists.com', \n",
    "#     'journals.aps.org', 'papers.ssrn.com', 'academic.oup.com', 'www.hifo.uzh.ch', \n",
    "#     'royalsocietypublishing.org', 'www.biorxiv.org', 'www.ingentaconnect.com', 'ujms.net', 'enpubs.faculty.ucdavis.edu', \n",
    "#     'psychiatryonline.org', 'n.neurology.org', 'www.annualreviews.org', 'ruor.uottawa.ca', \n",
    "#     'www.jstage.jst.go.jp', 'synapse.koreamed.org', 'journals.physiology.org', 'linkinghub.elsevier.com', \n",
    "#     'www.tandfonline.com', 'www.jneurosci.org', 'pubs.asahq.org', \n",
    "#     'thejns.org', 'biomedcentral.com', 'journals.sagepub.com', 'direct.mit.edu', \n",
    "#     'pubs.acs.org', 'aspetjournals.org', 'journals.lww.com', 'jnm.snmjournals.org'\n",
    "# ]\n",
    "\n",
    "# input_path = fpath.poten_litera_litera_db\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "# for website in pdf_source_set:\n",
    "#     for ind in df.index:\n",
    "#         if df.at[ind, \"PDF_SOURCE\"] != df.at[ind, \"PDF_SOURCE\"]:\n",
    "#             continue\n",
    "#         if website in df.at[ind, \"PDF_SOURCE\"]:\n",
    "#             print(\"# \" + website)\n",
    "#             print(\"\\\"\" + df.at[ind, \"PDF_URL\"] + \"\\\"\")\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_path = fpath.poten_litera_litera_db\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# print(df.shape)\n",
    "# print(df.head(5))\n",
    "# # (10776, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download pdfs and jsons, rename them to build a database\n",
    "# input_path = fpath.poten_litera_db\n",
    "# pdf_folder = fpath.pdf_folder\n",
    "# download_pdf(input_path, pdf_folder, 0, 10776)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the downloaded pdfs and jsons\n",
    "def test_pdf(pdf_path):       \n",
    "    # opens the file for reading\n",
    "    with open(pdf_path, 'rb') as p:\n",
    "        txt = (p.readlines())\n",
    "\n",
    "    actual_line = len(txt)\n",
    "    \n",
    "    for i, x in enumerate(txt[::-1]):\n",
    "        if b'%%EOF' in x:\n",
    "            actual_line = len(txt)-i\n",
    "            # print(f'EOF found at line position {-i} = actual {actual_line}, with value {x}')\n",
    "            break\n",
    "    \n",
    "    if actual_line != len(txt):\n",
    "        # get the new list terminating correctly\n",
    "        txtx = txt[:actual_line]\n",
    "\n",
    "        # write to new pdf\n",
    "        with open(pdf_path, 'wb') as f:\n",
    "            f.writelines(txtx)\n",
    "        f.close()\n",
    "\n",
    "    fixed_pdf = PyPDF2.PdfReader(pdf_path)\n",
    "\n",
    "    page_max = len(fixed_pdf.pages)\n",
    "\n",
    "    if page_max < 5:\n",
    "        print(page_max)\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "# --------------------start of test code--------------------\n",
    "# input_path = fpath.poten_litera_db\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "# pdf_folder = fpath.pdf_folder\n",
    "\n",
    "# start = 0\n",
    "# end = 10776\n",
    "\n",
    "# for ind in range(start, end):\n",
    "    # index = str(int(df.at[ind, \"INDEX\"]))\n",
    "    # pdf_file_name = str(index) + \".pdf\"\n",
    "    # json_file_name = str(index) + \".json\"\n",
    "    # pdf_path = os.path.join(pdf_folder, pdf_file_name)\n",
    "    # json_path = os.path.join(pdf_folder, json_file_name)\n",
    "\n",
    "    # try:\n",
    "    #     if os.path.exists(json_path):\n",
    "    #         print(ind, index)\n",
    "    #         continue\n",
    "    #     elif os.path.exists(pdf_path):\n",
    "    #         if test_pdf(pdf_path):\n",
    "    #             print(ind, index)\n",
    "    #         else:\n",
    "    #             print(\"\\n\")\n",
    "    #             print(\"PDF LEGNTH < 3\")\n",
    "    #             print(df.at[ind, \"INDEX\"], df.at[ind, \"DOI\"], df.at[ind, \"PMID\"], df.at[ind, \"PMCID\"])\n",
    "    #             print(df.at[ind, \"FULL_TEXT_URL\"], df.at[ind, \"FULL_TEXT_SOURCE\"])\n",
    "    #             print(df.at[ind, \"PDF_URL\"], df.at[ind, \"PDF_SOURCE\"])\n",
    "    #             print(ind, index)\n",
    "    #             print(\"\\n\")\n",
    "    #     else:\n",
    "    #         print(\"\\n\")\n",
    "    #         print(\"PDF NOT AVAILABLE\")\n",
    "    #         print(df.at[ind, \"INDEX\"], df.at[ind, \"DOI\"], df.at[ind, \"PMID\"], df.at[ind, \"PMCID\"])\n",
    "    #         print(df.at[ind, \"FULL_TEXT_URL\"], df.at[ind, \"FULL_TEXT_SOURCE\"])\n",
    "    #         print(df.at[ind, \"PDF_URL\"], df.at[ind, \"PDF_SOURCE\"])\n",
    "    #         print(ind, index)\n",
    "    #         print(\"\\n\")\n",
    "    # except:\n",
    "    #     print(\"\\n\")\n",
    "    #     print(\"PDF Corrupted\")\n",
    "    #     print(df.at[ind, \"INDEX\"], df.at[ind, \"DOI\"], df.at[ind, \"PMID\"], df.at[ind, \"PMCID\"])\n",
    "    #     print(df.at[ind, \"FULL_TEXT_URL\"], df.at[ind, \"FULL_TEXT_SOURCE\"])\n",
    "    #     print(df.at[ind, \"PDF_URL\"], df.at[ind, \"PDF_SOURCE\"])\n",
    "    #     print(ind, index)\n",
    "    #     print(\"\\n\")\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # copy the relevant and not relevant pdfs and jsons from \"pdfs\" to repective folders \"relevant_pdfs\" and \"not_relevant_pdfs\"\n",
    "# test_path = fpath.poten_litera_testing_set_300_read_index_corrected\n",
    "# destination1 = \"/media/hou/DIDIHOU/relevant_pdfs\"\n",
    "# destination2 = \"/media/hou/DIDIHOU/not_relevant_pdfs\"\n",
    "\n",
    "# df_test = pd.read_csv(test_path, header=0, sep=',')\n",
    "# df_test.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANT\"]\n",
    "\n",
    "# for ind in df_test.index:\n",
    "#     index = df_test.at[ind, \"INDEX\"]\n",
    "#     if df_test.at[ind, \"RELEVANT\"] == \"YES\": # relevant\n",
    "#         flag = False\n",
    "#         print(ind, index)\n",
    "        \n",
    "#         json_path = os.path.join(fpath.pdf_folder, str(index) + \".json\")\n",
    "#         pdf_path = os.path.join(fpath.pdf_folder, str(index) + \".pdf\")\n",
    "        \n",
    "#         if os.path.exists(json_path):\n",
    "#             shutil.copy(json_path, destination1)\n",
    "#             flag = True\n",
    "#         if os.path.exists(pdf_path):\n",
    "#             shutil.copy(pdf_path, destination1)\n",
    "#             flag = True\n",
    "        \n",
    "#         if not flag:\n",
    "#             print(\"No file found for index: \", index)\n",
    "#     else: # not relevant\n",
    "#         json_path = os.path.join(fpath.pdf_folder, str(index) + \".json\")\n",
    "#         pdf_path = os.path.join(fpath.pdf_folder, str(index) + \".pdf\")\n",
    "\n",
    "#         if os.path.exists(json_path):\n",
    "#             shutil.copy(json_path, destination2)\n",
    "#         if os.path.exists(pdf_path):\n",
    "#             shutil.copy(pdf_path, destination2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract text to store to a text file, record the literatures whose pdfs or jsons are not available\n",
    "# input_path = fpath.poten_litera_db\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "# # poten_litera_pdf_not_available = fpath.poten_litera_pdf_not_available\n",
    "# # plib.clear_file(poten_litera_pdf_not_available)\n",
    "\n",
    "# pdf_folder = fpath.pdf_folder\n",
    "# text_folder = fpath.text_folder\n",
    "# # text_length_to_extract = params.text_length_to_extract\n",
    "\n",
    "# for ind in df.index:\n",
    "#     # time.sleep(1)\n",
    "#     index = str(int(df.at[ind, \"INDEX\"]))\n",
    "#     pdf_file_name = str(index) + \".pdf\"\n",
    "#     json_file_name = str(index) + \".json\"\n",
    "#     pdf_path = os.path.join(pdf_folder, pdf_file_name)\n",
    "#     json_path = os.path.join(pdf_folder, json_file_name)\n",
    "#     text_path = os.path.join(text_folder, str(index) + \".txt\")\n",
    "    \n",
    "#     if os.path.exists(json_path):\n",
    "#         # pass\n",
    "#         json2text(json_path, text_path)\n",
    "#     elif os.path.exists(pdf_path):\n",
    "#         # pass\n",
    "#         pdf2text(pdf_path, text_path)\n",
    "#     else:\n",
    "#         # selected_row = df.iloc[[ind]]\n",
    "#         # selected_row.to_csv(poten_litera_pdf_not_available, mode='a', header=False, index=False)\n",
    "#         print(\"\\n\")\n",
    "#         print(df.at[ind, \"INDEX\"], df.at[ind, \"DOI\"], df.at[ind, \"PMID\"], df.at[ind, \"PMCID\"])\n",
    "#         print(df.at[ind, \"FULL_TEXT_URL\"], df.at[ind, \"FULL_TEXT_SOURCE\"])\n",
    "#         print(df.at[ind, \"PDF_URL\"], df.at[ind, \"PDF_SOURCE\"])\n",
    "#         print(\"\\n\")\n",
    "    \n",
    "#     print(ind, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # count the number of literatures whose pdfs or jsons are not available\n",
    "# input_path = fpath.poten_litera_pdf_not_available\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# print(df.shape)\n",
    "# # (406, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4> 4. Training and testing data set split </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # select 300 random papers from poten_litera_ids_ftl_filled_filtered for testing\n",
    "# source_path = fpath.poten_litera_ids_ftl_filled_filtered\n",
    "# output_path = fpath.poten_litera_testing_set_300\n",
    "\n",
    "# # clear the file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "# df = df.sample(n=300, random_state=1, axis='index', ignore_index=False)\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df.to_csv(output_path, header=True, index=False)\n",
    "# # --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_testing_set_300\n",
    "# df = pd.read_csv(source_path, header=0, sep=',')\n",
    "# print(df.shape)\n",
    "# # (300, 12)\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # process, correct the INDEX in potential_related_literature_testing_set_300_read.csv\n",
    "# input_path = fpath.poten_litera_testing_set_300_read\n",
    "# output_path = fpath.poten_litera_testing_set_300_read_index_corrected\n",
    "# # plib.clear_file(output_path)\n",
    "# db_path = fpath.poten_litera_db\n",
    "\n",
    "# df_input = pd.read_csv(input_path, header=0, sep=',')\n",
    "# df_input.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANCE\"]\n",
    "# df_db = pd.read_csv(db_path, header=None, sep=',')\n",
    "# df_db.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "# df_db = df_db.fillna(0)\n",
    "# df_db = df_db.astype({\"PMID\": int})\n",
    "# # print(df_input.shape)\n",
    "# # print(df_input.head(5))\n",
    "# # (300, 12)\n",
    "# # print(df_db.shape)\n",
    "# # print(df_db.head(10))\n",
    "# # (10776, 11)\n",
    "\n",
    "# for ind in df_input.index:\n",
    "#     index = df_input.at[ind, \"INDEX\"]\n",
    "#     doi = df_input.at[ind, \"DOI\"]\n",
    "#     pmid = df_input.at[ind, \"PMID\"]\n",
    "#     # print(pmid, df_db.at[ind, \"PMID\"])\n",
    "#     # print(pmid.type(), df_db.at[ind, \"PMID\"].type())\n",
    "#     pmcid = df_input.at[ind, \"PMCID\"]\n",
    "#     full_text_url = df_input.at[ind, \"FULL_TEXT_URL\"]\n",
    "#     full_text_source = df_input.at[ind, \"FULL_TEXT_SOURCE\"]\n",
    "#     title = df_input.at[ind, \"TITLE\"].lower()\n",
    "\n",
    "#     if doi == doi:\n",
    "#         try:\n",
    "#             index = df_db.loc[df_db[\"DOI\"] == doi, 'INDEX'].values[0]\n",
    "#             df_input.at[ind, \"INDEX\"] = index\n",
    "#         except:\n",
    "#             print(\"DOI not found in db:\", df_input.at[ind, \"INDEX\"], df_input.at[ind, \"RELEVANCE\"])\n",
    "#             df_input.drop(ind, inplace=True)\n",
    "#     elif pmid == pmid:\n",
    "#         try:\n",
    "#             index = df_db.loc[df_db[\"PMID\"]==int(pmid), 'INDEX'].values[0]\n",
    "#             df_input.at[ind, \"INDEX\"] = index\n",
    "#         except:\n",
    "#             print(\"PMID not found in db:\", df_input.at[ind, \"INDEX\"], df_input.at[ind, \"RELEVANCE\"])\n",
    "#             df_input.drop(ind, inplace=True)\n",
    "#     elif pmcid == pmcid:\n",
    "#         index = df_db.loc[df_db[\"PMCID\"] == pmcid, 'INDEX'].values[0]\n",
    "#         df_input.at[ind, \"INDEX\"] = index\n",
    "#     elif title.lower() == title.lower():\n",
    "#         index = df_db.loc[df_db[\"TITLE\"].str.lower() == title, 'INDEX'].values[0]\n",
    "#         df_input.at[ind, \"INDEX\"] = index\n",
    "#     else:\n",
    "#         print(\"ALL 4 identifiers and title are missing:\", df_input.at[ind, \"INDEX\"], df_input.at[ind, \"RELEVANCE\"])\n",
    "    \n",
    "# df_input.reset_index(drop=True, inplace=True)\n",
    "# df_input.to_csv(output_path, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check the corrected file\n",
    "# corrected = fpath.poten_litera_testing_set_300_read_index_corrected\n",
    "# df_input = pd.read_csv(corrected, header=0, sep=',')\n",
    "# print(df_input.shape)\n",
    "# # (292, 12)\n",
    "# print(df_input.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test if the result matches the db\n",
    "# result_path = fpath.poten_litera_testing_set_300_read_index_corrected\n",
    "# db_path = fpath.poten_litera_db\n",
    "\n",
    "# df_result= pd.read_csv(result_path, header=0, sep=',')\n",
    "# df_result.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANCE\"]\n",
    "# df_db = pd.read_csv(db_path, header=None, sep=',')\n",
    "# df_db.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "# # print(df_result.shape)\n",
    "# # print(df_result.head(5))\n",
    "# # (292, 12)\n",
    "# # print(df_db.shape)\n",
    "# # print(df_db.head(10))\n",
    "# # (10776, 11)\n",
    "\n",
    "# for ind in df_result.index:\n",
    "#     index = int(df_result.at[ind, \"INDEX\"])\n",
    "#     title = df_result.at[ind, \"TITLE\"].lower()\n",
    "#     # title = ''.join([char for char in df_result.at[ind, \"TITLE\"].lower() if re.match(r'[a-z\\s-]', char)])\n",
    "#     cleaned_title = re.sub(r'\\s+', ' ', title).strip().replace(\".\", \"\")\n",
    "#     title_db = df_db.loc[df_db[\"INDEX\"].astype(int) == index, 'TITLE'].values[0].lower()\n",
    "#     # title_db = ''.join([char for char in df_db.loc[df_db[\"INDEX\"].astype(int) == index, 'TITLE'].values[0].lower() if re.match(r'[a-z\\s-]', char)])\n",
    "#     cleaned_title_db = re.sub(r'\\s+', ' ', title_db).strip().replace(\".\", \"\")\n",
    "    \n",
    "#     if cleaned_title == cleaned_title_db:\n",
    "#         pass\n",
    "#     else:\n",
    "#         # pass\n",
    "#         print(index)\n",
    "#         print(cleaned_title)\n",
    "#         print(cleaned_title_db)\n",
    "#         print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # select another 708 random papers from poten_litera_db to form 1000 papers as training and testing set\n",
    "# db_path = fpath.poten_litera_db\n",
    "# test_300_path = fpath.poten_litera_testing_set_300_read_index_corrected\n",
    "# test_708_path = fpath.poten_litera_testing_set_708\n",
    "# plib.clear_file(test_708_path)\n",
    "\n",
    "# df_db = pd.read_csv(db_path, header=None, sep=',')\n",
    "# df_db.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "# df_300 = pd.read_csv(test_300_path, header=0, sep=',')\n",
    "# df_300.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANCE\"]\n",
    "\n",
    "# # Get the indices of the previously selected 300 rows\n",
    "# selected_indices = df_300['INDEX'].values\n",
    "# # print(selected_indices)\n",
    "\n",
    "# # Drop the previously selected 300 rows from the original dataframe\n",
    "# df_remaining = df_db[~df_db['INDEX'].isin(selected_indices)]\n",
    "\n",
    "# # Randomly sample 708 rows from the remaining rows\n",
    "# df_708 = df_remaining.sample(n=708, random_state=42)  # Change random_state if needed\n",
    "# df_708['RELEVANCE'] = np.nan\n",
    "# df_708.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# df_708.to_csv(test_708_path, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine and obtain 1000 papers as training and testing set\n",
    "# test_300_path = fpath.poten_litera_testing_set_300_read_index_corrected\n",
    "# test_708_path = fpath.poten_litera_testing_set_708\n",
    "# test_1000_path = fpath.poten_litera_testing_set_1000\n",
    "# plib.clear_file(test_1000_path)\n",
    "\n",
    "# df_300 = pd.read_csv(test_300_path, header=0, sep=',')\n",
    "# df_708 = pd.read_csv(test_708_path, header=0, sep=',')\n",
    "# df_1000 = pd.concat([df_300, df_708], axis=0)\n",
    "# df_1000.reset_index(drop=True, inplace=True)\n",
    "# df_1000.to_csv(test_1000_path, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if there're duplicates in the 1000 papers\n",
    "# test_1000_path = fpath.poten_litera_testing_set_1000\n",
    "# df_1000 = pd.read_csv(test_1000_path, header=0, sep=',')\n",
    "# df_1000.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANCE\"]\n",
    "# print(df_1000.shape)\n",
    "# # (1000, 12)\n",
    "\n",
    "# print(len(set(df_1000['INDEX'])))\n",
    "# # 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test if the result matches the db\n",
    "# test_1000_path = fpath.poten_litera_testing_set_1000\n",
    "# db_path = fpath.poten_litera_db\n",
    "\n",
    "# df_result= pd.read_csv(test_1000_path, header=0, sep=',')\n",
    "# df_result.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANCE\"]\n",
    "# df_db = pd.read_csv(db_path, header=None, sep=',')\n",
    "# df_db.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "# # print(df_result.shape)\n",
    "# # print(df_result.head(5))\n",
    "# # # (1000, 12)\n",
    "# # print(df_db.shape)\n",
    "# # print(df_db.head(10))\n",
    "# # # (10776, 11)\n",
    "\n",
    "# for ind in df_result.index:\n",
    "#     index = int(df_result.at[ind, \"INDEX\"])\n",
    "#     title = df_result.at[ind, \"TITLE\"].lower()\n",
    "#     # title = ''.join([char for char in df_result.at[ind, \"TITLE\"].lower() if re.match(r'[a-z\\s-]', char)])\n",
    "#     cleaned_title = re.sub(r'\\s+', ' ', title).strip().replace(\".\", \"\")\n",
    "#     title_db = df_db.loc[df_db[\"INDEX\"].astype(int) == index, 'TITLE'].values[0].lower()\n",
    "#     # title_db = ''.join([char for char in df_db.loc[df_db[\"INDEX\"].astype(int) == index, 'TITLE'].values[0].lower() if re.match(r'[a-z\\s-]', char)])\n",
    "#     cleaned_title_db = re.sub(r'\\s+', ' ', title_db).strip().replace(\".\", \"\")\n",
    "    \n",
    "#     if cleaned_title == cleaned_title_db:\n",
    "#         pass\n",
    "#     else:\n",
    "#         # pass\n",
    "#         print(index)\n",
    "#         print(cleaned_title)\n",
    "#         print(cleaned_title_db)\n",
    "#         print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # draw the distribution of the sampled 1000 papers to see if it's actually randomly sampled\n",
    "# test_1000_path = fpath.poten_litera_testing_set_1000\n",
    "# df_1000 = pd.read_csv(test_1000_path, header=0, sep=',')\n",
    "# df_1000.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANCE\"]\n",
    "\n",
    "# index_list = df_1000['INDEX'].tolist()\n",
    "# index_list.sort()\n",
    "# # print(index_list)\n",
    "# print(len(index_list))\n",
    "\n",
    "# # draw the histogram\n",
    "# plt.hist(index_list, bins=10)\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.title(\"Distribution of the sampled 1000 papers\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Next step: automatic filtering </h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
