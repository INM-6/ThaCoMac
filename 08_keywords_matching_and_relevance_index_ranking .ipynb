{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea1cce5-4f07-4bd7-8ca3-5f6fa51254d0",
   "metadata": {},
   "source": [
    "# Keywords matching and relevance index ranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5a61c5-f8c6-418a-b450-cdea0378ddab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from nltk import ngrams\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726b12c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib\n",
    "import parameters as params\n",
    "import dataframe_columns as df_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489f0451",
   "metadata": {},
   "source": [
    "## Predefined fucntions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(count):\n",
    "    # transform the count\n",
    "    return math.log(1 + count)\n",
    "# # --------------------start of test code--------------------\n",
    "# elem = 0\n",
    "# print(transform(10))\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_TT(index: int):\n",
    "    text_path = os.path.join(fpath.text_folder, str(index) + \".txt\")\n",
    "    \n",
    "    if os.path.exists(text_path):\n",
    "        with open(text_path, 'r', encoding=\"ascii\") as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        text = plib.process_text(text, lower=True)\n",
    "    \n",
    "        if len(text.split()) >= 4000 and (\"inject\" not in text):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "# # --------------------start of test code--------------------\n",
    "# index = 72\n",
    "# if_not_TT(index)\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_tak(ind):\n",
    "    df_db = pd.read_csv(fpath.poten_litera_db, header=0, sep=\"\\t\")\n",
    "    \n",
    "    tak = \"\"\n",
    "    \n",
    "    title = df_db.at[ind, \"TITLE\"]\n",
    "    abstract = df_db.at[ind, \"ABSTRACT\"]\n",
    "    keywords = df_db.at[ind, \"KEYWORDS\"]\n",
    "    \n",
    "    if title != title:\n",
    "        title = \"\"\n",
    "    \n",
    "    if abstract != abstract:\n",
    "        abstract = \"\"\n",
    "        \n",
    "    if keywords != keywords:\n",
    "        keywords = \"\"\n",
    "    \n",
    "    # if abstract is empty, then tak is empty\n",
    "    if abstract != abstract or abstract == \"\":\n",
    "        tak = \"\"\n",
    "    else:\n",
    "        tak = title + \" \" + abstract + \" \" + keywords\n",
    "    \n",
    "    # process title and tak\n",
    "    title = plib.process_text(title, lower=True)\n",
    "    tak = plib.process_text(tak, lower=True)\n",
    "        \n",
    "    return title, tak\n",
    "# # --------------------start of test code--------------------\n",
    "# ind = 1\n",
    "# title, tak = get_tak(ind)\n",
    "# print title\n",
    "# print tak\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_in_text(keyword, text):\n",
    "    # process keyword and text\n",
    "    keyword = plib.process_text(keyword, lower=True)\n",
    "    text = plib.process_text(text, lower=True)\n",
    "    \n",
    "    # if keyword is in exact match keyword list\n",
    "    if keyword in params.exact_match_kw_list:\n",
    "        # length of the keyword\n",
    "        len_word = len(keyword.split())\n",
    "        \n",
    "        # word tokenization\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # get the ngrams of length len_word from text\n",
    "        ng = list(ngrams(tokens, len_word))\n",
    "        words = [' '.join(gram) for gram in ng]\n",
    "        # print(words)\n",
    "\n",
    "        for w in words:\n",
    "            if keyword == w:\n",
    "                return True\n",
    "    # if keyword is not in exact match keyword list\n",
    "    else:\n",
    "        if keyword in text:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "# # --------------------start of test code--------------------\n",
    "# keyword = \"new world titi monkey\"\n",
    "# text = \"There cat new new world titi monkeyworld monkey rodent is an owl monkey, a spider monkey, some howler monkeys, and then just a monkey playing around.\"\n",
    "# if_keyword_in_text(keyword, text)\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_keywords_from_text(text, keywords):\n",
    "    # Iterate until no keywords are found in the text\n",
    "    while any(re.search(r'\\b' + re.escape(keyword) + r's?\\b', text, re.IGNORECASE) for keyword in keywords):\n",
    "        for keyword in keywords:\n",
    "            # Use regular expression to remove both singular and plural forms of the keyword\n",
    "            text = re.sub(r'\\b' + re.escape(keyword) + r's?\\b', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "            # Remove extra spaces left after removal\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "# # --------------------start of test code--------------------\n",
    "# # Example text\n",
    "# text = \"There is an owl monkeys, a macaque monkeys, and a capuchin monkey in the zoos. 'cat', 'cats', 'rat', 'rats', 'mouse', 'mice', 'marmoset monkey', 'marmoset', 'human', 'humans', 'man', 'men', 'dog', 'dogs', 'rabbit', 'sheep', 'frog', 'frogs', 'squirrel monkey', 'saimiri sciureus', 'hedgehog', 'erinaceus europaeus', 'callithrix jacchus', 'hamster', 'phodopus sungorus', 'pig', 'pigs', 'minipig', 'cavia aperta', 'wallaby', 'macropus eugenii', 'bat', 'bats', 'otolemur garnetti', 'new world titi monkey', 'callicebus moloch', 'owl monkey', 'aotus trivirgatus', 'new world monkey', 'ferret', 'ferrets', 'bush baby', 'galago', 'galagos', 'rodent', 'rodents', 'cebus apella', 'cebus monkey', 'chicken', 'possum', 'trichosurus vulpecula', 'prosimian', 'antrozous pallidus', 'shrew', 'shrews', 'suncus murinus', 'gerbil', 'gerbils', 'swine', 'swines',\"\n",
    "\n",
    "# # List of keywords to remove\n",
    "# keywords = params.other_spiecies_group\n",
    "\n",
    "# # Remove keywords from the text\n",
    "# updated_text = remove_keywords_from_text(text, keywords)\n",
    "\n",
    "# print(updated_text)\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_macaque_but_others(tak):\n",
    "    # return True if the text is not about macaque but other species\n",
    "    # return False for other cases\n",
    "    \n",
    "    # if tak is empty\n",
    "    if tak == \"\":\n",
    "        return False\n",
    "    \n",
    "    # process text\n",
    "    tak = plib.process_text(tak, lower=True)\n",
    "    \n",
    "    macaque_label = False\n",
    "    other_species_label = False\n",
    "     \n",
    "    # other species   \n",
    "    for keyword in params.other_spiecies_group:\n",
    "        if keyword_in_text(keyword, tak):\n",
    "            other_species_label = True\n",
    "    \n",
    "    text = remove_keywords_from_text(tak, params.other_spiecies_group)\n",
    "    # print(text)\n",
    "    \n",
    "    # macaque\n",
    "    for word in params.possible_macaque_group:\n",
    "        if keyword_in_text(word, text):\n",
    "            macaque_label = True\n",
    "    \n",
    "    if (not macaque_label) and other_species_label:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "# # --------------------start of test code--------------------\n",
    "# tak = \"The  different squirrel monkeys prefrontal cortical regions exert executive control over processing occurring in posterior cortical regions. We examined with the autoradiographic method, in the, the course and terminations of the efferent corticocortical connections of the rostral prefrontal region, the function of which is least understood. Three efferent streams of fibers organized into three distinct fasciculi convey rostral prefrontal influences on posterior cortical areas. These connections provide powerful insights into the cortical regions on which executive control is being exercised. The lateral stream of fibers via the extreme capsule targets the midsection of the auditory superior temporal region and the multisensory areas of the superior temporal sulcus, thus permitting control over the most integrated aspects of cognitive processing. The fibers coursing through the extreme capsule originating in areas 10 and 9 continue as part of the white matter of the superior temporal gyrus (i.e., the middle longitudinal fasciculus) to target the midportion of the superior temporal gyrus (areas TAa, TS2, and TS3) and adjacent multisensory area TPO within the upper bank of the superior temporal sulcus. Some of the fibers from areas 10 and 9 that enter the extreme capsule terminate in the ventral part of the insula. The dorsomedial limbic stream via the cingulate fasciculus targets the anterior and posterior cingulate cortex, as well as the retrosplenial cortex, allowing control over motivational and memory processes. A ventral limbic stream via the uncinate fasciculus targets the temporal proisocortex and the amygdala, indicating an additional powerful influence over the emotional motivational sphere.\"\n",
    "# not_macaque_but_others(tak)\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02fb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance_index(not_TT, not_macaque_but_others, count_dict):\n",
    "    relevance_index = 0\n",
    "    \n",
    "    for key in count_dict.keys():\n",
    "        relevance_index += transform(count_dict[key]) * (params.ranking_kw_groups_weights[key])\n",
    "    \n",
    "    if not_TT == \"Y\" or not_macaque_but_others == \"Y\":\n",
    "        relevance_index = 0.1 * relevance_index\n",
    "        \n",
    "    return relevance_index\n",
    "# --------------------start of test code--------------------\n",
    "# keywords_count_or_fre = {}\n",
    "# index = calcul_related(keywords_count_or_fre, params.on_topic_kws_weights)\n",
    "# print(index)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edde53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_relevance_index(db_count_path, db_relevance_index_path, rank_by):\n",
    "    df = pd.read_csv(db_count_path, header=0, sep=\"\\t\")\n",
    "    \n",
    "    if rank_by == 'tak':\n",
    "        column_name = '_COUNT_IN_TAK'\n",
    "    elif rank_by == '500':\n",
    "        column_name = '_COUNT_IN_500'\n",
    "    elif rank_by == 'full_text':\n",
    "        column_name = '_COUNT_IN_FULL_TEXT'\n",
    "        \n",
    "    count_dict = {}\n",
    "    \n",
    "    for ind in df.index:\n",
    "        # index = int(df.at[ind, \"INDEX\"])\n",
    "        \n",
    "        not_TT = df.at[ind, \"NOT_TT\"]\n",
    "        not_macaque_but_others = df.at[ind, \"NOT_MACAQUE_BUT_OTHERS\"] \n",
    "        \n",
    "        for key in params.ranking_kw_groups.keys():\n",
    "            # if key == 'INJECT' or 'METHOD':\n",
    "            #     value = df.at[ind, key+'_COUNT_IN_FULL_TEXT']\n",
    "            # else:\n",
    "            #     value = df.at[ind, key+column_name]\n",
    "            value = df.at[ind, key+column_name]\n",
    "            \n",
    "            if value != value:\n",
    "                value = 0\n",
    "                \n",
    "            count_dict[key] = int(value)\n",
    "        # print(count_dict)\n",
    "        \n",
    "        relev_index = relevance_index(not_TT, not_macaque_but_others, count_dict)\n",
    "        \n",
    "        df.at[ind, \"RELEVANCE_INDEX\"] = relev_index\n",
    "\n",
    "        line_number_in_csv = ind + 1\n",
    "        print(\"Line number:\", line_number_in_csv, \" INDEX:\", int(df.at[ind, \"INDEX\"]))\n",
    "    \n",
    "    df.columns = df_col.db_ranked_columns\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.to_csv(db_relevance_index_path, header=True, index=False, sep=\"\\t\")\n",
    "# --------------------start of test code--------------------\n",
    "# input_path = fpath.poten_litera_db_kw_count\n",
    "# output_path = fpath.poten_litera_db_ranked\n",
    "# rank(input_path, output_path, params.ranking_params_weights)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_and_save(input_path, output_path):\n",
    "    plib.clear_file(output_path)\n",
    "    \n",
    "    # sort\n",
    "    df_to_rank = pd.read_csv(input_path, header=0, sep=\"\\t\")\n",
    "\n",
    "    # sort by relevance index\n",
    "    df_to_rank.sort_values(by=['RELEVANCE_INDEX'], ascending=False, inplace=True)\n",
    "        \n",
    "    df_to_rank.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df_to_rank.to_csv(output_path, header=True, index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e562971",
   "metadata": {},
   "source": [
    "## Main program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns of \"NOT_TT\" and \"NOT_MACAQUE_BUT_OTHER\"\n",
    "input_path = fpath.poten_litera_db_kw_count\n",
    "df = pd.read_csv(input_path, header=0, sep=\"\\t\")\n",
    "\n",
    "output_path = fpath.poten_litera_db_kw_count_with_2_new_columns\n",
    "plib.clear_file(output_path)\n",
    "\n",
    "for ind in df.index:\n",
    "    index = int(df.at[ind, \"INDEX\"])\n",
    "    \n",
    "    title, tak = get_title_tak(ind)\n",
    "    \n",
    "    if not_TT(index):\n",
    "        df.at[ind, \"NOT_TT\"] = \"Y\"\n",
    "    \n",
    "    if not_macaque_but_others(tak):\n",
    "        df.at[ind, \"NOT_MACAQUE_BUT_OTHERS\"] = \"Y\"\n",
    "\n",
    "    line_number_in_csv = ind + 1\n",
    "    print(\"Line number:\", line_number_in_csv, \" INDEX:\", int(df.at[ind, \"INDEX\"]))\n",
    "    \n",
    "df.columns = df_col.db_ranked_columns_with_2_new_columns\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.to_csv(output_path, header=True, index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Calculate relevance index and rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_by = 'tak'\n",
    "# rank_by = '500'\n",
    "# rank_by = 'full_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute relevance index and rank\n",
    "input_path = fpath.poten_litera_db_kw_count_with_2_new_columns\n",
    "relevance_index_path = fpath.poten_litera_db_relevance_index\n",
    "plib.clear_file(relevance_index_path)\n",
    "\n",
    "# compute relevance index\n",
    "compute_relevance_index(input_path, relevance_index_path, rank_by)\n",
    "\n",
    "# rank\n",
    "if rank_by == 'tak':\n",
    "    # rank by tak    \n",
    "    rank_and_save(relevance_index_path, fpath.poten_litera_db_ranked_by_tak)\n",
    "elif rank_by == '500':\n",
    "    # rank by 500    \n",
    "    rank_and_save(relevance_index_path, fpath.poten_litera_db_ranked_by_500)\n",
    "elif rank_by == 'full_text':\n",
    "    # rank by full text\n",
    "    rank_and_save(relevance_index_path, fpath.poten_litera_db_ranked_by_full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36685b74",
   "metadata": {},
   "source": [
    "### 2. Ranking results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize lists\n",
    "\n",
    "# relevant group\n",
    "R_species = []\n",
    "R_other_species = []\n",
    "R_tc_ct = []\n",
    "R_thalam = []\n",
    "R_cortex = []\n",
    "R_inject = []\n",
    "R_method = []\n",
    "R_connectivity = []\n",
    "\n",
    "# index list of the relevant group\n",
    "R_index_list = []\n",
    "# relevance index list of the relevant group\n",
    "R_relevance_index_list = []\n",
    "\n",
    "R_review_index_list = []\n",
    "\n",
    "# Irrelevant group\n",
    "IR_species = []\n",
    "IR_other_species = []\n",
    "IR_tc_ct = []\n",
    "IR_thalam = []\n",
    "IR_cortex = []\n",
    "IR_inject = []\n",
    "IR_method = []\n",
    "IR_connectivity = []\n",
    "\n",
    "# index list of the irrelevant group\n",
    "IR_index_list = []\n",
    "# relevance index list of the irrelevant group\n",
    "IR_relevance_index_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the df_db_ranked based on the rank_by\n",
    "if rank_by == 'tak':\n",
    "    # rank by tak\n",
    "    df_db_ranked = pd.read_csv(fpath.poten_litera_db_ranked_by_tak, header=0, sep='\\t')\n",
    "    column_name_base = \"TAK\"\n",
    "# elif rank_by == '500':\n",
    "#     # rank by 500\n",
    "#     df_db_ranked = pd.read_csv(fpath.poten_litera_db_ranked_by_500, header=0, sep='\\t')\n",
    "#     column_name_base = \"500\"\n",
    "# elif rank_by == 'full_text':\n",
    "#     # rank by full_text\n",
    "#     df_db_ranked = pd.read_csv(fpath.poten_litera_db_ranked_by_full_text, header=0, sep='\\t')\n",
    "#     column_name_base = \"FULL_TEXT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the final_manually_read_csv_1_labeled\n",
    "\n",
    "# read the labeled testing set\n",
    "df = pd.read_csv(fpath.final_manually_read_csv_1_labeled, header=0, sep='\\t')\n",
    "\n",
    "for ind in df.index:\n",
    "    index = int(df.at[ind, \"INDEX\"])\n",
    "    \n",
    "    if (index in R_index_list) or (index in IR_index_list) or (index in R_review_index_list):\n",
    "        continue\n",
    "    \n",
    "    if df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"] == \"Y\" and df.at[ind, \"REVIEW(Y/N)\"] == \"Y\":\n",
    "        R_review_index_list.append(index)\n",
    "        \n",
    "    if df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"] == \"Y\" and df.at[ind, \"REVIEW(Y/N)\"] != \"Y\":\n",
    "        R_index_list.append(index)\n",
    "        R_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "        \n",
    "        R_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"MACAQUE_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_other_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"OTHER_SPIECIES_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_tc_ct.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"TC_CT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_thalam.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"THALAM_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_cortex.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CORTEX_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_inject.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"INJECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_method.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"METHOD_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_connectivity.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CONNECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "    \n",
    "    if df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"] == \"N\":\n",
    "        IR_index_list.append(index)\n",
    "        IR_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "        \n",
    "        IR_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"MACAQUE_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_other_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"OTHER_SPIECIES_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_tc_ct.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"TC_CT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_thalam.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"THALAM_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_cortex.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CORTEX_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_inject.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"INJECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_method.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"METHOD_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_connectivity.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CONNECT_COUNT_IN_\"+column_name_base].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(R_index_list)\n",
    "print(R_review_index_list)\n",
    "print(IR_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the final_manually_read_csv_2_labeled\n",
    "\n",
    "# read the labeled testing set\n",
    "df = pd.read_csv(fpath.final_manually_read_csv_2_labeled, header=0, sep='\\t')\n",
    "\n",
    "for ind in df.index:\n",
    "    index = int(df.at[ind, \"INDEX\"])\n",
    "\n",
    "    if (index in R_index_list) or (index in IR_index_list) or (index in R_review_index_list):\n",
    "        continue\n",
    "    \n",
    "    if df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"] == \"Y\" and df.at[ind, \"REVIEW(Y/N)\"] == \"Y\":\n",
    "        R_review_index_list.append(index)\n",
    "    \n",
    "    if df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"] == \"Y\" and df.at[ind, \"REVIEW(Y/N)\"] != \"Y\":\n",
    "        R_index_list.append(index)\n",
    "        R_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "        \n",
    "        R_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"MACAQUE_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_other_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"OTHER_SPIECIES_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_tc_ct.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"TC_CT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_thalam.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"THALAM_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_cortex.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CORTEX_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_inject.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"INJECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_method.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"METHOD_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_connectivity.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CONNECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "    \n",
    "    if df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"] == \"N\":\n",
    "        IR_index_list.append(index)\n",
    "        IR_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "        \n",
    "        IR_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"MACAQUE_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_other_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"OTHER_SPIECIES_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_tc_ct.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"TC_CT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_thalam.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"THALAM_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_cortex.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CORTEX_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_inject.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"INJECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_method.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"METHOD_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_connectivity.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CONNECT_COUNT_IN_\"+column_name_base].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(R_index_list)\n",
    "print(R_review_index_list)\n",
    "print(IR_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the final_manually_read_csv_3_labeled\n",
    "\n",
    "# read the labeled testing set\n",
    "df = pd.read_csv(fpath.final_manually_read_csv_3_labeled, header=0, sep='\\t')\n",
    "\n",
    "for ind in df.index:\n",
    "    index = int(df.at[ind, \"INDEX\"])\n",
    "\n",
    "    if (index in R_index_list) or (index in IR_index_list) or (index in R_review_index_list):\n",
    "        continue\n",
    "    \n",
    "    if df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"] == \"Y\" and df.at[ind, \"REVIEW(Y/N)\"] == \"Y\":\n",
    "        R_review_index_list.append(index)\n",
    "    \n",
    "    if df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"] == \"Y\" and df.at[ind, \"REVIEW(Y/N)\"] != \"Y\":\n",
    "        R_index_list.append(index)\n",
    "        R_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "        \n",
    "        R_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"MACAQUE_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_other_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"OTHER_SPIECIES_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_tc_ct.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"TC_CT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_thalam.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"THALAM_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_cortex.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CORTEX_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_inject.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"INJECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_method.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"METHOD_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_connectivity.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CONNECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "    \n",
    "    if df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"] == \"N\":\n",
    "        IR_index_list.append(index)\n",
    "        IR_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "        \n",
    "        IR_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"MACAQUE_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_other_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"OTHER_SPIECIES_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_tc_ct.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"TC_CT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_thalam.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"THALAM_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_cortex.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CORTEX_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_inject.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"INJECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_method.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"METHOD_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_connectivity.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CONNECT_COUNT_IN_\"+column_name_base].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(R_index_list)\n",
    "print(R_review_index_list)\n",
    "print(IR_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the final_manually_read_csv_4_labeled\n",
    "\n",
    "# read the labeled testing set\n",
    "df = pd.read_csv(fpath.final_manually_read_csv_4_labeled, header=0, sep='\\t')\n",
    "\n",
    "for ind in df.index:\n",
    "    index = int(df.at[ind, \"INDEX\"])\n",
    "\n",
    "    if (index in R_index_list) or (index in IR_index_list) or (index in R_review_index_list):\n",
    "        continue\n",
    "    \n",
    "    if df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"] == \"Y\" and df.at[ind, \"REVIEW(Y/N)\"] == \"Y\":\n",
    "        R_review_index_list.append(index)\n",
    "    \n",
    "    if df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"] == \"Y\" and df.at[ind, \"REVIEW(Y/N)\"] != \"Y\":\n",
    "        R_index_list.append(index)\n",
    "        R_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "        \n",
    "        R_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"MACAQUE_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_other_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"OTHER_SPIECIES_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_tc_ct.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"TC_CT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_thalam.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"THALAM_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_cortex.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CORTEX_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_inject.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"INJECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_method.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"METHOD_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_connectivity.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CONNECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "    \n",
    "    if df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"] == \"N\":\n",
    "        IR_index_list.append(index)\n",
    "        IR_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "        \n",
    "        IR_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"MACAQUE_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_other_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"OTHER_SPIECIES_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_tc_ct.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"TC_CT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_thalam.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"THALAM_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_cortex.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CORTEX_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_inject.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"INJECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_method.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"METHOD_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_connectivity.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CONNECT_COUNT_IN_\"+column_name_base].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(R_index_list)\n",
    "print(R_review_index_list)\n",
    "print(IR_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the test_set_1000\n",
    "\n",
    "# read the labeled testing set\n",
    "df = pd.read_csv(fpath.poten_litera_testing_set_1000_labeled_complete, header=0, sep='\\t')\n",
    "\n",
    "for ind in df.index:\n",
    "    index = int(df.at[ind, \"INDEX\"])\n",
    "\n",
    "    if (index in R_index_list) or (index in IR_index_list) or (index in R_review_index_list):\n",
    "        continue\n",
    "    \n",
    "    if df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"] == \"Y\" and (df.at[ind, \"COMMENT\"] == df.at[ind, \"COMMENT\"]) and (\"review\" in df.at[ind, \"COMMENT\"]):\n",
    "        R_review_index_list.append(index)\n",
    "    \n",
    "    if (df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"] == \"Y\") and (df.at[ind, \"COMMENT\"] == df.at[ind, \"COMMENT\"]) and (\"review\" not in df.at[ind, \"COMMENT\"]):\n",
    "        R_index_list.append(index)\n",
    "        R_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "        \n",
    "        R_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"MACAQUE_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_other_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"OTHER_SPIECIES_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_tc_ct.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"TC_CT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_thalam.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"THALAM_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_cortex.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CORTEX_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_inject.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"INJECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_method.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"METHOD_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_connectivity.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CONNECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "    \n",
    "    if df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"] == \"N\":\n",
    "        IR_index_list.append(index)\n",
    "        IR_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "        \n",
    "        IR_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"MACAQUE_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_other_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"OTHER_SPIECIES_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_tc_ct.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"TC_CT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_thalam.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"THALAM_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_cortex.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CORTEX_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_inject.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"INJECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_method.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"METHOD_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_connectivity.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CONNECT_COUNT_IN_\"+column_name_base].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(R_index_list)\n",
    "print(R_review_index_list)\n",
    "print(IR_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whether to transfer the counts using, for example, log function\n",
    "tranform = True\n",
    "# transform = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the transformation if needed\n",
    "if tranform:\n",
    "    # relevant group\n",
    "    R_species = [transform(elem) for elem in R_species]\n",
    "    R_other_species = [transform(elem) for elem in R_other_species]\n",
    "    R_tc_ct = [transform(elem) for elem in R_tc_ct]\n",
    "    R_thalam = [transform(elem) for elem in R_thalam]\n",
    "    R_cortex = [transform(elem) for elem in R_cortex]\n",
    "    R_inject = [transform(elem) for elem in R_inject]\n",
    "    R_method = [transform(elem) for elem in R_method]\n",
    "    R_connectivity = [transform(elem) for elem in R_connectivity]\n",
    "\n",
    "    # irrelevant group\n",
    "    IR_species = [transform(elem) for elem in IR_species]\n",
    "    IR_other_species = [transform(elem) for elem in IR_other_species]\n",
    "    IR_tc_ct = [transform(elem) for elem in IR_tc_ct]\n",
    "    IR_thalam = [transform(elem) for elem in IR_thalam]\n",
    "    IR_cortex = [transform(elem) for elem in IR_cortex]\n",
    "    IR_inject = [transform(elem) for elem in IR_inject]\n",
    "    IR_method = [transform(elem) for elem in IR_method]\n",
    "    IR_connectivity = [transform(elem) for elem in IR_connectivity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the 8 dot plots\n",
    "# plt.figure(figsize=(10, 10))\n",
    "\n",
    "# plt.subplot(4, 2, 1)\n",
    "# plt.plot(R_index_list, R_species, 'ro', label=\"YES\")\n",
    "# plt.plot(IR_index_list, IR_species, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Species Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(4, 2, 2)\n",
    "# plt.plot(R_index_list, R_other_species, 'ro', label=\"YES\")\n",
    "# plt.plot(IR_index_list, IR_other_species, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Other Species Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(4, 2, 3)\n",
    "# plt.plot(R_index_list, R_tc_ct, 'ro', label=\"YES\")\n",
    "# plt.plot(IR_index_list, IR_tc_ct, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"TC_CT Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(4, 2, 4)\n",
    "# plt.plot(R_index_list, R_thalam, 'ro', label=\"YES\")\n",
    "# plt.plot(IR_index_list, IR_thalam, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Thalam Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(4, 2, 5)\n",
    "# plt.plot(R_index_list, R_cortex, 'ro', label=\"YES\")\n",
    "# plt.plot(IR_index_list, IR_cortex, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Cortex Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(4, 2, 6)\n",
    "# plt.plot(R_index_list, R_cortex, 'ro', label=\"YES\")\n",
    "# plt.plot(IR_index_list, IR_cortex, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Inject Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(4, 2, 7)\n",
    "# plt.plot(R_index_list, R_method, 'ro', label=\"YES\")\n",
    "# plt.plot(IR_index_list, IR_method, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Method Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(4, 2, 8)\n",
    "# plt.plot(R_index_list, R_connectivity, 'ro', label=\"YES\")\n",
    "# plt.plot(IR_index_list, IR_connectivity, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Connectivity Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d5dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the dot plot of the relevance_index of YESs and NOs of the test data set\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(R_index_list, R_relevance_index_list, 'ro', label=\"YES\")\n",
    "plt.plot(IR_index_list, IR_relevance_index_list, 'bo', label=\"NO\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Relevance Index\")\n",
    "plt.legend()\n",
    "\n",
    "# add labels for the relevant index points\n",
    "for i, index in enumerate(R_index_list):\n",
    "    plt.text(index, R_relevance_index_list[i]+0.1, str(index), color='black', fontsize=10)\n",
    "\n",
    "# # add labels for the relevant index points\n",
    "# for i, index in enumerate(IR_index_list):\n",
    "#     plt.text(index, IR_relevance_index_list[i], str(index), color='black', fontsize=10)\n",
    "\n",
    "# plt.text(37, IR_relevance_index_list[IR_index_list.index(37)], str(37), color='black', fontsize=10)\n",
    "# plt.text(3683, R_relevance_index_list[R_index_list.index(3683)], str(3683), color='black', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_relevance_index_index = 4525\n",
    "min_relevance_index_index = R_index_list[R_relevance_index_list.index(min(x for x in R_relevance_index_list if x != 0))]\n",
    "print(min_relevance_index_index)\n",
    "\n",
    "# for i in range(len(relvant_index_list)):\n",
    "#     print(relvant_index_list[i], relevant_relevance_index_list[i])\n",
    "# print(relevant_relevance_index_list)\n",
    "\n",
    "for ind in df_db_ranked.index:\n",
    "    if df_db_ranked.at[ind, \"INDEX\"] == min_relevance_index_index:\n",
    "        min_relevance_index_ind = ind\n",
    "        break \n",
    "\n",
    "min_relev_index_num = min_relevance_index_ind  + 1\n",
    "num = math.ceil(min_relev_index_num * 1.3)\n",
    "threshold_index = df_db_ranked.at[num-1, 'INDEX']\n",
    "\n",
    "print(f\"The index with the minimum relevance index: {min_relevance_index_index}\")\n",
    "print(f\"The number of articles with relevance index larger or equal than that of {min_relevance_index_index}: {min_relev_index_num}\")\n",
    "print(f\"The index of the article with the threshold relevance index: {threshold_index}\")\n",
    "print(f\"The number of articles to manually check is {num}\")\n",
    "print(f\"The threshold index is {threshold_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6260d21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_values_uniformly(data, n):\n",
    "    \"\"\"Pick up `n` values uniformly from `data`.\"\"\"\n",
    "    if n < 1:\n",
    "        return []\n",
    "\n",
    "    # Determine the range of the data\n",
    "    min_val, max_val = min(data), max(data)\n",
    "\n",
    "    threshold = (max_val - min_val) / n / 2\n",
    "\n",
    "    # If n is 1, just return the midpoint\n",
    "    if n == 1:\n",
    "        return [(min_val + max_val) / 2]\n",
    "\n",
    "    # Calculate the interval size\n",
    "    interval = (max_val - min_val) / (n - 1)\n",
    "\n",
    "    # Get the uniform values\n",
    "    return [min_val + i * interval for i in range(n)], threshold\n",
    "\n",
    "# data = [1, 3, 5, 2, 8, 10, 2]\n",
    "n = 5\n",
    "density_display_index, thres = pick_values_uniformly(R_relevance_index_list + IR_relevance_index_list, n)\n",
    "# print(density_display_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73da3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the distribution of the relevance group and the non-relevance group as violin plots\n",
    "\n",
    "# Printing the length of lists\n",
    "print(\"Numer of relevant literature:\", len(R_relevance_index_list))\n",
    "print(\"Number of not relevant literature:\", len(IR_relevance_index_list))\n",
    "print()\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df = pd.DataFrame({'Relevance Index': R_relevance_index_list + IR_relevance_index_list, \n",
    "                   'Label': ['Relevant'] * len(R_relevance_index_list) + ['Not Relevant'] * len(IR_relevance_index_list)})\n",
    "\n",
    "# Draw the violin plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.violinplot(x='Label', y='Relevance Index', data=df, bw='scott', cut=0)\n",
    "\n",
    "# relevance_indices = density_display_index  # Replace with your relevance indices\n",
    "\n",
    "# threshold = thres  # Adjust this based on your desired range around the relevance index\n",
    "\n",
    "# for index in relevance_indices:\n",
    "#     ax.axhline(index, color='gray', linestyle='--')\n",
    "    \n",
    "#     for i, label in enumerate(df['Label'].unique()):\n",
    "#         # Filter data points close to the current relevance index\n",
    "#         close_points = df[(df['Label'] == label) & (np.abs(df['Relevance Index'] - index) < threshold)]\n",
    "#         density = len(close_points)\n",
    "        \n",
    "#         ax.text(i, index + 0.1, str(density), ha='center', va='center', color='red', fontsize=9)  # adjust the vertical offset (0.1 here) as necessary\n",
    "\n",
    "plt.title('Distribution of Relevance Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the equality of variances\n",
    "var_relevant = np.var(R_relevance_index_list)\n",
    "var_non_relevant = np.var(IR_relevance_index_list)\n",
    "print('Variance of RELEVANCE_INDEX of the relevant group:', var_relevant)\n",
    "print('Variance of RELEVANCE_INDEX of the non-relevant group:', var_non_relevant)\n",
    "print('Ratio of the above variance:', var_relevant/var_non_relevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare the final list of articles to manually read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the DataFrame from a CSV file\n",
    "df = pd.read_csv(fpath.poten_litera_db_ranked_by_tak, header=0, sep='\\t')  # Replace with your CSV file path\n",
    "\n",
    "index_above_threshold = []\n",
    "index_below_threshold = []\n",
    "\n",
    "# flag of being relevant\n",
    "flag = True\n",
    "# Iterate through the DataFrame and add the index to the list\n",
    "for ind, row in df.iterrows():\n",
    "    index = int(row['INDEX'])\n",
    "    \n",
    "    if not flag and (index == threshold_index):\n",
    "        raise Exception(\"The index of the article with the threshold relevance index is not unique!\")\n",
    "    \n",
    "    if index == threshold_index:\n",
    "        index_above_threshold.append(index)\n",
    "        flag = False\n",
    "    else: \n",
    "        if flag:\n",
    "            index_above_threshold.append(index)\n",
    "        else:\n",
    "            index_below_threshold.append(index)\n",
    "    \n",
    "print(len(index_above_threshold))\n",
    "print(len(index_below_threshold))\n",
    "\n",
    "print(index_above_threshold)\n",
    "print(index_below_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add the index of the articles with 'inject' in the full text and articles without full text\n",
    "# articles_with_keyword = []\n",
    "# articles_short = []\n",
    "# articles_without_full_text = []\n",
    "\n",
    "# for article_index in index_below_threshold:\n",
    "#     full_text_file_path = os.path.join(fpath.text_folder, str(article_index)+'.txt')\n",
    "    \n",
    "#     # if the full text file exists\n",
    "#     if os.path.exists(full_text_file_path):\n",
    "#         with open(full_text_file_path, 'r', encoding='ascii') as file:\n",
    "#             # Read the contents of the file\n",
    "#             text = file.read()\n",
    "#         text = plib.process_text(text, lower=True)\n",
    "        \n",
    "#         species = False\n",
    "#         inject = False\n",
    "#         thalam = False\n",
    "        \n",
    "#         for word in ['rhesus', 'macaque', 'macaca']:\n",
    "#             if word in text:\n",
    "#                 species = True\n",
    "        \n",
    "#         if 'inject' in text:\n",
    "#             inject = True\n",
    "            \n",
    "#         if 'thalam' in text:\n",
    "#             thalam = True\n",
    "            \n",
    "#         if species and inject and thalam and len(text.split()) >= 3000:\n",
    "#             articles_with_keyword.append(article_index)\n",
    "#         if len(text.split()) < 3000:\n",
    "#             articles_short.append(article_index)\n",
    "#     else:\n",
    "#         articles_without_full_text.append(article_index)\n",
    "\n",
    "# print(len(articles_with_keyword))\n",
    "# print(len(articles_short))\n",
    "# print(len(articles_without_full_text))\n",
    "# print(articles_with_keyword)\n",
    "# print(articles_short)\n",
    "# print(articles_without_full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the articels above the threshold index, articles with short text and articles with no text are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_above_threshold = index_above_threshold + articles_short + articles_without_full_text\n",
    "# print(len(index_above_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the index_above_threshold\n",
    "with open(fpath.article_list_to_manually_read, 'w') as file:\n",
    "    # Convert each number to string and join them with commas\n",
    "    numbers_str = ','.join(map(str, index_above_threshold))\n",
    "    \n",
    "    # Write the string to the file\n",
    "    file.write(numbers_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the R_review_index_list\n",
    "with open(fpath.relevant_article_and_is_review, 'w') as file:\n",
    "    # Convert each number to string and join them with commas\n",
    "    numbers_str = ','.join(map(str, R_review_index_list))\n",
    "    \n",
    "    # Write the string to the file\n",
    "    file.write(numbers_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc9ccf",
   "metadata": {},
   "source": [
    "<h3> Next step: manually read papers and find all actually related literature </h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
