{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea1cce5-4f07-4bd7-8ca3-5f6fa51254d0",
   "metadata": {},
   "source": [
    "# Keywords matching and relevance index ranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5a61c5-f8c6-418a-b450-cdea0378ddab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from nltk import ngrams\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726b12c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib\n",
    "import parameters as params\n",
    "import dataframe_columns as df_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489f0451",
   "metadata": {},
   "source": [
    "## Predefined fucntions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(elem):\n",
    "    # transform the count\n",
    "    return math.log(1 + elem)\n",
    "# # --------------------start of test code--------------------\n",
    "# elem = 0\n",
    "# print(transform(10))\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def if_not_TT(index: int):\n",
    "    text_path = os.path.join(fpath.text_folder, str(index) + \".txt\")\n",
    "    \n",
    "    if os.path.exists(text_path):\n",
    "        with open(text_path, 'r', encoding=\"ascii\") as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        text = plib.process_text(text, lower=True)\n",
    "    \n",
    "        if len(text.split()) >= 4000 and (\"inject\" not in text):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "# # --------------------start of test code--------------------\n",
    "# index = 72\n",
    "# if_not_TT(index)\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tak(ind):\n",
    "    df_db = pd.read_csv(fpath.poten_litera_db, header=0, sep=\"\\t\")\n",
    "    \n",
    "    tak = \"\"\n",
    "    \n",
    "    title = df_db.at[ind, \"TITLE\"]\n",
    "    abstract = df_db.at[ind, \"ABSTRACT\"]\n",
    "    keywords = df_db.at[ind, \"KEYWORDS\"]\n",
    "    \n",
    "    if title != title:\n",
    "        title = \"\"\n",
    "    \n",
    "    if abstract != abstract:\n",
    "        abstract = \"\"\n",
    "        \n",
    "    if keywords != keywords:\n",
    "        keywords = \"\"\n",
    "    \n",
    "    # if abstract is empty, then tak is empty\n",
    "    if abstract != abstract or abstract == \"\":\n",
    "        tak = \"\"\n",
    "    else:\n",
    "        tak = title + \" \" + abstract + \" \" + keywords\n",
    "    \n",
    "    # process tak\n",
    "    tak = plib.process_text(tak, lower=True)\n",
    "        \n",
    "    return tak\n",
    "# # --------------------start of test code--------------------\n",
    "# ind = 1\n",
    "# get_tak(ind)\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def if_not_macaque_but_other(tak):\n",
    "    if tak == \"\":\n",
    "        return False\n",
    "    \n",
    "    # process text\n",
    "    tak = plib.process_text(tak, lower=True)\n",
    "    \n",
    "    # word tokenization\n",
    "    tokens = word_tokenize(tak)\n",
    "    \n",
    "    macaque_label = False\n",
    "    other_species_label = False\n",
    "    # monkey_label = False\n",
    "    # other_submonkey_species_label = False\n",
    "    \n",
    "    # macaque\n",
    "    for word in params.possible_macaque_group:\n",
    "        if word in tak:\n",
    "            macaque_label = True\n",
    "    \n",
    "    # other species   \n",
    "    for keyword in params.other_spiecies_group:\n",
    "        if keyword in params.exact_match_kw_list:\n",
    "            # length of the keyword\n",
    "            len_word = len(keyword.split())\n",
    "            \n",
    "            # get the ngrams of length len_word from text\n",
    "            ng = list(ngrams(tokens, len_word))\n",
    "            words = [' '.join(gram) for gram in ng]\n",
    "            # print(words)\n",
    "    \n",
    "            for w in words:\n",
    "                if keyword == w:\n",
    "                    other_species_label = True\n",
    "        else:\n",
    "            if keyword in tak:\n",
    "                other_species_label = True\n",
    "    \n",
    "    # # the word \"monkey\"\n",
    "    # if \"monkey\" in tak:\n",
    "    #     monkey_label = True\n",
    "    \n",
    "    # other submonkey species\n",
    "    for keyword in params.other_submonkey_species:\n",
    "        if keyword in tak:\n",
    "            other_submonkey_species_label = True\n",
    "    \n",
    "    if not macaque_label and other_species_label:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "# # --------------------start of test code--------------------\n",
    "# tak = \"monkey\"\n",
    "# if_not_macaque_but_other(tak)\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02fb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance_index(not_TT, not_macaque_but_others, count_dict):\n",
    "    relevance_index = 0\n",
    "    \n",
    "    for key in count_dict.keys():\n",
    "        relevance_index += transform(count_dict[key]) * (params.ranking_kw_groups_weights[key])\n",
    "    \n",
    "    if not_TT == \"Y\" or not_macaque_but_others == \"Y\":\n",
    "        relevance_index = 0.1 * relevance_index\n",
    "        \n",
    "    return relevance_index\n",
    "# --------------------start of test code--------------------\n",
    "# keywords_count_or_fre = {}\n",
    "# index = calcul_related(keywords_count_or_fre, params.on_topic_kws_weights)\n",
    "# print(index)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edde53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_relevance_index(db_count_path, db_relevance_index_path, rank_by):\n",
    "    df = pd.read_csv(db_count_path, header=0, sep=\"\\t\")\n",
    "    \n",
    "    if rank_by == 'tak':\n",
    "        column_name = '_COUNT_IN_TAK'\n",
    "    elif rank_by == '500':\n",
    "        column_name = '_COUNT_IN_500'\n",
    "    elif rank_by == 'full_text':\n",
    "        column_name = '_COUNT_IN_FULL_TEXT'\n",
    "        \n",
    "    count_dict = {}\n",
    "    \n",
    "    for ind in df.index:\n",
    "        # index = int(df.at[ind, \"INDEX\"])\n",
    "        \n",
    "        not_TT = df.at[ind, \"NOT_TT\"]\n",
    "        not_macaque_but_others = df.at[ind, \"NOT_MACAQUE_BUT_OTHERS\"] \n",
    "        \n",
    "        for key in params.ranking_kw_groups.keys():\n",
    "            # if key == 'INJECT' or 'METHOD':\n",
    "            #     value = df.at[ind, key+'_COUNT_IN_FULL_TEXT']\n",
    "            # else:\n",
    "            #     value = df.at[ind, key+column_name]\n",
    "            value = df.at[ind, key+column_name]\n",
    "            \n",
    "            if value != value:\n",
    "                value = 0\n",
    "            count_dict[key] = int(value)\n",
    "        # print(count_dict)\n",
    "        \n",
    "        relev_index = relevance_index(not_TT, not_macaque_but_others, count_dict)\n",
    "        \n",
    "        df.at[ind, \"RELEVANCE_INDEX\"] = relev_index\n",
    "\n",
    "        # line_number_in_csv = ind + 1\n",
    "        # print(\"Line number:\", line_number_in_csv, \" INDEX:\", int(df.at[ind, \"INDEX\"]))\n",
    "    \n",
    "    df.columns = df_col.db_ranked_columns\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.to_csv(db_relevance_index_path, header=True, index=False, sep=\"\\t\")\n",
    "    \n",
    "    # print(\"Weighting and ranking the potentially related literature succeded!\")\n",
    "    # print(\"Enjoy reading!\")\n",
    "# --------------------start of test code--------------------\n",
    "# input_path = fpath.poten_litera_db_kw_count\n",
    "# output_path = fpath.poten_litera_db_ranked\n",
    "# rank(input_path, output_path, params.ranking_params_weights)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_and_save(input_path, output_path):\n",
    "    plib.clear_file(output_path)\n",
    "    \n",
    "    # sort\n",
    "    df_to_rank = pd.read_csv(input_path, header=0, sep=\"\\t\")\n",
    "\n",
    "    # sort by relevance index\n",
    "    df_to_rank.sort_values(by=['RELEVANCE_INDEX'], ascending=False, inplace=True)\n",
    "        \n",
    "    df_to_rank.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df_to_rank.to_csv(output_path, header=True, index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e562971",
   "metadata": {},
   "source": [
    "## Main program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns of \"NOT_TT\" and \"NOT_MACAQUE_BUT_OTHER\"\n",
    "input_path = fpath.poten_litera_db_kw_count\n",
    "df = pd.read_csv(input_path, header=0, sep=\"\\t\")\n",
    "\n",
    "output_path = fpath.poten_litera_db_kw_count_with_2_new_columns\n",
    "plib.clear_file(output_path)\n",
    "\n",
    "for ind in df.index:\n",
    "    index = int(df.at[ind, \"INDEX\"])\n",
    "    \n",
    "    tak = get_tak(ind)\n",
    "    \n",
    "    if if_not_TT(index):\n",
    "        df.at[ind, \"NOT_TT\"] = \"Y\"\n",
    "    \n",
    "    if if_not_macaque_but_other(tak):\n",
    "        df.at[ind, \"NOT_MACAQUE_BUT_OTHERS\"] = \"Y\"\n",
    "\n",
    "    # line_number_in_csv = ind + 1\n",
    "    # print(\"Line number:\", line_number_in_csv, \" INDEX:\", int(df.at[ind, \"INDEX\"]))\n",
    "    \n",
    "df.columns = df_col.db_ranked_columns_with_2_new_columns\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.to_csv(output_path, header=True, index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Calculate relevance index and rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_by = 'tak'\n",
    "# rank_by = '500'\n",
    "# rank_by = 'full_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = fpath.poten_litera_db_kw_count\n",
    "relevance_index_path = fpath.poten_litera_db_relevance_index\n",
    "plib.clear_file(relevance_index_path)\n",
    "\n",
    "# compute relevance index\n",
    "compute_relevance_index(input_path, relevance_index_path, rank_by)\n",
    "\n",
    "# rank\n",
    "if rank_by == 'tak':\n",
    "    # rank by tak    \n",
    "    rank_and_save(relevance_index_path, fpath.poten_litera_db_ranked_by_tak)\n",
    "elif rank_by == '500':\n",
    "    # rank by 500    \n",
    "    rank_and_save(relevance_index_path, fpath.poten_litera_db_ranked_by_500)\n",
    "elif rank_by == 'full_text':\n",
    "    # rank by full text\n",
    "    rank_and_save(relevance_index_path, fpath.poten_litera_db_ranked_by_full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36685b74",
   "metadata": {},
   "source": [
    "### 2. Ranking results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize lists\n",
    "\n",
    "# relevant group\n",
    "R_species = []\n",
    "R_other_species = []\n",
    "R_tc_ct = []\n",
    "R_thalam = []\n",
    "R_cortex = []\n",
    "R_inject = []\n",
    "R_method = []\n",
    "R_connectivity = []\n",
    "\n",
    "# index list of the relevant group\n",
    "R_index_list = []\n",
    "# relevance index list of the relevant group\n",
    "R_relevance_index_list = []\n",
    "\n",
    "# Irrelevant group\n",
    "IR_species = []\n",
    "IR_other_species = []\n",
    "IR_tc_ct = []\n",
    "IR_thalam = []\n",
    "IR_cortex = []\n",
    "IR_inject = []\n",
    "IR_method = []\n",
    "IR_connectivity = []\n",
    "\n",
    "# index list of the irrelevant group\n",
    "IR_index_list = []\n",
    "# relevance index list of the irrelevant group\n",
    "IR_relevance_index_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the df_db_ranked based on the rank_by\n",
    "if rank_by == 'tak':\n",
    "    # rank by tak\n",
    "    df_db_ranked = pd.read_csv(fpath.poten_litera_db_ranked_by_tak, header=0, sep='\\t')\n",
    "    column_name_base = \"TAK\"\n",
    "# elif rank_by == '500':\n",
    "#     # rank by 500\n",
    "#     df_db_ranked = pd.read_csv(fpath.poten_litera_db_ranked_by_500, header=0, sep='\\t')\n",
    "#     column_name_base = \"500\"\n",
    "# elif rank_by == 'full_text':\n",
    "#     # rank by full_text\n",
    "#     df_db_ranked = pd.read_csv(fpath.poten_litera_db_ranked_by_full_text, header=0, sep='\\t')\n",
    "#     column_name_base = \"FULL_TEXT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the final_manually_read_csv_1_labeled\n",
    "\n",
    "# read the labeled testing set\n",
    "df = pd.read_csv(fpath.final_manually_read_csv_1_labeled, header=0, sep='\\t')\n",
    "\n",
    "for ind in df.index:\n",
    "    index = int(df.at[ind, \"INDEX\"])\n",
    "    \n",
    "    if (index in R_index_list) or (index in IR_index_list):\n",
    "        continue\n",
    "\n",
    "    if df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"] == \"Y\" and df.at[ind, \"REVIEW(Y/N)\"] != \"Y\":\n",
    "        R_index_list.append(index)\n",
    "        R_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "        \n",
    "        R_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"MACAQUE_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_other_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"OTHER_SPIECIES_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_tc_ct.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"TC_CT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_thalam.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"THALAM_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_cortex.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CORTEX_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_inject.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"INJECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_method.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"METHOD_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_connectivity.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CONNECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "    \n",
    "    if df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"] == \"N\":\n",
    "        IR_index_list.append(index)\n",
    "        IR_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "        \n",
    "        IR_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"MACAQUE_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_other_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"OTHER_SPIECIES_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_tc_ct.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"TC_CT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_thalam.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"THALAM_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_cortex.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CORTEX_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_inject.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"INJECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_method.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"METHOD_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_connectivity.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CONNECT_COUNT_IN_\"+column_name_base].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the final_manually_read_csv_2_labeled\n",
    "\n",
    "# read the labeled testing set\n",
    "df = pd.read_csv(fpath.final_manually_read_csv_2_labeled, header=0, sep='\\t')\n",
    "\n",
    "for ind in df.index:\n",
    "    index = int(df.at[ind, \"INDEX\"])\n",
    "\n",
    "    if (index in R_index_list) or (index in IR_index_list):\n",
    "        continue\n",
    "    \n",
    "    if df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"] == \"Y\" and df.at[ind, \"REVIEW(Y/N)\"] != \"Y\":\n",
    "        R_index_list.append(index)\n",
    "        R_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "        \n",
    "        R_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"MACAQUE_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_other_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"OTHER_SPIECIES_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_tc_ct.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"TC_CT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_thalam.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"THALAM_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_cortex.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CORTEX_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_inject.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"INJECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_method.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"METHOD_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_connectivity.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CONNECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "    \n",
    "    if df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"] == \"N\":\n",
    "        IR_index_list.append(index)\n",
    "        IR_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "        \n",
    "        IR_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"MACAQUE_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_other_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"OTHER_SPIECIES_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_tc_ct.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"TC_CT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_thalam.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"THALAM_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_cortex.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CORTEX_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_inject.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"INJECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_method.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"METHOD_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_connectivity.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CONNECT_COUNT_IN_\"+column_name_base].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the final_manually_read_csv_3_labeled\n",
    "\n",
    "# read the labeled testing set\n",
    "df = pd.read_csv(fpath.final_manually_read_csv_3_labeled, header=0, sep='\\t')\n",
    "\n",
    "for ind in df.index:\n",
    "    index = int(df.at[ind, \"INDEX\"])\n",
    "\n",
    "    if (index in R_index_list) or (index in IR_index_list):\n",
    "        continue\n",
    "    \n",
    "    if df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"] == \"Y\" and df.at[ind, \"REVIEW(Y/N)\"] != \"Y\":\n",
    "        R_index_list.append(index)\n",
    "        R_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "        \n",
    "        R_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"MACAQUE_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_other_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"OTHER_SPIECIES_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_tc_ct.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"TC_CT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_thalam.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"THALAM_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_cortex.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CORTEX_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_inject.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"INJECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_method.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"METHOD_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_connectivity.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CONNECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "    \n",
    "    if df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"] == \"N\":\n",
    "        IR_index_list.append(index)\n",
    "        IR_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "        \n",
    "        IR_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"MACAQUE_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_other_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"OTHER_SPIECIES_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_tc_ct.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"TC_CT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_thalam.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"THALAM_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_cortex.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CORTEX_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_inject.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"INJECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_method.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"METHOD_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_connectivity.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CONNECT_COUNT_IN_\"+column_name_base].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the final_manually_read_csv_4_labeled\n",
    "\n",
    "# read the labeled testing set\n",
    "df = pd.read_csv(fpath.final_manually_read_csv_4_labeled, header=0, sep='\\t')\n",
    "\n",
    "for ind in df.index:\n",
    "    index = int(df.at[ind, \"INDEX\"])\n",
    "\n",
    "    if (index in R_index_list) or (index in IR_index_list):\n",
    "        continue\n",
    "    \n",
    "    if df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"] == \"Y\" and df.at[ind, \"REVIEW(Y/N)\"] != \"Y\":\n",
    "        R_index_list.append(index)\n",
    "        R_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "        \n",
    "        R_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"MACAQUE_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_other_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"OTHER_SPIECIES_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_tc_ct.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"TC_CT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_thalam.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"THALAM_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_cortex.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CORTEX_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_inject.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"INJECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_method.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"METHOD_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_connectivity.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CONNECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "    \n",
    "    if df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"] == \"N\":\n",
    "        IR_index_list.append(index)\n",
    "        IR_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "        \n",
    "        IR_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"MACAQUE_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_other_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"OTHER_SPIECIES_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_tc_ct.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"TC_CT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_thalam.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"THALAM_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_cortex.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CORTEX_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_inject.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"INJECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_method.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"METHOD_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_connectivity.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CONNECT_COUNT_IN_\"+column_name_base].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the test_set_1000\n",
    "\n",
    "# read the labeled testing set\n",
    "df = pd.read_csv(fpath.poten_litera_testing_set_1000_labeled_complete, header=0, sep=',')\n",
    "\n",
    "for ind in df.index:\n",
    "    index = int(df.at[ind, \"INDEX\"])\n",
    "\n",
    "    if (index in R_index_list) or (index in IR_index_list):\n",
    "        continue\n",
    "    \n",
    "    if (df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"] == \"Y\") and (df.at[ind, \"COMMENT\"] == df.at[ind, \"COMMENT\"]) and (\"review\" not in df.at[ind, \"COMMENT\"]):\n",
    "        R_index_list.append(index)\n",
    "        R_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "        \n",
    "        R_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"MACAQUE_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_other_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"OTHER_SPIECIES_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_tc_ct.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"TC_CT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_thalam.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"THALAM_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_cortex.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CORTEX_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_inject.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"INJECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_method.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"METHOD_COUNT_IN_\"+column_name_base].values[0])\n",
    "        R_connectivity.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CONNECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "    \n",
    "    if df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"] == \"N\":\n",
    "        IR_index_list.append(index)\n",
    "        IR_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "        \n",
    "        IR_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"MACAQUE_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_other_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"OTHER_SPIECIES_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_tc_ct.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"TC_CT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_thalam.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"THALAM_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_cortex.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CORTEX_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_inject.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"INJECT_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_method.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"METHOD_COUNT_IN_\"+column_name_base].values[0])\n",
    "        IR_connectivity.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CONNECT_COUNT_IN_\"+column_name_base].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whether to transfer the counts using, for example, log function\n",
    "tranform = True\n",
    "# transform = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the transformation if needed\n",
    "if tranform:\n",
    "    # relevant group\n",
    "    R_species = [transform(elem) for elem in R_species]\n",
    "    R_other_species = [transform(elem) for elem in R_other_species]\n",
    "    R_tc_ct = [transform(elem) for elem in R_tc_ct]\n",
    "    R_thalam = [transform(elem) for elem in R_thalam]\n",
    "    R_cortex = [transform(elem) for elem in R_cortex]\n",
    "    R_inject = [transform(elem) for elem in R_inject]\n",
    "    R_method = [transform(elem) for elem in R_method]\n",
    "    R_connectivity = [transform(elem) for elem in R_connectivity]\n",
    "\n",
    "    # irrelevant group\n",
    "    IR_species = [transform(elem) for elem in IR_species]\n",
    "    IR_other_species = [transform(elem) for elem in IR_other_species]\n",
    "    IR_tc_ct = [transform(elem) for elem in IR_tc_ct]\n",
    "    IR_thalam = [transform(elem) for elem in IR_thalam]\n",
    "    IR_cortex = [transform(elem) for elem in IR_cortex]\n",
    "    IR_inject = [transform(elem) for elem in IR_inject]\n",
    "    IR_method = [transform(elem) for elem in IR_method]\n",
    "    IR_connectivity = [transform(elem) for elem in IR_connectivity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the 8 dot plots\n",
    "# plt.figure(figsize=(10, 10))\n",
    "\n",
    "# plt.subplot(4, 2, 1)\n",
    "# plt.plot(R_index_list, R_species, 'ro', label=\"YES\")\n",
    "# plt.plot(IR_index_list, IR_species, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Species Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(4, 2, 2)\n",
    "# plt.plot(R_index_list, R_other_species, 'ro', label=\"YES\")\n",
    "# plt.plot(IR_index_list, IR_other_species, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Other Species Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(4, 2, 3)\n",
    "# plt.plot(R_index_list, R_tc_ct, 'ro', label=\"YES\")\n",
    "# plt.plot(IR_index_list, IR_tc_ct, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"TC_CT Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(4, 2, 4)\n",
    "# plt.plot(R_index_list, R_thalam, 'ro', label=\"YES\")\n",
    "# plt.plot(IR_index_list, IR_thalam, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Thalam Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(4, 2, 5)\n",
    "# plt.plot(R_index_list, R_cortex, 'ro', label=\"YES\")\n",
    "# plt.plot(IR_index_list, IR_cortex, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Cortex Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(4, 2, 6)\n",
    "# plt.plot(R_index_list, R_cortex, 'ro', label=\"YES\")\n",
    "# plt.plot(IR_index_list, IR_cortex, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Inject Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(4, 2, 7)\n",
    "# plt.plot(R_index_list, R_method, 'ro', label=\"YES\")\n",
    "# plt.plot(IR_index_list, IR_method, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Method Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(4, 2, 8)\n",
    "# plt.plot(R_index_list, R_connectivity, 'ro', label=\"YES\")\n",
    "# plt.plot(IR_index_list, IR_connectivity, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Connectivity Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d5dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the dot plot of the relevance_index of YESs and NOs of the test data set\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(R_index_list, R_relevance_index_list, 'ro', label=\"YES\")\n",
    "plt.plot(IR_index_list, IR_relevance_index_list, 'bo', label=\"NO\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Relevance Index\")\n",
    "plt.legend()\n",
    "\n",
    "# add labels for the relevant index points\n",
    "for i, index in enumerate(R_index_list):\n",
    "    plt.text(index, R_relevance_index_list[i]+0.1, str(index), color='black', fontsize=10)\n",
    "\n",
    "# # add labels for the relevant index points\n",
    "# for i, index in enumerate(IR_index_list):\n",
    "#     plt.text(index, IR_relevance_index_list[i], str(index), color='black', fontsize=10)\n",
    "\n",
    "# plt.text(37, IR_relevance_index_list[IR_index_list.index(37)], str(37), color='black', fontsize=10)\n",
    "# plt.text(3683, R_relevance_index_list[R_index_list.index(3683)], str(3683), color='black', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_relevance_index_index = 3683\n",
    "# min_relevance_index_index = R_index_list[R_relevance_index_list.index(min(R_relevance_index_list))]\n",
    "\n",
    "# for i in range(len(relvant_index_list)):\n",
    "#     print(relvant_index_list[i], relevant_relevance_index_list[i])\n",
    "# print(relevant_relevance_index_list)\n",
    "\n",
    "for ind in df_db_ranked.index:\n",
    "    if df_db_ranked.at[ind, \"INDEX\"] == min_relevance_index_index:\n",
    "        min_relevance_index_ind = ind\n",
    "        break \n",
    "\n",
    "min_relev_index_num = min_relevance_index_ind  + 1\n",
    "num = math.ceil(min_relev_index_num * 1.2)\n",
    "threshold_index = df_db_ranked.at[num-1, 'INDEX']\n",
    "\n",
    "print(f\"The index with the minimum relevance index: {min_relevance_index_index}\")\n",
    "print(f\"The number of articles with relevance index larger or equal than that of {min_relevance_index_index}: {min_relev_index_num}\")\n",
    "print(f\"The index of the article with the threshold relevance index: {threshold_index}\")\n",
    "print(f\"The number of articles to manually check is {num}\")\n",
    "print(f\"The threshold index is {threshold_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6260d21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_values_uniformly(data, n):\n",
    "    \"\"\"Pick up `n` values uniformly from `data`.\"\"\"\n",
    "    if n < 1:\n",
    "        return []\n",
    "\n",
    "    # Determine the range of the data\n",
    "    min_val, max_val = min(data), max(data)\n",
    "\n",
    "    threshold = (max_val - min_val) / n / 2\n",
    "\n",
    "    # If n is 1, just return the midpoint\n",
    "    if n == 1:\n",
    "        return [(min_val + max_val) / 2]\n",
    "\n",
    "    # Calculate the interval size\n",
    "    interval = (max_val - min_val) / (n - 1)\n",
    "\n",
    "    # Get the uniform values\n",
    "    return [min_val + i * interval for i in range(n)], threshold\n",
    "\n",
    "# data = [1, 3, 5, 2, 8, 10, 2]\n",
    "n = 5\n",
    "density_display_index, thres = pick_values_uniformly(R_relevance_index_list + IR_relevance_index_list, n)\n",
    "# print(density_display_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73da3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the distribution of the relevance group and the non-relevance group as violin plots\n",
    "\n",
    "# Printing the length of lists\n",
    "print(\"Numer of relevant literature:\", len(R_relevance_index_list))\n",
    "print(\"Number of not relevant literature:\", len(IR_relevance_index_list))\n",
    "print()\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df = pd.DataFrame({'Relevance Index': R_relevance_index_list + IR_relevance_index_list, \n",
    "                   'Label': ['Relevant'] * len(R_relevance_index_list) + ['Not Relevant'] * len(IR_relevance_index_list)})\n",
    "\n",
    "# Draw the violin plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.violinplot(x='Label', y='Relevance Index', data=df, bw='scott', cut=0)\n",
    "\n",
    "# relevance_indices = density_display_index  # Replace with your relevance indices\n",
    "\n",
    "# threshold = thres  # Adjust this based on your desired range around the relevance index\n",
    "\n",
    "# for index in relevance_indices:\n",
    "#     ax.axhline(index, color='gray', linestyle='--')\n",
    "    \n",
    "#     for i, label in enumerate(df['Label'].unique()):\n",
    "#         # Filter data points close to the current relevance index\n",
    "#         close_points = df[(df['Label'] == label) & (np.abs(df['Relevance Index'] - index) < threshold)]\n",
    "#         density = len(close_points)\n",
    "        \n",
    "#         ax.text(i, index + 0.1, str(density), ha='center', va='center', color='red', fontsize=9)  # adjust the vertical offset (0.1 here) as necessary\n",
    "\n",
    "plt.title('Distribution of Relevance Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the equality of variances\n",
    "var_relevant = np.var(R_relevance_index_list)\n",
    "var_non_relevant = np.var(IR_relevance_index_list)\n",
    "print('Variance of RELEVANCE_INDEX of the relevant group:', var_relevant)\n",
    "print('Variance of RELEVANCE_INDEX of the non-relevant group:', var_non_relevant)\n",
    "print('Ratio of the above variance:', var_relevant/var_non_relevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare the final list of articles to manually read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the DataFrame from a CSV file\n",
    "# df = pd.read_csv(fpath.poten_litera_db_ranked_by_tak, header=0, sep='\\t')  # Replace with your CSV file path\n",
    "\n",
    "# index_above_threshold = []\n",
    "# index_below_threshold = []\n",
    "\n",
    "# # flag of being relevant\n",
    "# flag = True\n",
    "# # Iterate through the DataFrame and add the index to the list\n",
    "# for ind, row in df.iterrows():\n",
    "#     index = int(row['INDEX'])\n",
    "    \n",
    "#     if not flag and (index == threshold_index):\n",
    "#         raise Exception(\"The index of the article with the threshold relevance index is not unique!\")\n",
    "    \n",
    "#     if index == threshold_index:\n",
    "#         index_above_threshold.append(index)\n",
    "#         flag = False\n",
    "#     else: \n",
    "#         if flag:\n",
    "#             index_above_threshold.append(index)\n",
    "#         else:\n",
    "#             index_below_threshold.append(index)\n",
    "    \n",
    "# print(len(index_above_threshold))\n",
    "# print(len(index_below_threshold))\n",
    "\n",
    "# print(index_above_threshold)\n",
    "# print(index_below_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add the index of the articles with 'inject' in the full text and articles without full text\n",
    "# articles_with_keyword = []\n",
    "# articles_short = []\n",
    "# articles_without_full_text = []\n",
    "\n",
    "# for article_index in index_below_threshold:\n",
    "#     full_text_file_path = os.path.join(fpath.text_folder, str(article_index)+'.txt')\n",
    "    \n",
    "#     # if the full text file exists\n",
    "#     if os.path.exists(full_text_file_path):\n",
    "#         with open(full_text_file_path, 'r', encoding='ascii') as file:\n",
    "#             # Read the contents of the file\n",
    "#             text = file.read()\n",
    "#         text = plib.process_text(text, lower=True)\n",
    "        \n",
    "#         species = False\n",
    "#         inject = False\n",
    "#         thalam = False\n",
    "        \n",
    "#         for word in ['rhesus', 'macaque', 'macaca']:\n",
    "#             if word in text:\n",
    "#                 species = True\n",
    "        \n",
    "#         if 'inject' in text:\n",
    "#             inject = True\n",
    "            \n",
    "#         if 'thalam' in text:\n",
    "#             thalam = True\n",
    "            \n",
    "#         if species and inject and thalam and len(text.split()) >= 3000:\n",
    "#             articles_with_keyword.append(article_index)\n",
    "#         if len(text.split()) < 3000:\n",
    "#             articles_short.append(article_index)\n",
    "#     else:\n",
    "#         articles_without_full_text.append(article_index)\n",
    "\n",
    "# print(len(articles_with_keyword))\n",
    "# print(len(articles_short))\n",
    "# print(len(articles_without_full_text))\n",
    "# print(articles_with_keyword)\n",
    "# print(articles_short)\n",
    "# print(articles_without_full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the articels above the threshold index, articles with short text and articles with no text are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_above_threshold = index_above_threshold + articles_short + articles_without_full_text\n",
    "# print(len(index_above_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the index_above_threshold\n",
    "# with open(fpath.article_list_to_manually_read, 'w') as file:\n",
    "#     # Convert each number to string and join them with commas\n",
    "#     numbers_str = ','.join(map(str, index_above_threshold))\n",
    "    \n",
    "#     # Write the string to the file\n",
    "#     file.write(numbers_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc9ccf",
   "metadata": {},
   "source": [
    "<h3> Next step: manually read papers and find all actually related literature </h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
