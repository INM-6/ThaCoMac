{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "<h4> 4. Training and testing data set split </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # select 300 random papers from poten_litera_ids_ftl_filled_filtered for testing\n",
    "# source_path = fpath.poten_litera_ids_ftl_filled_filtered\n",
    "# output_path = fpath.poten_litera_testing_set_300\n",
    "\n",
    "# # clear the file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "# df = df.sample(n=300, random_state=1, axis='index', ignore_index=False)\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df.to_csv(output_path, header=True, index=False)\n",
    "# # --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_testing_set_300\n",
    "# df = pd.read_csv(source_path, header=0, sep=',')\n",
    "# print(df.shape)\n",
    "# # (300, 12)\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # process, correct the INDEX in potential_related_literature_testing_set_300_read.csv\n",
    "# input_path = fpath.poten_litera_testing_set_300_read\n",
    "# output_path = fpath.poten_litera_testing_set_300_read_index_corrected\n",
    "# # plib.clear_file(output_path)\n",
    "# db_path = fpath.poten_litera_db\n",
    "\n",
    "# df_input = pd.read_csv(input_path, header=0, sep=',')\n",
    "# df_input.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANCE\"]\n",
    "# df_db = pd.read_csv(db_path, header=None, sep=',')\n",
    "# df_db.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "# df_db = df_db.fillna(0)\n",
    "# df_db = df_db.astype({\"PMID\": int})\n",
    "# # print(df_input.shape)\n",
    "# # print(df_input.head(5))\n",
    "# # (300, 12)\n",
    "# # print(df_db.shape)\n",
    "# # print(df_db.head(10))\n",
    "# # (10776, 11)\n",
    "\n",
    "# for ind in df_input.index:\n",
    "#     index = df_input.at[ind, \"INDEX\"]\n",
    "#     doi = df_input.at[ind, \"DOI\"]\n",
    "#     pmid = df_input.at[ind, \"PMID\"]\n",
    "#     # print(pmid, df_db.at[ind, \"PMID\"])\n",
    "#     # print(pmid.type(), df_db.at[ind, \"PMID\"].type())\n",
    "#     pmcid = df_input.at[ind, \"PMCID\"]\n",
    "#     full_text_url = df_input.at[ind, \"FULL_TEXT_URL\"]\n",
    "#     full_text_source = df_input.at[ind, \"FULL_TEXT_SOURCE\"]\n",
    "#     title = df_input.at[ind, \"TITLE\"].lower()\n",
    "\n",
    "#     if doi == doi:\n",
    "#         try:\n",
    "#             index = df_db.loc[df_db[\"DOI\"] == doi, 'INDEX'].values[0]\n",
    "#             df_input.at[ind, \"INDEX\"] = index\n",
    "#         except:\n",
    "#             print(\"DOI not found in db:\", df_input.at[ind, \"INDEX\"], df_input.at[ind, \"RELEVANCE\"])\n",
    "#             df_input.drop(ind, inplace=True)\n",
    "#     elif pmid == pmid:\n",
    "#         try:\n",
    "#             index = df_db.loc[int(df_db[\"PMID\"])==int(pmid), 'INDEX'].values[0]\n",
    "#             df_input.at[ind, \"INDEX\"] = index\n",
    "#         except:\n",
    "#             print(\"PMID not found in db:\", df_input.at[ind, \"INDEX\"], df_input.at[ind, \"RELEVANCE\"])\n",
    "#             df_input.drop(ind, inplace=True)\n",
    "#     elif pmcid == pmcid:\n",
    "#         index = df_db.loc[df_db[\"PMCID\"] == pmcid, 'INDEX'].values[0]\n",
    "#         df_input.at[ind, \"INDEX\"] = index\n",
    "#     elif title.lower() == title.lower():\n",
    "#         index = df_db.loc[df_db[\"TITLE\"].str.lower() == title, 'INDEX'].values[0]\n",
    "#         df_input.at[ind, \"INDEX\"] = index\n",
    "#     else:\n",
    "#         print(\"ALL 4 identifiers and title are missing:\", df_input.at[ind, \"INDEX\"], df_input.at[ind, \"RELEVANCE\"])\n",
    "    \n",
    "# df_input.reset_index(drop=True, inplace=True)\n",
    "# df_input.to_csv(output_path, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check the corrected file\n",
    "# corrected = fpath.poten_litera_testing_set_300_read_index_corrected\n",
    "# df_input = pd.read_csv(corrected, header=0, sep=',')\n",
    "# print(df_input.shape)\n",
    "# # (292, 12)\n",
    "# print(df_input.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test if the result matches the db\n",
    "# result_path = fpath.poten_litera_testing_set_300_read_index_corrected\n",
    "# db_path = fpath.poten_litera_db\n",
    "\n",
    "# df_result= pd.read_csv(result_path, header=0, sep=',')\n",
    "# df_result.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANCE\"]\n",
    "# df_db = pd.read_csv(db_path, header=None, sep=',')\n",
    "# df_db.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "# # print(df_result.shape)\n",
    "# # print(df_result.head(5))\n",
    "# # (292, 12)\n",
    "# # print(df_db.shape)\n",
    "# # print(df_db.head(10))\n",
    "# # (10776, 11)\n",
    "\n",
    "# for ind in df_result.index:\n",
    "#     index = int(df_result.at[ind, \"INDEX\"])\n",
    "#     title = df_result.at[ind, \"TITLE\"].lower()\n",
    "#     # title = ''.join([char for char in df_result.at[ind, \"TITLE\"].lower() if re.match(r'[a-z\\s-]', char)])\n",
    "#     cleaned_title = re.sub(r'\\s+', ' ', title).strip().replace(\".\", \"\")\n",
    "#     title_db = df_db.loc[df_db[\"INDEX\"].astype(int) == index, 'TITLE'].values[0].lower()\n",
    "#     # title_db = ''.join([char for char in df_db.loc[df_db[\"INDEX\"].astype(int) == index, 'TITLE'].values[0].lower() if re.match(r'[a-z\\s-]', char)])\n",
    "#     cleaned_title_db = re.sub(r'\\s+', ' ', title_db).strip().replace(\".\", \"\")\n",
    "    \n",
    "#     if cleaned_title == cleaned_title_db:\n",
    "#         pass\n",
    "#     else:\n",
    "#         # pass\n",
    "#         print(index)\n",
    "#         print(cleaned_title)\n",
    "#         print(cleaned_title_db)\n",
    "#         print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # select another 708 random papers from poten_litera_db to form 1000 papers as training and testing set\n",
    "# db_path = fpath.poten_litera_db\n",
    "# test_300_path = fpath.poten_litera_testing_set_300_read_index_corrected\n",
    "# test_708_path = fpath.poten_litera_testing_set_708\n",
    "# plib.clear_file(test_708_path)\n",
    "\n",
    "# df_db = pd.read_csv(db_path, header=None, sep=',')\n",
    "# df_db.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "# df_300 = pd.read_csv(test_300_path, header=0, sep=',')\n",
    "# df_300.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANCE\"]\n",
    "\n",
    "# # Get the indices of the previously selected 300 rows\n",
    "# selected_indices = df_300['INDEX'].values\n",
    "# # print(selected_indices)\n",
    "\n",
    "# # Drop the previously selected 300 rows from the original dataframe\n",
    "# df_remaining = df_db[~df_db['INDEX'].isin(selected_indices)]\n",
    "\n",
    "# # Randomly sample 708 rows from the remaining rows\n",
    "# df_708 = df_remaining.sample(n=708, random_state=42)  # Change random_state if needed\n",
    "# df_708['RELEVANCE'] = np.nan\n",
    "# df_708.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# df_708.to_csv(test_708_path, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine and obtain 1000 papers as training-test-set\n",
    "# test_300_path = fpath.poten_litera_testing_set_300_read_index_corrected\n",
    "# test_708_path = fpath.poten_litera_testing_set_708\n",
    "# test_1000_path = fpath.poten_litera_testing_set_1000\n",
    "# plib.clear_file(test_1000_path)\n",
    "\n",
    "# df_300 = pd.read_csv(test_300_path, header=0, sep=',')\n",
    "# df_708 = pd.read_csv(test_708_path, header=0, sep=',')\n",
    "# df_1000 = pd.concat([df_300, df_708], axis=0)\n",
    "# df_1000.reset_index(drop=True, inplace=True)\n",
    "# df_1000.to_csv(test_1000_path, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if there're duplicates in the 1000 papers\n",
    "# test_1000_path = fpath.poten_litera_testing_set_1000\n",
    "# df_1000 = pd.read_csv(test_1000_path, header=0, sep=',')\n",
    "# df_1000.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANCE\"]\n",
    "# print(df_1000.shape)\n",
    "# # (1000, 12)\n",
    "\n",
    "# print(len(set(df_1000['INDEX'])))\n",
    "# # 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test if poten_litera_testing_set_1000 matches poten_litera_db\n",
    "# test_1000_path = fpath.poten_litera_testing_set_1000\n",
    "# db_path = fpath.poten_litera_db\n",
    "\n",
    "# df_result= pd.read_csv(test_1000_path, header=0, sep=',')\n",
    "# df_result.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANCE\"]\n",
    "# df_db = pd.read_csv(db_path, header=None, sep=',')\n",
    "# df_db.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "# # print(df_result.shape)\n",
    "# # print(df_result.head(5))\n",
    "# # # (1000, 12)\n",
    "# # print(df_db.shape)\n",
    "# # print(df_db.head(10))\n",
    "# # # (10776, 11)\n",
    "\n",
    "# for ind in df_result.index:\n",
    "#     index = int(df_result.at[ind, \"INDEX\"])\n",
    "#     title = df_result.at[ind, \"TITLE\"].lower()\n",
    "#     # title = ''.join([char for char in df_result.at[ind, \"TITLE\"].lower() if re.match(r'[a-z\\s-]', char)])\n",
    "#     cleaned_title = re.sub(r'\\s+', ' ', title).strip().replace(\".\", \"\")\n",
    "#     title_db = df_db.loc[df_db[\"INDEX\"].astype(int) == index, 'TITLE'].values[0].lower()\n",
    "#     # title_db = ''.join([char for char in df_db.loc[df_db[\"INDEX\"].astype(int) == index, 'TITLE'].values[0].lower() if re.match(r'[a-z\\s-]', char)])\n",
    "#     cleaned_title_db = re.sub(r'\\s+', ' ', title_db).strip().replace(\".\", \"\")\n",
    "    \n",
    "#     if cleaned_title == cleaned_title_db:\n",
    "#         pass\n",
    "#     else:\n",
    "#         # pass\n",
    "#         print(index)\n",
    "#         print(cleaned_title)\n",
    "#         print(cleaned_title_db)\n",
    "#         print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # draw the distribution of the sampled 1000 papers to see if it's actually randomly sampled\n",
    "# test_1000_path = fpath.poten_litera_testing_set_1000\n",
    "# df_1000 = pd.read_csv(test_1000_path, header=0, sep=',')\n",
    "# df_1000.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANCE\"]\n",
    "\n",
    "# index_list = df_1000['INDEX'].tolist()\n",
    "# index_list.sort()\n",
    "# # print(index_list)\n",
    "# print(len(index_list))\n",
    "\n",
    "# # draw the histogram\n",
    "# plt.hist(index_list, bins=10)\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.title(\"Distribution of the sampled 1000 papers\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h3> Next step: automatic filtering </h3>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
