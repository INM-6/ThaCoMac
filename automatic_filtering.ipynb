{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea1cce5-4f07-4bd7-8ca3-5f6fa51254d0",
   "metadata": {},
   "source": [
    "<h2> Automatic filtering </h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ebb242",
   "metadata": {},
   "source": [
    "<h3> Notebook description: </h3>\n",
    "Some description text here to write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28e51706-2334-49bb-8c1e-4939b52c60b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import internal .py files\n",
    "import file_path_management as fpath\n",
    "import public_library as plib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad5a61c5-f8c6-418a-b450-cdea0378ddab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99735798",
   "metadata": {},
   "source": [
    "<h3> Parameters: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22d9559b-7ccd-48b5-9bd0-422f6d7c7644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# on-topic keyword lexicon\n",
    "on_topic_kws = ['thalamocortical', 'thalamo-cortical', 'corticothalamic', 'cortico-thalamic',\n",
    "                'tracing', 'tracer', 'tract tracing', 'tract-tracing', 'axonal tracing', 'neural tracing', 'anatomical tracing', 'anatomical neural tracing',\n",
    "                'connection', 'projection', 'connectivity', 'connectome', \n",
    "                'thalamus', 'cortex', 'thalamic', 'cortical']\n",
    "\n",
    "related_kws_weights = {'tracing': 100000, 'tracer': 100000, 'tract tracing': 100000, 'tract-tracing': 100000, 'axonal tracing': 100000, 'neural tracing': 100000, 'anatomical tracing': 100000, 'anatomical neural tracing': 100000,\n",
    "                       'thalamocortical': 1000, 'thalamo-cortical': 1000, 'corticothalamic': 1000, 'cortico-thalamic': 1000,\n",
    "                       'connection': 10, 'projection': 10, 'connectivity': 10, 'connectome': 10, \n",
    "                       'thalamus': 1, 'cortex': 1, 'thalamic': 1, 'cortical': 1}\n",
    "\n",
    "# ChatGPT, queries for relatedness of topic\n",
    "ChatGPT_related_queries = ['Does the given text include information of thalamocotical connection?',\n",
    "                           'Does this paper provide data of thalamocotical connection?',\n",
    "                           'Does the given text include information of connection between thalamus and cortex?']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489f0451",
   "metadata": {},
   "source": [
    "<h3> Predefined fucntions: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8837aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# four_identifier_fill_in\n",
    "def four_identifier_fill_in():\n",
    "    None\n",
    "    print(\"All 4 identifiers filled in when possible.\")\n",
    "# end of four_identifier_fill_in\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c8e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplciations based on identifiers in the potential related literature\n",
    "def remove_dupli():\n",
    "    print(\"Duplication in the potential related literature removed.\")\n",
    "# end of remove_dupli\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce32f7b8-96b7-4f5c-a2da-3ff931cdeaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of times that certain on-topic keyword appear in a given text\n",
    "def count_keyword(text: str, keyword: str) -> int:\n",
    "    # print(text)\n",
    "    # remove spaces before and after the text and split the string by word\n",
    "    text = text.strip().split(\" \")\n",
    "    word_count = 0\n",
    "    for word in text:\n",
    "        # print(word)\n",
    "        if word == keyword:\n",
    "            word_count += 1\n",
    "    return word_count\n",
    "# end of count_keyword\n",
    "# --------------------start of test code--------------------\n",
    "# text = 'This apple 6i7s very tasty？、  2but th&e banana is not delicious at all.6'\n",
    "# keyword = 'is'\n",
    "# count = count_keyword(text, keyword)\n",
    "# print(count)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b488729c-2e38-43e8-bba2-d81126f38847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of times all on-topic keywords appear in the text\n",
    "# extracted from the given url\n",
    "def count_freq_from_liter(text, on_topic_kws, type):\n",
    "    print(text)\n",
    "    text_length = len(text)\n",
    "    keywords_count_fre = {}\n",
    "    # count the on-topic keywords\n",
    "    for i in range(len(on_topic_kws)):\n",
    "        word_count = count_keyword(text, on_topic_kws[i])\n",
    "        if type == \"count\":\n",
    "            keywords_count_fre[on_topic_kws[i]] = word_count\n",
    "        elif type == \"frequency\":\n",
    "            keywords_count_fre[on_topic_kws[i]] = word_count/text_length\n",
    "    return keywords_count_fre\n",
    "# end of count_freq_from_liter\n",
    "# --------------------start of test code--------------------\n",
    "# text = ''\n",
    "# keywords_count_fre = count_freq_from_liter(text, on_topic_kws)\n",
    "# print(keywords_count_fre)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93f93d7d-bbbc-4afe-94f7-f12cca267a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan each url in list_of_literature_urls.txt and record information and download pdf\n",
    "def scan_download_record(on_topic_kws, pdf_folder_path):\n",
    "    # scam each row in the potential related literature and extract information\n",
    "    # print(soup.find_all(\"a\", {'class': 'id-link'}, href = True)[1]['href'])\n",
    "    doi = soup.find_all(\"a\", {'class': 'id-link'}, href = True)[1]['href']\n",
    "    # extract title\n",
    "    title = soup.select('h1')[0].get_text().strip()\n",
    "    title = re.sub(' +', ' ', title).capitalize()\n",
    "\n",
    "\n",
    "    # extract title, abstract, keywords, introduction from the returned html file\n",
    "    # count keywords from abstract + keywords\n",
    "    abs_kws = soup.find_all(\"div\", {'class': 'abstract'})[0].get_text()\n",
    "    abs_kws = abs_kws.strip()\n",
    "    abs_kws = re.sub(' +', ' ', abs_kws)\n",
    "    text = title + ' ' + abs_kws\n",
    "    text = re.sub(r\"[^a-zA-Z' ']\",\"\",text).lower()\n",
    "    \n",
    "    # record the information into json\n",
    "    info_json = {}\n",
    "    info_json['DOI'] = doi,\n",
    "    info_json['url'] = url,\n",
    "    info_json['title'] = title\n",
    "    # info_json['pdf_link'] = pdf_link\n",
    "    columns = ['DOI', 'url', 'title'] + on_topic_kws\n",
    "    file_index = 0\n",
    "    with open(path_urls, 'r') as url_file:\n",
    "        for url in url_file:\n",
    "            # print(url)\n",
    "            info_json = {}\n",
    "            info_json = count_freq_from_liter(url.strip(), on_topic_kws)\n",
    "            fpath.add_row_to_csv(fpath.poten_litera_csv, info_json, columns)\n",
    "            # download_pdf_file(info_json['pdf_link'], pdf_folder_path, str(file_index))\n",
    "            file_index += 1\n",
    "    keywords_count_fre = count_freq_from_liter(text, on_topic_kws)\n",
    "# end of scan_record_download\n",
    "# --------------------start of test code--------------------\n",
    "# scan_record_download(path_urls, on_topic_kws, pdf_folder_path)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e08f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_and_rank(weights_dict):\n",
    "    # weight formula\n",
    "    print(\"Enjoy reading!\")\n",
    "# end of weight_and_rank\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e562971",
   "metadata": {},
   "source": [
    "<h3> Main program: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c191fa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to iterate every row, fill in the empty cells and search for the text information we need \n",
    "# according to existing information\n",
    "# what we need is: DOI, PMID, PMCID, Title, Authors, Abstract, Keywords, full_text_url, pdf_url\n",
    "\n",
    "# step 1: fill in empty cells of the four identifiers: DOI, PMID, PMCID, Title and make sure the title is lower case\n",
    "four_identifier_fill_in()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1247c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: remove duplications according to identifiers\n",
    "remove_dupli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a01765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: search for text information of the literature\n",
    "# download the .pdf file when available and \n",
    "# record the keywords matching results\n",
    "scan_download_record(on_topic_kws, fpath.litera_pdf_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9493b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: assign weight to each literature and rank them\n",
    "weight_and_rank(related_kws_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b8d7e0",
   "metadata": {},
   "source": [
    "<h3> Some test code, please ignore: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24db482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# if \"//doi.org/\" in \"https://doi.org/10.1016/0165-0173(96)00003-3\":\n",
    "#     print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77cc0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test redirect when access the doi link\n",
    "# from elsapy.elsdoc import FullDoc, AbsDoc\n",
    "# from elsapy.elsclient import ElsClient\n",
    "# import json\n",
    "# headers = {\n",
    "#     \"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9\", \n",
    "#     \"X-ELS-APIKEY\": \"310946e6e005957982c2c9cad6833ad3\",\n",
    "#     \"Accept\": \"application/pdf\",\n",
    "#     \"X-ELS-Insttoken\": \"instToken\",\n",
    "#     \"view\": \"FULL\"\n",
    "# } \n",
    "# # url = \"https://www.jneurosci.org/content/28/43/11042.short\"\n",
    "#  #url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2613515/\"\n",
    "\n",
    "# # Journal of Neurophysiology\n",
    "# # url = \"https://doi.org/10.1152/jn.2001.85.1.219\"\n",
    "# # url = \"https://journals.physiology.org/doi/10.1152/jn.2001.85.1.219\"\n",
    "\n",
    "# # science direct\n",
    "# # url = \"https://doi.org/10.1016/j.biopsych.2004.10.014\"\n",
    "# # url = \"https://linkinghub.elsevier.com/retrieve/pii/S0006322304010947\"\n",
    "# # url = \"https://www.sciencedirect.com/science/article/pii/S0006322304010947?via%3Dihub\"\n",
    "# url = \"https://api.elsevier.com/content/article/doi/{10.1016/j.biopsych.2004.10.014}\"\n",
    "\n",
    "# # response = requests.get(url, headers = headers)\n",
    "# # soup = BeautifulSoup(response.content,\"lxml\")\n",
    "# # print(soup)\n",
    "# # print(response.history)\n",
    "# # print(response.url)\n",
    "# # # Load configuration\n",
    "# # con_file = open(\"config.json\")\n",
    "# # config = json.load(con_file)\n",
    "# # con_file.close()\n",
    "\n",
    "# # response = requests.get(url, headers = headers)\n",
    "# # print(response)\n",
    "\n",
    "# # ## Initialize client\n",
    "# # client = ElsClient(config[\"apikey\"])\n",
    "\n",
    "# # ## ScienceDirect (full-text) document example using DOI\n",
    "# # doi_doc = FullDoc(doi = \"10.1016/j.biopsych.2004.10.014\")\n",
    "# # print(doi_doc)\n",
    "# # if doi_doc.read(client):\n",
    "# #     print (\"doi_doc.title: \", doi_doc.title)\n",
    "# #     doi_doc.write(\"doi_doc\")   \n",
    "# # else:\n",
    "# #     print (\"Read document failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c1c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find DOI\n",
    "# # this link does not have \"DOI\" in href form but text from\n",
    "# url = \"https://www.jneurosci.org/content/28/43/11042.short\"\n",
    "# # url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2613515/\"\n",
    "# response = requests.get(url, headers = plib.headers)\n",
    "# soup = BeautifulSoup(response.content,\"lxml\")\n",
    "# # print(soup)\n",
    "# doi_list = []\n",
    "# num_results_str = soup.select(\"a\", href = True)\n",
    "# # print(num_results_str)\n",
    "# for item in num_results_str:\n",
    "#     if \"//doi.org/\" in item[\"href\"]:\n",
    "#         print(item[\"href\"])\n",
    "#         doi_list.append(item[\"href\"].split(\"//doi.org/\")[1])\n",
    "\n",
    "# print(doi_list)\n",
    "        \n",
    "# if len(doi_list) == 0:\n",
    "#     print(\"Ops! Did't find DOI on this page!\")\n",
    "\n",
    "\n",
    "\n",
    "# test extract doi from url\n",
    "# with open(fpath.gs_poten_urls, \"r\") as file:\n",
    "#     lines = []\n",
    "#     for line in file:\n",
    "#         print(line)\n",
    "#         line = line.strip()\n",
    "#         lines.append(line)\n",
    "# print(len(lines))\n",
    "# doi_list = []\n",
    "# for url in lines:\n",
    "#     response = requests.get(url, headers = plib.headers)\n",
    "#     while\n",
    "#     soup = BeautifulSoup(response.content,\"lxml\")\n",
    "#     # print(soup)\n",
    "#     num_results_str = soup.select(\"a\", href = True)\n",
    "#     print(num_results_str)\n",
    "#     for href in num_results_str:\n",
    "#         if \"//doi.org/\" in href[\"href\"]:\n",
    "#             doi_list.append(href[\"href\"])\n",
    "#             print(href[\"href\"])\n",
    "#         else:\n",
    "#             print(\"Ops! Did't find DOI on this page!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5131b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # extract PDF link if exists\n",
    "#     print(doi)\n",
    "#     response_pdf = requests.get(doi, headers = headers)\n",
    "#     print(response_pdf.url)\n",
    "#     pdf_page_link = response_pdf.url\n",
    "        \n",
    "#     pdf_page = soup.find_all(\"a\", {'class':'link-item dialog-focus'}, href = True)[0]['href']\n",
    "    \n",
    "#     # print(pdf_page_link)\n",
    "#     pdf_page = requests.get(pdf_page_link, headers = headers)\n",
    "#     soup_pdf = BeautifulSoup(pdf_page.content,'lxml')\n",
    "#     print(len(soup_pdf.find_all(\"a\", href = True)))\n",
    "#     pdf_link = soup_pdf.find_all(\"a\", href = True)[0]['href']\n",
    "    \n",
    "    \n",
    "#     print(pdf_link)\n",
    "#     pdf_link = 'https://www.ncbi.nlm.nih.gov' + pdf_link"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
