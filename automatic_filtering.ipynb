{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea1cce5-4f07-4bd7-8ca3-5f6fa51254d0",
   "metadata": {},
   "source": [
    "<h2> Automatic filtering </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e51706-2334-49bb-8c1e-4939b52c60b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import internal modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib\n",
    "import extract_info\n",
    "import parameters as params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5a61c5-f8c6-418a-b450-cdea0378ddab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489f0451",
   "metadata": {},
   "source": [
    "<h3> Predefined fucntions: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f93d7d-bbbc-4afe-94f7-f12cca267a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_filling(input_path, output_path, start, end):\n",
    "    # scan each row in the potential related literature and extract information\n",
    "    df = pd.read_csv(input_path, header=None, sep=\",\")\n",
    "    df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # info = {\n",
    "        #     \"doi\": np.nan,\n",
    "        #     \"pmid\": np.nan,\n",
    "        #     \"pmcid\": np.nan,\n",
    "        #     \"title\": np.nan,\n",
    "        #     \"abstract\": np.nan,\n",
    "        #     \"keywords\": np.nan,\n",
    "        #     \"pdf_link\": np.nan\n",
    "        # }\n",
    "\n",
    "        # initialzie\n",
    "        index = df.at[ind, \"INDEX\"]\n",
    "        doi = df.at[ind, \"DOI\"]\n",
    "        pmid = df.at[ind, \"PMID\"]\n",
    "        pmcid = df.at[ind, \"PMCID\"]\n",
    "        full_text_url = df.at[ind, \"FULL_TEXT_URL\"]\n",
    "        full_text_source = df.at[ind, \"FULL_TEXT_SOURCE\"]\n",
    "        pdf_url = np.nan\n",
    "        pdf_source = np.nan\n",
    "        title = df.at[ind, \"TITLE\"]\n",
    "        abstract = df.at[ind, \"ABSTRACT\"]\n",
    "        keywords = df.at[ind, \"KEYWORDS\"]\n",
    "\n",
    "        if full_text_url != full_text_url: # full text url not found\n",
    "            if df.at[ind, \"PDF_URL\"] == df.at[ind, \"PDF_URL\"]:\n",
    "                url = str(df.at[ind, \"PDF_URL\"]).strip()\n",
    "                url1, status_code = plib.get_final_redirected_url(url)\n",
    "                if status_code == 200:\n",
    "                    pdf_url = url\n",
    "                    pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                elif status_code == 403:\n",
    "                    # print(status_code, \"when getting final redirected url: \", url)\n",
    "                    pdf_url = url\n",
    "                    pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                else:\n",
    "                    print(status_code, \"when getting final redirected url: \", url)\n",
    "                    pdf_url = np.nan\n",
    "                    pdf_source = np.nan  \n",
    "            else:\n",
    "                print(\"full text url and pdf url are not available!\")\n",
    "                pdf_url = np.nan\n",
    "                pdf_source = np.nan\n",
    "        elif ('.pdf' in full_text_url and full_text_url.split('.pdf')[1] == '') or ('.PDF' in full_text_url and full_text_url.split('.PDF')[1] == ''):\n",
    "            print(\"full text url is a pdf file: \", full_text_url)\n",
    "            pdf_url = full_text_url\n",
    "            pdf_source = full_text_source\n",
    "            full_text_url = np.nan\n",
    "            full_text_source = np.nan\n",
    "        else: # full text url found\n",
    "            flag = False\n",
    "            for website in params.websites:\n",
    "                if website in full_text_source:\n",
    "                    flag = True\n",
    "                    break\n",
    "            if not flag:\n",
    "                continue\n",
    "\n",
    "            url = str(full_text_url).strip()\n",
    "            try:\n",
    "                info = extract_info.extract_info_from_webpage(url, params.websites)\n",
    "            except:\n",
    "                raise Exception(\"Error! Cannot extract information from the webpage: \", url)\n",
    "            \n",
    "            # doi\n",
    "            if info['doi'] == info['doi'] and doi == doi and info['doi'] != doi:\n",
    "                print(doi)\n",
    "                print(info['doi'])\n",
    "\n",
    "            if info['doi'] == info['doi']:\n",
    "                doi = info['doi'].lower()\n",
    "            else:\n",
    "                doi = doi\n",
    "            \n",
    "            # pmid\n",
    "            if info['pmid'] == info['pmid'] and df.at[ind, \"PMID\"] == df.at[ind, \"PMID\"] and str(int(info['pmid'])) != str(int(df.at[ind, \"PMID\"])):\n",
    "                print(str(int(df.at[ind, \"PMID\"]))) \n",
    "                print(str(int(info['pmid'])))      \n",
    "\n",
    "            if info['pmid'] == info['pmid']:\n",
    "                pmid = str(int(info['pmid']))\n",
    "            elif pmid == pmid:\n",
    "                pmid = str(int(pmid)).strip()\n",
    "            else:\n",
    "                pmid = np.nan\n",
    "            \n",
    "            # pmcid\n",
    "            if info['pmcid'] == info['pmcid'] and pmcid == pmcid and info['pmcid'] != pmcid:\n",
    "                print(pmcid)\n",
    "                print(info['pmcid'])\n",
    "\n",
    "            if info['pmcid'] == info['pmcid']:\n",
    "                pmcid = info['pmcid']\n",
    "            else:\n",
    "                pmcid = df.at[ind, \"PMCID\"]\n",
    "            \n",
    "            # full_text_url, full_text_surce\n",
    "            if full_text_url == full_text_url:\n",
    "                full_text_source = full_text_url.split(\"://\")[1].split(\"/\")[0]\n",
    "            else:\n",
    "                print(\"full text url is not available\")\n",
    "                full_text_url = np.nan\n",
    "                full_text_source = np.nan\n",
    "\n",
    "            if pdf_url != pdf_url and info['pdf_link'] == info['pdf_link']:\n",
    "                pdf_url = str(info['pdf_link']).strip()\n",
    "                pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                # try:\n",
    "                #     pdf_url, status_code = plib.get_final_redirected_url(url)\n",
    "                #     if status_code == 200:\n",
    "                #         pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                #     elif status_code == 403:\n",
    "                #         # print(status_code, \"when getting final redirected url: \", url)\n",
    "                #         pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                #     else:\n",
    "                #         print(status_code, \"when getting final redirected url: \", url)\n",
    "                #         pdf_url = np.nan\n",
    "                #         pdf_source = np.nan\n",
    "                # except:\n",
    "                #     pdf_url = np.nan\n",
    "                #     pdf_source = np.nan \n",
    "                    \n",
    "            if pdf_url != pdf_url and df.at[ind, \"PDF_URL\"] == df.at[ind, \"PDF_URL\"]:\n",
    "                print(\"PDF_URL not extracted from info: , but existed already: \", df.at[ind, \"PDF_URL\"])\n",
    "                pdf_url = df.at[ind, \"PDF_URL\"]\n",
    "                pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "            \n",
    "            if pdf_url != pdf_url:\n",
    "                if full_text_url == full_text_url and full_text_url.split(\"://\")[1].split(\"/\")[0] == 'www.ncbi.nlm.nih.gov':\n",
    "                    if doi == doi:\n",
    "                        url = \"https://doi.org/\" + doi\n",
    "                        url, status_code = plib.get_final_redirected_url(url)\n",
    "                        info = extract_info.extract_info_from_webpage(url, params.websites)\n",
    "                        full_text_url = url\n",
    "                        full_text_source = url.split(\"://\")[1].split(\"/\")[0]\n",
    "                        pdf_url = info[\"pdf_link\"]\n",
    "                        pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                    else:\n",
    "                        pdf_url = np.nan\n",
    "                        pdf_source = np.nan\n",
    "                else:  \n",
    "                    print(\"PDF_URL not found for: \", doi, pmid, pmcid, full_text_url)\n",
    "                    pdf_url = np.nan\n",
    "                    pdf_source = np.nan\n",
    "                \n",
    "            # title\n",
    "            if info['title'] == info['title']:\n",
    "                title = info['title']\n",
    "                title = title.replace(\";\", \",\")\n",
    "            else:\n",
    "                title = title\n",
    "            \n",
    "            # abstract\n",
    "            if info['abstract'] == info['abstract']:\n",
    "                abstract = info['abstract']\n",
    "                abstract = ''.join(e for e in abstract if (e.isalpha() or e == \" \" or e == \"-\"))\n",
    "            else:\n",
    "                abstract = abstract\n",
    "            \n",
    "            # keywords\n",
    "            if info['keywords'] == info['keywords']:\n",
    "                keywords = info['keywords']\n",
    "                keywords = keywords.replace(\";\", \",\")\n",
    "                keywords = ''.join(e for e in keywords if (e.isalpha() or e == \" \" or e == \"-\" or e == \",\"))\n",
    "            else:\n",
    "                keywords = keywords\n",
    "        \n",
    "        columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "        \n",
    "        row = {\n",
    "            \"INDEX\": [index],\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"FULL_TEXT_URL\": [full_text_url],\n",
    "            \"FULL_TEXT_SOURCE\": [full_text_source],\n",
    "            \"PDF_URL\": [pdf_url],\n",
    "            \"PDF_SOURCE\": [pdf_source],\n",
    "            \"TITLE\": [title],\n",
    "            \"ABSTRACT\": [abstract],\n",
    "            \"KEYWORDS\": [keywords]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# input_path = fpath.poten_litera_ids_ftl_filled\n",
    "# output_path = fpath.poten_litera_litera_db\n",
    "\n",
    "# # clear file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# info_filling(input_path, output_path)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d8a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce32f7b8-96b7-4f5c-a2da-3ff931cdeaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_keyword(text, keyword, keyword_length):\n",
    "    # remove non-alphabetic characters but keep spaces and \"-\"\n",
    "    text = ''.join(e for e in text if (e.isalpha() or e == \" \" or e == \"-\"))\n",
    "    # print(text)\n",
    "    text = text.strip().lower()\n",
    "    # print(text)\n",
    "    \n",
    "    words = []\n",
    "    # sentence = 'I have a laptop case and a laptop bag'\n",
    "    n = keyword_length\n",
    "    n_grams = ngrams(text.split(), n)\n",
    "    for gram in n_grams:\n",
    "        word = gram[0]\n",
    "        if n > 0:\n",
    "            for i in range(1, n):\n",
    "                word = word + \" \" + gram[i]\n",
    "        words.append(word)\n",
    "    \n",
    "    # print(words)\n",
    "    \n",
    "    word_count = 0\n",
    "    for word in words:\n",
    "        # print(word)\n",
    "        if word == keyword:\n",
    "            word_count += 1\n",
    "    return word_count\n",
    "# --------------------start of test code--------------------\n",
    "# text = 'This apple 6i7s very tasty？、  2but th&e banana this is not delicious at Is all.6'\n",
    "# keyword = 'this apple'\n",
    "# count = count_keyword(text, keyword, 2)\n",
    "# print(count)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b488729c-2e38-43e8-bba2-d81126f38847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_freq_from_liter(text, on_topic_kws, type):\n",
    "    text = ''.join(e for e in text if (e.isalpha() or e == \" \" or e == \"-\"))\n",
    "    # print(text)\n",
    "    text = text.strip().lower()\n",
    "    # print(text)\n",
    "\n",
    "    text_length = len(text.split(\" \"))\n",
    "    keywords_count = dict()\n",
    "    keywords_fre = dict()\n",
    "\n",
    "    # count the on-topic keywords\n",
    "    for i in range(len(on_topic_kws)):\n",
    "        word_count = count_keyword(text, on_topic_kws[i], len(on_topic_kws[i].split(\" \")))\n",
    "        print(word_count)\n",
    "        if type == \"count\":\n",
    "            keywords_count[on_topic_kws[i]] = word_count\n",
    "        elif type == \"frequency\":\n",
    "            keywords_fre[on_topic_kws[i]] = math.ceil((word_count*100/text_length))/100\n",
    "        else:\n",
    "            raise Exception(\"Error! The only two options for type are 'count' or 'frequency'!\")\n",
    "    \n",
    "    if type == \"count\":\n",
    "        return keywords_count\n",
    "    elif type == \"frequency\":\n",
    "        return keywords_fre\n",
    "    else:\n",
    "        raise Exception(\"Error! The only two options for type are 'count' or 'frequency'!\")\n",
    "# --------------------start of test code--------------------\n",
    "# text = 'Vision for action: thalamic and cortical inputs to the macaque superior tract neural tracing, parietal lobule The dorsal visual stream, the cortical circuit that in the primate brain is mainly dedicated to the visual control of actions, is split into two routes, a lateral and a medial one, both involved in coding different aspects of sensorimotor control of actions. The lateral route, named \"lateral grasping network\", is mainly involved in the control of the distal part of prehension, namely grasping and manipulation. The medial route, named \"reach-to-grasp network\", is involved in the control of the full deployment of prehension act, from the direction of arm movement to the shaping of the hand according to the object to be grasped. In macaque monkeys, the reach-to-grasp network (the target of this review) includes areas of the superior parietal lobule (SPL) that hosts visual and somatosensory neurons well suited to control goal-directed limb movements toward stationary as well as moving objects. After a brief summary of the neuronal functional properties of these areas, we will analyze their cortical and thalamic inputs thanks to retrograde neuronal tracers separately injected into the SPL areas V6, V6A, PEc, and PE. These areas receive visual and somatosensory information distributed in a caudorostral, visuosomatic trend, and some of them are directly connected with the dorsal premotor cortex. This review is particularly focused on the origin and type of visual information reaching the SPL, and on the functional role this information can play in guiding limb interaction with objects in structured and dynamic environments. Area PEc; Area V6; Area V6A; Dorsal visual stream; Goal-directed arm movement; Sensorimotor integration.'\n",
    "# keywords_count_fre = count_freq_from_liter(text, params.on_topic_kws, type=\"count\")\n",
    "# print(keywords_count_fre)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02fb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul_index(keywords_count_or_fre, on_topic_kws_weights):\n",
    "    index = 0\n",
    "    for key in keywords_count_or_fre.keys():\n",
    "        index += keywords_count_or_fre[key] * on_topic_kws_weights[key]\n",
    "    return index\n",
    "# --------------------start of test code--------------------\n",
    "# keywords_count_or_fre = {}\n",
    "# index = calcul_related(keywords_count_or_fre, params.on_topic_kws_weights)\n",
    "# print(index)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e08f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_and_rank(input_path, output_path, on_topic_kws_weights):\n",
    "    df = pd.read_csv(input_path, header=None, sep=\",\")\n",
    "    df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_link\", \"full_text_source\", \"pdf_link\", \"Title\", \"Abstract\", \"Keywords\", \"index\"]\n",
    "\n",
    "    for ind in df.index:\n",
    "        text = \"\"\n",
    "        if df.at[ind, \"Title\"] == df.at[ind, \"Title\"]:\n",
    "            text += df.at[ind, \"Title\"] + \" \"\n",
    "        if df.at[ind, \"Abstract\"] == df.at[ind, \"Abstract\"]:\n",
    "            text += df.at[ind, \"Abstract\"] + \" \"\n",
    "        if df.at[ind, \"Keywords\"] == df.at[ind, \"Keywords\"]:\n",
    "            text += df.at[ind, \"Keywords\"] + \" \"\n",
    "        # print(text)\n",
    "        # type = \"count\"\n",
    "        type = \"frequency\"\n",
    "        keywords_count_or_fre = count_freq_from_liter(text, params.on_topic_kws, type)\n",
    "        index = calcul_index(keywords_count_or_fre, on_topic_kws_weights)\n",
    "        # print(index)\n",
    "        df.at[ind, \"index\"] = index\n",
    "        # print(ind)\n",
    "    \n",
    "    # rank\n",
    "    df = df.sort_values(by=[\"index\"], ascending=False)\n",
    "    df.to_csv(output_path, header=True, index=False)\n",
    "    print(\"Weighting and ranking the potentially related literature succeded!\")\n",
    "    print(\"Enjoy reading!\")\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e562971",
   "metadata": {},
   "source": [
    "<h3> Main program: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2836c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: extract and filling info\n",
    "input_path = fpath.poten_litera_ids_ftl_filled_filtered\n",
    "output_path = fpath.poten_litera_litera_db\n",
    "\n",
    "# clear file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "info_filling(input_path, output_path, 2116, 10980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0e62a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # process the resuls: remove duplicates and reset index\n",
    "# input_path = fpath.poten_litera_litera_db\n",
    "# output_path = fpath.poten_litera_litera_db\n",
    "\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "# identifiers = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\"]\n",
    "\n",
    "# for identifier in identifiers:\n",
    "#     remove_dup_by = identifier\n",
    "#     df = df[df[remove_dup_by].isnull() | ~df[df[remove_dup_by].notnull()].duplicated(subset=remove_dup_by, keep='last')]\n",
    "\n",
    "# # reset index\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# df.to_csv(output_path, header=False, index=False)\n",
    "# print(\"Duplication removed.\")\n",
    "# # --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_litera_db\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# print(df.head(5))\n",
    "# print(df.shape)\n",
    "# # (10147, 11)\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5870a3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # list all pdf source\n",
    "# input_path = fpath.poten_litera_litera_db\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "# pdf_source_set = set(df['PDF_SOURCE'].tolist())\n",
    "# print(pdf_source_set)\n",
    "# # {'journals.physiology.org', 'zsp.com.pk', 'www.ncbi.nlm.nih.gov', 'nyaspubs.onlinelibrary.wiley.com', \n",
    "# # 'journals.lww.com', 'ruor.uottawa.ca', 'www.ijpp.com', 'synapse.koreamed.org', 'citeseerx.ist.psu.edu', \n",
    "# # 'ELSEVIER', 'www.cell.com', 'www.hifo.uzh.ch', 'movementdisorders.onlinelibrary.wiley.com', \n",
    "# # 'analyticalsciencejournals.onlinelibrary.wiley.com', 'anatomypubs.onlinelibrary.wiley.com', nan, \n",
    "# # 'onlinelibrary.wiley.com', 'link.springer.com', 'physoc.onlinelibrary.wiley.com', 'www.annualreviews.org', \n",
    "# # 'bpb-us-e1.wpmucdn.com'}\n",
    "# website_pdf = [\n",
    "#     'journals.physiology.org', 'zsp.com.pk', 'www.ncbi.nlm.nih.gov', 'nyaspubs.onlinelibrary.wiley.com', \n",
    "#     'journals.lww.com', 'ruor.uottawa.ca', 'www.ijpp.com', 'synapse.koreamed.org', 'citeseerx.ist.psu.edu', \n",
    "#     'ELSEVIER', 'www.cell.com', 'www.hifo.uzh.ch', 'movementdisorders.onlinelibrary.wiley.com', \n",
    "#     'analyticalsciencejournals.onlinelibrary.wiley.com', 'anatomypubs.onlinelibrary.wiley.com', 'nan', \n",
    "#     'onlinelibrary.wiley.com', 'link.springer.com', 'physoc.onlinelibrary.wiley.com', 'www.annualreviews.org', \n",
    "#     'bpb-us-e1.wpmucdn.com'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4bab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sort the websites by the number of pdfs they have\n",
    "# input_path = fpath.poten_litera_litera_db\n",
    "# df = pd.read_csv(input_path, header=None, sep=\",\")\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "# pdf_source_dict = {website: 0 for website in website_pdf}\n",
    "\n",
    "# for ind in df.index:\n",
    "#     if df.at[ind, \"PDF_SOURCE\"] != df.at[ind, \"PDF_SOURCE\"]:\n",
    "#         pdf_source_dict[\"nan\"] += 1\n",
    "#         continue\n",
    "#     for website in website_pdf:\n",
    "#         if website in df.at[ind, \"PDF_SOURCE\"]:\n",
    "#             pdf_source_dict[website] += 1\n",
    "#             break\n",
    "\n",
    "# # Sort dictionary by values\n",
    "# sorted_dict = dict(sorted(pdf_source_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "# print(sorted_dict)\n",
    "# # {'www.ncbi.nlm.nih.gov': 7614, 'ELSEVIER': 1021, 'onlinelibrary.wiley.com': 648, 'nan': 323, 'link.springer.com': 270, \n",
    "# # 'journals.physiology.org': 216, 'nyaspubs.onlinelibrary.wiley.com': 17, 'www.cell.com': 10, \n",
    "# # 'movementdisorders.onlinelibrary.wiley.com': 8, 'analyticalsciencejournals.onlinelibrary.wiley.com': 7, \n",
    "# # 'journals.lww.com': 2, 'citeseerx.ist.psu.edu': 2, 'anatomypubs.onlinelibrary.wiley.com': 2, 'zsp.com.pk': 1, \n",
    "# # 'ruor.uottawa.ca': 1, 'www.ijpp.com': 1, 'synapse.koreamed.org': 1, 'www.hifo.uzh.ch': 1, 'www.annualreviews.org': 1, \n",
    "# # 'bpb-us-e1.wpmucdn.com': 1, 'physoc.onlinelibrary.wiley.com': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daccd342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # step 2: download pdfs and rename them to build a database\n",
    "# download_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9493b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # step 3: assign index to each literature and rank them\n",
    "# input_path = fpath.poten_litera_litera_db\n",
    "# output_path = fpath.poten_litera_litera_db_ranked\n",
    "\n",
    "# # clear file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# weight_and_rank(input_path, output_path, on_topic_kws_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc9ccf",
   "metadata": {},
   "source": [
    "<h3> Next step: manually read papers and find all actually related literature </h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
