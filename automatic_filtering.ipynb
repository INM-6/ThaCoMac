{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea1cce5-4f07-4bd7-8ca3-5f6fa51254d0",
   "metadata": {},
   "source": [
    "<h2> Automatic filtering </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28e51706-2334-49bb-8c1e-4939b52c60b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import internal .py files\n",
    "import file_path_management as fpath\n",
    "import public_library as plib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad5a61c5-f8c6-418a-b450-cdea0378ddab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "from numpy import NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99735798",
   "metadata": {},
   "source": [
    "<h3> Parameters: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22d9559b-7ccd-48b5-9bd0-422f6d7c7644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# on-topic keyword lexicon\n",
    "on_topic_kws = ['thalamocortical', 'thalamo-cortical', 'corticothalamic', 'cortico-thalamic',\n",
    "                'tracing', 'tracer', 'tract tracing', 'tract-tracing', 'axonal tracing', 'neural tracing', 'anatomical tracing', 'neuroanatomical tracing',\n",
    "                'thalamus', 'cortex', 'thalamic', 'cortical', \"staining\", \"dye\", \n",
    "                'connection', 'projection', 'connectivity', 'connectome', \"anterograde\", \"retrograde\", \"injection\", \"injected\", \"injecting\", \"inject\"]\n",
    "\n",
    "related_kws_weights = {'tracing': 10, 'tracer': 10, 'tract tracing': 10, 'tract-tracing': 10, 'axonal tracing': 10, 'neural tracing': 10, 'anatomical tracing': 10, 'anatomical neural tracing': 10,\n",
    "                       'thalamocortical': 5, 'thalamo-cortical': 5, 'corticothalamic': 5, 'cortico-thalamic': 5,\n",
    "                       'connection': 2, 'projection': 2, 'connectivity': 2, \n",
    "                       'thalamus': 1, 'cortex': 1, 'thalamic': 1, 'cortical': 1, \n",
    "                       'connectome': 1}\n",
    "\n",
    "# false negative, false positive\n",
    "\n",
    "# ChatGPT, queries for relatedness of topic\n",
    "ChatGPT_related_queries = ['Does the given text include information of thalamocotical connection?',\n",
    "                           'Does this paper provide data of thalamocotical connection?',\n",
    "                           'Does the given text include information of connection between thalamus and cortex?']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489f0451",
   "metadata": {},
   "source": [
    "<h3> Predefined fucntions: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce32f7b8-96b7-4f5c-a2da-3ff931cdeaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of times that certain on-topic keyword appear in a given text\n",
    "def count_keyword(text: str, keyword: str) -> int:\n",
    "    # print(text)\n",
    "    # remove spaces before and after the text and split the string by word\n",
    "    text = text.strip().split(\" \")\n",
    "    word_count = 0\n",
    "    for word in text:\n",
    "        # print(word)\n",
    "        if word == keyword:\n",
    "            word_count += 1\n",
    "    return word_count\n",
    "# end of count_keyword\n",
    "# --------------------start of test code--------------------\n",
    "# text = 'This apple 6i7s very tasty？、  2but th&e banana is not delicious at all.6'\n",
    "# keyword = 'is'\n",
    "# count = count_keyword(text, keyword)\n",
    "# print(count)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b488729c-2e38-43e8-bba2-d81126f38847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of times all on-topic keywords appear in the text\n",
    "# extracted from the given url\n",
    "def count_freq_from_liter(text, on_topic_kws, type):\n",
    "    print(text)\n",
    "    text_length = len(text)\n",
    "    keywords_count_fre = {}\n",
    "    # count the on-topic keywords\n",
    "    for i in range(len(on_topic_kws)):\n",
    "        word_count = count_keyword(text, on_topic_kws[i])\n",
    "        if type == \"count\":\n",
    "            keywords_count_fre[on_topic_kws[i]] = word_count\n",
    "        elif type == \"frequency\":\n",
    "            keywords_count_fre[on_topic_kws[i]] = word_count/text_length\n",
    "    return keywords_count_fre\n",
    "# end of count_freq_from_liter\n",
    "# --------------------start of test code--------------------\n",
    "# text = 'Vision for action: thalamic and cortical inputs to the macaque superior parietal lobule The dorsal visual stream, the cortical circuit that in the primate brain is mainly dedicated to the visual control of actions, is split into two routes, a lateral and a medial one, both involved in coding different aspects of sensorimotor control of actions. The lateral route, named \"lateral grasping network\", is mainly involved in the control of the distal part of prehension, namely grasping and manipulation. The medial route, named \"reach-to-grasp network\", is involved in the control of the full deployment of prehension act, from the direction of arm movement to the shaping of the hand according to the object to be grasped. In macaque monkeys, the reach-to-grasp network (the target of this review) includes areas of the superior parietal lobule (SPL) that hosts visual and somatosensory neurons well suited to control goal-directed limb movements toward stationary as well as moving objects. After a brief summary of the neuronal functional properties of these areas, we will analyze their cortical and thalamic inputs thanks to retrograde neuronal tracers separately injected into the SPL areas V6, V6A, PEc, and PE. These areas receive visual and somatosensory information distributed in a caudorostral, visuosomatic trend, and some of them are directly connected with the dorsal premotor cortex. This review is particularly focused on the origin and type of visual information reaching the SPL, and on the functional role this information can play in guiding limb interaction with objects in structured and dynamic environments. Area PEc; Area V6; Area V6A; Dorsal visual stream; Goal-directed arm movement; Sensorimotor integration.'\n",
    "# keywords_count_fre = count_freq_from_liter(text, on_topic_kws)\n",
    "# print(keywords_count_fre)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88f4adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul_related(info_count, related_kws_weights):\n",
    "    weight = 0\n",
    "    for key, value in info_count.items():\n",
    "        weight += info_count[key] * related_kws_weights[key]\n",
    "    return weight\n",
    "# end of def calcul_related(info_count, related_kws_weights):\n",
    "# --------------------start of test code--------------------\n",
    "info_count = {'thalamocortical': 2, 'thalamo-cortical': 2, 'corticothalamic': 0, 'cortico-thalamic': 1,\n",
    "              'tracing': 0, 'tracer': 0, 'tract tracing': 0, 'tract-tracing': 0, 'axonal tracing': 1, 'neural tracing': 1, 'anatomical tracing': 1, 'anatomical neural tracing': 0,\n",
    "              'connection': 1, 'projection': 2, 'connectivity': 0, 'connectome': 0, \n",
    "              'thalamus': 0, 'cortex': 4, 'thalamic': 2, 'cortical': 3}\n",
    "weight = calcul_related(info_count, related_kws_weights)\n",
    "print(weight)\n",
    "# ---------------------end of test code---------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93f93d7d-bbbc-4afe-94f7-f12cca267a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan each url in list_of_literature_urls.txt and record information and download pdf\n",
    "def scan_download_record(on_topic_kws, pdf_folder_path):\n",
    "    # scan each row in the potential related literature and extract information\n",
    "    df = pd.read_csv(fpath.poten_litera_csv, sep = \",\")\n",
    "    for ind in df.index:\n",
    "        if df[\"PMID\"][ind] is not NaN:\n",
    "            pmid = df[\"PMID\"][ind]\n",
    "            url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "            print(url)\n",
    "            response = requests.get(url, headers = plib.headers)\n",
    "            if response.status_code != 200:\n",
    "                raise Exception(\"Error when request webpages!\")\n",
    "            soup = BeautifulSoup(response.content, \"lxml\")\n",
    "            l = soup.find_all(\"a\", {\"class: id-link\"}, {\"data-ga-action\": \"DOI\"})\n",
    "            if(len(l) != 0):\n",
    "                # print(l[0].get_text().strip())\n",
    "                doi = l[0].get_text().strip()\n",
    "            else:\n",
    "                doi = NaN\n",
    "            l = soup.find_all({\"class\": \"current-id\"}, {\"title\": \"PubMed ID\"})\n",
    "            if(len(l) != 0):\n",
    "                # print(l[0].get_text().strip())\n",
    "                pmcid = l[0].get_text().strip()\n",
    "            else:\n",
    "                doi = NaN\n",
    "            title = soup.find_all(\"h1\", {\"class\": \"heading-title\"})[0].get_text().strip()\n",
    "            abstract = soup.find_all(\"div\", {\"class\": \"abstract-content selected\"})[0].get_text().strip()\n",
    "            keywords = soup.find_all(\"p\", {\"class\": \"sub-title\"})[0].get_text().strip()\n",
    "\n",
    "            # extract title, abstract, keywords, introduction from the returned html file\n",
    "            # count keywords from title + abstract + keywords\n",
    "            # and make sure the process the text so that only punctuation marks and numbers are removed\n",
    "            text = title + \" \" + title + \" \" + \"abstract\" + \" \" + \"keywords\"\n",
    "            text = text.strip()\n",
    "            text = re.sub(' +', ' ', text)\n",
    "            text = re.sub(r\"[^a-zA-Z' ']\", \"\", text).lower()\n",
    "    \n",
    "            # record the information into json\n",
    "            info_json = {}\n",
    "            info_json['DOI'] = doi\n",
    "            info_json['PMID'] = pmid\n",
    "            info_json['PMCID'] = pmcid\n",
    "            info_json['title'] = title\n",
    "            info_json['abstract'] = abstract\n",
    "            info_json['keywords'] = keywords\n",
    "            info_count = count_freq_from_liter(text, on_topic_kws, type = \"count\")\n",
    "            info_json = {**info_json, **info_count}\n",
    "            weight = calcul_related(info_count, related_kws_weights)\n",
    "            info_json[\"weight\"] = weight\n",
    "            print(info_json)\n",
    "            columns = ['DOI', \"PMID\", \"PMCID\", 'title', \"abstract\", \"keywords\", \"weight\"] + on_topic_kws\n",
    "            plib.add_row_to_csv(fpath.auto_filtered, info_json, columns)\n",
    "# end of scan_record_download\n",
    "# --------------------start of test code--------------------\n",
    "# scan_record_download(path_urls, on_topic_kws, pdf_folder_path)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e08f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_and_rank(weights_dict):\n",
    "    # weight formula\n",
    "    print(\"Enjoy reading!\")\n",
    "# end of weight_and_rank\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e562971",
   "metadata": {},
   "source": [
    "<h3> Main program: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c191fa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to iterate every row, fill in the empty cells and search for the text information we need \n",
    "# according to existing information\n",
    "# what we need is: DOI, PMID, PMCID, Title, Authors, Abstract, Keywords, full_text_url, pdf_url\n",
    "\n",
    "# step 1: fill in empty cells of the three identifiers: DOI, PMID, PMCID, and make sure the title is in lower case\n",
    "fill_in_identifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1247c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: remove duplications according to identifiers\n",
    "remove_dupli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a01765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: search for text information of the literature\n",
    "# download the .pdf file when available and \n",
    "# record the keywords matching results\n",
    "scan_download_record(on_topic_kws, fpath.litera_pdf_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9493b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: assign weight to each literature and rank them\n",
    "weight_and_rank(related_kws_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b8d7e0",
   "metadata": {},
   "source": [
    "<h3> Some test code, please ignore: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24db482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# if \"//doi.org/\" in \"https://doi.org/10.1016/0165-0173(96)00003-3\":\n",
    "#     print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77cc0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test redirect when access the doi link\n",
    "# from elsapy.elsdoc import FullDoc, AbsDoc\n",
    "# from elsapy.elsclient import ElsClient\n",
    "# import json\n",
    "# headers = {\n",
    "#     \"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9\", \n",
    "#     \"X-ELS-APIKEY\": \"310946e6e005957982c2c9cad6833ad3\",\n",
    "#     \"Accept\": \"application/pdf\",\n",
    "#     \"X-ELS-Insttoken\": \"instToken\",\n",
    "#     \"view\": \"FULL\"\n",
    "# } \n",
    "# # url = \"https://www.jneurosci.org/content/28/43/11042.short\"\n",
    "#  #url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2613515/\"\n",
    "\n",
    "# # Journal of Neurophysiology\n",
    "# # url = \"https://doi.org/10.1152/jn.2001.85.1.219\"\n",
    "# # url = \"https://journals.physiology.org/doi/10.1152/jn.2001.85.1.219\"\n",
    "\n",
    "# # science direct\n",
    "# # url = \"https://doi.org/10.1016/j.biopsych.2004.10.014\"\n",
    "# # url = \"https://linkinghub.elsevier.com/retrieve/pii/S0006322304010947\"\n",
    "# # url = \"https://www.sciencedirect.com/science/article/pii/S0006322304010947?via%3Dihub\"\n",
    "# url = \"https://api.elsevier.com/content/article/doi/{10.1016/j.biopsych.2004.10.014}\"\n",
    "\n",
    "# # response = requests.get(url, headers = headers)\n",
    "# # soup = BeautifulSoup(response.content,\"lxml\")\n",
    "# # print(soup)\n",
    "# # print(response.history)\n",
    "# # print(response.url)\n",
    "# # # Load configuration\n",
    "# # con_file = open(\"config.json\")\n",
    "# # config = json.load(con_file)\n",
    "# # con_file.close()\n",
    "\n",
    "# # response = requests.get(url, headers = headers)\n",
    "# # print(response)\n",
    "\n",
    "# # ## Initialize client\n",
    "# # client = ElsClient(config[\"apikey\"])\n",
    "\n",
    "# # ## ScienceDirect (full-text) document example using DOI\n",
    "# # doi_doc = FullDoc(doi = \"10.1016/j.biopsych.2004.10.014\")\n",
    "# # print(doi_doc)\n",
    "# # if doi_doc.read(client):\n",
    "# #     print (\"doi_doc.title: \", doi_doc.title)\n",
    "# #     doi_doc.write(\"doi_doc\")   \n",
    "# # else:\n",
    "# #     print (\"Read document failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c1c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find DOI\n",
    "# # this link does not have \"DOI\" in href form but text from\n",
    "# url = \"https://www.jneurosci.org/content/28/43/11042.short\"\n",
    "# # url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2613515/\"\n",
    "# response = requests.get(url, headers = plib.headers)\n",
    "# soup = BeautifulSoup(response.content,\"lxml\")\n",
    "# # print(soup)\n",
    "# doi_list = []\n",
    "# num_results_str = soup.select(\"a\", href = True)\n",
    "# # print(num_results_str)\n",
    "# for item in num_results_str:\n",
    "#     if \"//doi.org/\" in item[\"href\"]:\n",
    "#         print(item[\"href\"])\n",
    "#         doi_list.append(item[\"href\"].split(\"//doi.org/\")[1])\n",
    "\n",
    "# print(doi_list)\n",
    "        \n",
    "# if len(doi_list) == 0:\n",
    "#     print(\"Ops! Did't find DOI on this page!\")\n",
    "\n",
    "\n",
    "\n",
    "# test extract doi from url\n",
    "# with open(fpath.gs_poten_urls, \"r\") as file:\n",
    "#     lines = []\n",
    "#     for line in file:\n",
    "#         print(line)\n",
    "#         line = line.strip()\n",
    "#         lines.append(line)\n",
    "# print(len(lines))\n",
    "# doi_list = []\n",
    "# for url in lines:\n",
    "#     response = requests.get(url, headers = plib.headers)\n",
    "#     while\n",
    "#     soup = BeautifulSoup(response.content,\"lxml\")\n",
    "#     # print(soup)\n",
    "#     num_results_str = soup.select(\"a\", href = True)\n",
    "#     print(num_results_str)\n",
    "#     for href in num_results_str:\n",
    "#         if \"//doi.org/\" in href[\"href\"]:\n",
    "#             doi_list.append(href[\"href\"])\n",
    "#             print(href[\"href\"])\n",
    "#         else:\n",
    "#             print(\"Ops! Did't find DOI on this page!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5131b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # extract PDF link if exists\n",
    "#     print(doi)\n",
    "#     response_pdf = requests.get(doi, headers = headers)\n",
    "#     print(response_pdf.url)\n",
    "#     pdf_page_link = response_pdf.url\n",
    "        \n",
    "#     pdf_page = soup.find_all(\"a\", {'class':'link-item dialog-focus'}, href = True)[0]['href']\n",
    "    \n",
    "#     # print(pdf_page_link)\n",
    "#     pdf_page = requests.get(pdf_page_link, headers = headers)\n",
    "#     soup_pdf = BeautifulSoup(pdf_page.content,'lxml')\n",
    "#     print(len(soup_pdf.find_all(\"a\", href = True)))\n",
    "#     pdf_link = soup_pdf.find_all(\"a\", href = True)[0]['href']\n",
    "    \n",
    "    \n",
    "#     print(pdf_link)\n",
    "#     pdf_link = 'https://www.ncbi.nlm.nih.gov' + pdf_link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc9ccf",
   "metadata": {},
   "source": [
    "<h3> Next step: semi-automated information search </h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
