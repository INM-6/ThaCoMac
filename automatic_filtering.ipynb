{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea1cce5-4f07-4bd7-8ca3-5f6fa51254d0",
   "metadata": {},
   "source": [
    "<h2> Automatic filtering </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5a61c5-f8c6-418a-b450-cdea0378ddab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "from nltk import ngrams\n",
    "import PyPDF2\n",
    "import json\n",
    "import time\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726b12c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib\n",
    "import extract_info\n",
    "import parameters as params\n",
    "import download_and_process_pdf as dpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489f0451",
   "metadata": {},
   "source": [
    "<h3> Predefined fucntions: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f93d7d-bbbc-4afe-94f7-f12cca267a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_filling(input_path, output_path, start, end):\n",
    "    # scan each row in the potential related literature and extract information\n",
    "    df = pd.read_csv(input_path, header=None, sep=\",\")\n",
    "    df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # info = {\n",
    "        #     \"doi\": np.nan,\n",
    "        #     \"pmid\": np.nan,\n",
    "        #     \"pmcid\": np.nan,\n",
    "        #     \"title\": np.nan,\n",
    "        #     \"abstract\": np.nan,\n",
    "        #     \"keywords\": np.nan,\n",
    "        #     \"pdf_link\": np.nan\n",
    "        # }\n",
    "\n",
    "        # initialzie\n",
    "        index = df.at[ind, \"INDEX\"]\n",
    "        doi = df.at[ind, \"DOI\"]\n",
    "        pmid = df.at[ind, \"PMID\"]\n",
    "        pmcid = df.at[ind, \"PMCID\"]\n",
    "        full_text_url = df.at[ind, \"FULL_TEXT_URL\"]\n",
    "        full_text_source = df.at[ind, \"FULL_TEXT_SOURCE\"]\n",
    "        pdf_url = np.nan\n",
    "        pdf_source = np.nan\n",
    "        title = df.at[ind, \"TITLE\"]\n",
    "        abstract = df.at[ind, \"ABSTRACT\"]\n",
    "        keywords = df.at[ind, \"KEYWORDS\"]\n",
    "\n",
    "        if full_text_url != full_text_url: # full text url not found\n",
    "            if df.at[ind, \"PDF_URL\"] == df.at[ind, \"PDF_URL\"]:\n",
    "                url = str(df.at[ind, \"PDF_URL\"]).strip()\n",
    "                url1, status_code = plib.get_final_redirected_url(url)\n",
    "                if status_code == 200:\n",
    "                    pdf_url = url\n",
    "                    pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                elif status_code == 403:\n",
    "                    # print(status_code, \"when getting final redirected url: \", url)\n",
    "                    pdf_url = url\n",
    "                    pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                else:\n",
    "                    print(status_code, \"when getting final redirected url: \", url)\n",
    "                    pdf_url = np.nan\n",
    "                    pdf_source = np.nan  \n",
    "            else:\n",
    "                print(\"full text url and pdf url are not available!\")\n",
    "                pdf_url = np.nan\n",
    "                pdf_source = np.nan\n",
    "        elif ('.pdf' in full_text_url and full_text_url.split('.pdf')[1] == '') or ('.PDF' in full_text_url and full_text_url.split('.PDF')[1] == ''):\n",
    "            print(\"full text url is a pdf file: \", full_text_url)\n",
    "            pdf_url = full_text_url\n",
    "            pdf_source = full_text_source\n",
    "            full_text_url = np.nan\n",
    "            full_text_source = np.nan\n",
    "        else: # full text url found\n",
    "            flag = False\n",
    "            for website in params.websites:\n",
    "                if website in full_text_source:\n",
    "                    flag = True\n",
    "                    break\n",
    "            if not flag:\n",
    "                continue\n",
    "\n",
    "            url = str(full_text_url).strip()\n",
    "            try:\n",
    "                info = extract_info.extract_info_from_webpage(url, params.websites)\n",
    "            except:\n",
    "                raise Exception(\"Error! Cannot extract information from the webpage: \", url)\n",
    "            \n",
    "            # doi\n",
    "            if info['doi'] == info['doi'] and doi == doi and info['doi'] != doi:\n",
    "                print(doi)\n",
    "                print(info['doi'])\n",
    "\n",
    "            if info['doi'] == info['doi']:\n",
    "                doi = info['doi'].lower()\n",
    "            else:\n",
    "                doi = doi\n",
    "            \n",
    "            # pmid\n",
    "            if info['pmid'] == info['pmid'] and df.at[ind, \"PMID\"] == df.at[ind, \"PMID\"] and str(int(info['pmid'])) != str(int(df.at[ind, \"PMID\"])):\n",
    "                print(str(int(df.at[ind, \"PMID\"]))) \n",
    "                print(str(int(info['pmid'])))      \n",
    "\n",
    "            if info['pmid'] == info['pmid']:\n",
    "                pmid = str(int(info['pmid']))\n",
    "            elif pmid == pmid:\n",
    "                pmid = str(int(pmid)).strip()\n",
    "            else:\n",
    "                pmid = np.nan\n",
    "            \n",
    "            # pmcid\n",
    "            if info['pmcid'] == info['pmcid'] and pmcid == pmcid and info['pmcid'] != pmcid:\n",
    "                print(pmcid)\n",
    "                print(info['pmcid'])\n",
    "\n",
    "            if info['pmcid'] == info['pmcid']:\n",
    "                pmcid = info['pmcid']\n",
    "            else:\n",
    "                pmcid = df.at[ind, \"PMCID\"]\n",
    "            \n",
    "            # full_text_url, full_text_surce\n",
    "            if full_text_url == full_text_url:\n",
    "                full_text_source = full_text_url.split(\"://\")[1].split(\"/\")[0]\n",
    "            else:\n",
    "                print(\"full text url is not available\")\n",
    "                full_text_url = np.nan\n",
    "                full_text_source = np.nan\n",
    "\n",
    "            if pdf_url != pdf_url and info['pdf_link'] == info['pdf_link']:\n",
    "                pdf_url = str(info['pdf_link']).strip()\n",
    "                pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                # try:\n",
    "                #     pdf_url, status_code = plib.get_final_redirected_url(url)\n",
    "                #     if status_code == 200:\n",
    "                #         pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                #     elif status_code == 403:\n",
    "                #         # print(status_code, \"when getting final redirected url: \", url)\n",
    "                #         pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                #     else:\n",
    "                #         print(status_code, \"when getting final redirected url: \", url)\n",
    "                #         pdf_url = np.nan\n",
    "                #         pdf_source = np.nan\n",
    "                # except:\n",
    "                #     pdf_url = np.nan\n",
    "                #     pdf_source = np.nan \n",
    "                    \n",
    "            if pdf_url != pdf_url and df.at[ind, \"PDF_URL\"] == df.at[ind, \"PDF_URL\"]:\n",
    "                print(\"PDF_URL not extracted from info: , but existed already: \", df.at[ind, \"PDF_URL\"])\n",
    "                pdf_url = df.at[ind, \"PDF_URL\"]\n",
    "                pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "            \n",
    "            if pdf_url != pdf_url:\n",
    "                if full_text_url == full_text_url and full_text_url.split(\"://\")[1].split(\"/\")[0] == 'www.ncbi.nlm.nih.gov':\n",
    "                    if doi == doi:\n",
    "                        url = \"https://doi.org/\" + doi\n",
    "                        url, status_code = plib.get_final_redirected_url(url)\n",
    "                        info = extract_info.extract_info_from_webpage(url, params.websites)\n",
    "                        full_text_url = url\n",
    "                        full_text_source = url.split(\"://\")[1].split(\"/\")[0]\n",
    "                        pdf_url = info[\"pdf_link\"]\n",
    "                        pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                    else:\n",
    "                        pdf_url = np.nan\n",
    "                        pdf_source = np.nan\n",
    "                else:  \n",
    "                    print(\"PDF_URL not found for: \", doi, pmid, pmcid, full_text_url)\n",
    "                    pdf_url = np.nan\n",
    "                    pdf_source = np.nan\n",
    "                \n",
    "            # title\n",
    "            if info['title'] == info['title']:\n",
    "                title = info['title']\n",
    "                title = title.replace(\";\", \",\")\n",
    "            else:\n",
    "                title = title\n",
    "            \n",
    "            # abstract\n",
    "            if info['abstract'] == info['abstract']:\n",
    "                abstract = info['abstract']\n",
    "                abstract = ''.join(e for e in abstract if (e.isalpha() or e == \" \" or e == \"-\"))\n",
    "            else:\n",
    "                abstract = abstract\n",
    "            \n",
    "            # keywords\n",
    "            if info['keywords'] == info['keywords']:\n",
    "                keywords = info['keywords']\n",
    "                keywords = keywords.replace(\";\", \",\")\n",
    "                keywords = ''.join(e for e in keywords if (e.isalpha() or e == \" \" or e == \"-\" or e == \",\"))\n",
    "            else:\n",
    "                keywords = keywords\n",
    "        \n",
    "        columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "        \n",
    "        row = {\n",
    "            \"INDEX\": [index],\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"FULL_TEXT_URL\": [full_text_url],\n",
    "            \"FULL_TEXT_SOURCE\": [full_text_source],\n",
    "            \"PDF_URL\": [pdf_url],\n",
    "            \"PDF_SOURCE\": [pdf_source],\n",
    "            \"TITLE\": [title],\n",
    "            \"ABSTRACT\": [abstract],\n",
    "            \"KEYWORDS\": [keywords]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# input_path = fpath.poten_litera_ids_ftl_filled\n",
    "# output_path = fpath.poten_litera_litera_db\n",
    "\n",
    "# # clear file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# info_filling(input_path, output_path)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d8a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(input_path, pdf_folder, start, end):\n",
    "    df = pd.read_csv(input_path, header=None, sep=',')\n",
    "    df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # time.sleep(3)\n",
    "        \n",
    "        # the flag to indicate whether the pdf is downloaded successfully, if not, save the row to poten_litera_pdf_not_available.csv\n",
    "        flag = False\n",
    "\n",
    "        pdf_url = df.at[ind, \"PDF_URL\"]\n",
    "\n",
    "        # pdf_url not found\n",
    "        if pdf_url != pdf_url:\n",
    "            flag = False\n",
    "        # pdf_url found\n",
    "        else:\n",
    "            doi = df.at[ind, \"DOI\"]\n",
    "            if dpp.download_and_rename_pdf(pdf_url, doi, df.at[ind, \"INDEX\"], pdf_folder):\n",
    "                flag = True\n",
    "            else:\n",
    "                print(df.at[ind, \"FULL_TEXT_URL\"])\n",
    "                flag = False\n",
    "\n",
    "        # # pdf_url not found\n",
    "        # if pdf_url != \"://linkinghub.elsevier.com/\":\n",
    "        #     continue\n",
    "        # # pdf_url found\n",
    "        # else:\n",
    "        #     doi = df.at[ind, \"DOI\"]\n",
    "        #     if dpp.download_and_rename_pdf(pdf_url, doi, df.at[ind, \"INDEX\"], pdf_folder):\n",
    "        #         flag = True\n",
    "        #     else:\n",
    "        #         print(df.at[ind, \"FULL_TEXT_URL\"])\n",
    "        #         flag = False\n",
    "        \n",
    "        if not flag:\n",
    "            print(\"PDF_URL not found or PDF not successfully downloaded for: \")\n",
    "            print(\"\\n\")\n",
    "            print(df.at[ind, \"INDEX\"], df.at[ind, \"DOI\"], df.at[ind, \"PMID\"], df.at[ind, \"PMCID\"], df.at[ind, \"FULL_TEXT_URL\"], df.at[ind, \"FULL_TEXT_SOURCE\"], df.at[ind, \"PDF_URL\"], df.at[ind, \"PDF_SOURCE\"], df.at[ind, \"TITLE\"], df.at[ind, \"ABSTRACT\"], df.at[ind, \"KEYWORDS\"])\n",
    "            # print(\"\\n\")\n",
    "\n",
    "        line_number_in_csv = ind + 1\n",
    "        print(ind, \" Line number:\", line_number_in_csv, \" INDEX:\", int(df.at[ind, \"INDEX\"]))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef86ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf2text(pdf_path, text_path, start_page, text_length_to_extract): \n",
    "    try:   \n",
    "        # creating a pdf reader object\n",
    "        reader = PyPDF2.PdfReader(pdf_path)\n",
    "        \n",
    "        # printing number of pages in pdf file\n",
    "        page_max = len(reader.pages)\n",
    "        # print(page_max)\n",
    "        \n",
    "        # getting a specific page from the pdf file\n",
    "        text = \"\"\n",
    "        \n",
    "        for i in range(start_page, page_max):\n",
    "            page = reader.pages[i]\n",
    "            text = text + \" \".join(page.extract_text().splitlines())\n",
    "            text = text.lower()\n",
    "            text = re.sub(r'[^a-z\\s\\-]', '', text)\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "            if len(text.split()) > text_length_to_extract:\n",
    "                break\n",
    "\n",
    "        with open(text_path, \"w\") as f:\n",
    "            f.write(text)\n",
    "        f.close()\n",
    "    except:\n",
    "        print(\"ERROR! ERROR!\")\n",
    "        print(\"ERROR when converting pdf to text for: \", pdf_path)\n",
    "# --------------------start of test code--------------------\n",
    "# index = 1\n",
    "# pdf_folder = fpath.pdf_folder\n",
    "# text_folder = fpath.text_folder\n",
    "# start_page = 0\n",
    "# end_page = 5\n",
    "# text_length_to_extract = 1000\n",
    "\n",
    "# pdf_file_name = str(index) + \".pdf\"\n",
    "# pdf_path = os.path.join(pdf_folder, pdf_file_name)\n",
    "# text_path = os.path.join(text_folder, pdf_file_name.split(\".pdf\")[0] + \".txt\")\n",
    "# # print(pdf_path)\n",
    "# # print(text_path)\n",
    "# pdf2text(pdf_path, text_path, start_page, text_length_to_extract)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bead6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json2text(json_path, text_path, text_length_to_extract):    \n",
    "    with open(json_path, \"r\") as f:\n",
    "        json_file = json.load(f)\n",
    "        text = json_file[\"full-text-retrieval-response\"][\"originalText\"]\n",
    "    f.close()\n",
    "\n",
    "    title = json_file[\"full-text-retrieval-response\"][\"coredata\"][\"dc:title\"]\n",
    "\n",
    "    text = title + \" \" + text.split(title)[1]\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s\\-]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    words = text.split()\n",
    "    text = ' '.join(words[:text_length_to_extract])\n",
    "\n",
    "    with open(text_path, \"w\") as f:\n",
    "        f.write(text)\n",
    "    f.close()\n",
    "# --------------------start of test code--------------------\n",
    "# index = 1\n",
    "# pdf_folder = fpath.pdf_folder\n",
    "# text_folder = fpath.text_folder\n",
    "# start_page = 0\n",
    "# text_length_to_extract = 1000\n",
    "\n",
    "# json_file_name = str(index) + \".json\"\n",
    "# json_path = os.path.join(pdf_folder, json_file_name)\n",
    "# text_path = os.path.join(text_folder, str(index) + \".txt\")\n",
    "# # print(pdf_path)\n",
    "# # print(text_path)\n",
    "# json2text(json_path, text_path, text_length_to_extract)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce32f7b8-96b7-4f5c-a2da-3ff931cdeaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_keyword(text, keyword):\n",
    "    # remove non-alphabetic characters but keep spaces and \"-\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s\\-]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # print(text)\n",
    "    \n",
    "    # words = []\n",
    "    # n = keyword_length\n",
    "    # n_grams = ngrams(text.split(), n)\n",
    "    # for gram in n_grams:\n",
    "    #     word = gram[0]\n",
    "    #     if n > 0:\n",
    "    #         for i in range(1, n):\n",
    "    #             word = word + \" \" + gram[i]\n",
    "    #     words.append(word)\n",
    "    # # print(words)\n",
    "    \n",
    "    # word_count = 0\n",
    "    # for word in words:\n",
    "    #     # print(word)\n",
    "    #     if word == keyword:\n",
    "    #         word_count += 1\n",
    "    word_count = text.count(keyword)\n",
    "    print(word_count)\n",
    "    return word_count\n",
    "# --------------------start of test code--------------------\n",
    "# text = 'This apple 6i7s very_tasty？、  2but th&e banana this is not delicious at Is all.6'\n",
    "# # text = \"Distribution of the dopamine innervation in the macaque and human thalamus Miguel Ángel García-Cabezas,aBeatriz Rico,a,b Miguel Ángel Sánchez-González,aand Carmen Cavadaa,⁎ aDepartamento de Anatomía, Histología y Neurociencia, Facultad de Medicina, Universidad Autónoma de Madrid, C/Arzobispo Morcillo s/n, 28029 Madrid, Spain bInstituto de Neurociencias de Alicante, Universidad Miguel Hernández-CSIC, 03550 Sant Joan d ’Alacant, Spain Received 19 April 2006; revised 8 June 2006; accepted 11 July 2006 Available online 30 November 2006 We recently defined the thalamic dopaminergic system in primates; it arises from numerous dopaminergic cell groups and selectively targetsnumerous thalamic nuclei. Given the central position of the thalamus in subcortical and cortical interplay, and the functional relevance of dopamine neuromodulation in the brain, detailing dopamine dis-tribution in the thalamus should supply important information. Tothis end we performed immunohistochemistry for dopamine and the dopamine transporter in the thalamus of macaque monkeys and humans to generate maps, in the stereotaxic coronal plane, of thedistribution of dopaminergic axons. The dopamine innervation of the thalamus follows the same pattern in both species and is most dense in midline limbic nuclei, the mediodorsal and lateral posteriorassociation nuclei, and in the ventral lateral and ventral anteriormotor nuclei. This distribution suggests that thalamic dopamine has a prominent role in emotion, attention, cognition and complex somatosensory and visual processing, as well as in motor control.Most thalamic dopaminergic axons are thin and varicose and targetboth the neuropil and small blood vessels, suggesting that, besides neuronal modulation, thalamic dopamine may have a direct influence on microcirculation. The maps provided here should be a usefulreference in future experimental and neuroimaging studies aiming atclarifying the role of the thalamic dopaminergic system in health and in conditions involving brain dopamine, including Parkinson ’s disease, drug addiction and schizophrenia.© 2006 Elsevier Inc. All rights reserved. Keywords: Dopamine; Thalamus; Monkey; Human; Primate; Dopamine transporter; Parkinson; Schizophrenia; AddictionIntroduction The thalamus is made up of multiple nuclei relaying information from subcortical centers or from other cortices to the cerebral cortex (Sherman and Guillery, 2005 ), as well as the striatum, the nucleus accumbens and the amygdala ( Steriade et al., 1997 ). In addition to specific subcortical and cortical afferents, the primate thalamus receives axons containing the neuromodulators acetylcholine (Heckers et al., 1992 ), histamine ( Manning et al., 1996 ), serotonin (Morrison and Foote, 1986; Lavoie and Parent, 1991 ), and the catecholamines adrenaline ( Rico and Cavada, 1998a ), noradrenaline (Morrison and Foote, 1986; Ginsberg et al., 1993 ) and dopamine (Sánchez-González et al., 2005 ). Until recently, the existence of significant dopamine innervation in the primate thalamus has been largely ignored, probably becausedopamine innervation of the rodent thalamus is very scant(Groenewegen, 1988; Papadopoulos and Parnavelas, 1990 ). However, fragmentary data scattered through the literature endorse the presence of dopamine innervation in the primate thalamus.Postmortem biochemical studies showed the presence of dopamine in the thalamus of macaques ( Brown et al., 1979; Goldman-Rakic and Brown, 1981; Pifl et al., 1990, 1991 ) and human subjects ( Oke and Adams, 1987 ). Later, receptor binding and in situ hybridization analyses detected the presence of dopamine D2-like ( Joyce et al., 1991; Kessler et al., 1993; Hall et al., 1996; Langer et al., 1999;Rieck et al., 2004 ) and D3-like receptors ( Gurevich and Joyce, 1999 ) in several human thalamic nuclei. Positron emission tomography (PET) radioligand studies have also demonstratedthe presence of the dopamine transporter (DAT) ( Wang et al., 1995; Halldin et al., 1996; Helfenbein et al., 1999; Brownell et al., 2003 ) and of D2-like receptors ( Farde et al., 1997; Langer et al., 1999; Okubo et al., 1999; Brownell et al., 2003; Rieck et al., 2004 ) in the human and macaque thalamus. In the course of PET studies focusing on schizophrenia, D2- and D3-like radioligand binding was also found in the thalamus of control subjects ( Talvik et al., 2003; Yasuno et al., 2004 ). Finally, an immunohistochemical study using anti-DAT antibodies detected the presence of dopaminergic www.elsevier.com/locate/ynimg NeuroImage 34 (2007) 965 –984 ⁎Corresponding author. Fax: +34 91 497 53 15. E-mail address: carmen.cavada@uam.es (C. Cavada). Available online on ScienceDirect (www.sciencedirect.com). 1053-8119/$ - see front matter © 2006 Elsevier Inc. All rights reserved. doi:10.1016/j.neuroimage.2006.07.032\"\n",
    "# keyword = 'but the'\n",
    "# count = count_keyword(text, keyword)\n",
    "# print(count)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b488729c-2e38-43e8-bba2-d81126f38847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_on_topic_kws_from_liter(text_tak, text_txt, on_topic_kws, type):\n",
    "    # text_tak = ''.join(e for e in text_tak if (e.isalpha() or e == \" \" or e == \"-\"))\n",
    "    # text_tak = text_tak.strip().lower()\n",
    "    # text_tak = ' '.join(text_tak.split())\n",
    "    if text_tak == text_tak:\n",
    "        text = text_tak + \" \" + text_txt\n",
    "    else:\n",
    "        text = text_txt\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s\\-]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # print(text_tak)\n",
    "\n",
    "    # text_txt = ''.join(e for e in text_txt if (e.isalpha() or e == \" \" or e == \"-\"))\n",
    "    # text_txt = text_txt.strip().lower()\n",
    "    # text_txt = ' '.join(text_txt.split())\n",
    "    # # print(text_txt)\n",
    "\n",
    "    # text_text = text_tak + \" \" + text_txt\n",
    "\n",
    "    # length_tak = len(text_tak.split())\n",
    "    # length_txt = len(text_txt.split())\n",
    "    # length_text = length_tak + length_txt\n",
    "    length_text = len(text.split())\n",
    "\n",
    "    # keywords_count = dict()\n",
    "    # keywords_freq = dict()\n",
    "\n",
    "    word_count = 0\n",
    "    # count the on-topic keywords\n",
    "    for word in on_topic_kws:\n",
    "        # print(on_topic_kws[i])\n",
    "        word_count = word_count + count_keyword(text, word)\n",
    "        # print(word_count)\n",
    "        # if type == \"freq\":\n",
    "        #     # keywords_freq[on_topic_kws[i]] = round(word_count * 10 / length_text, 2)\n",
    "        #     fre = round(word_count * 10 / length_text, 2)\n",
    "        # elif type == \"count\":\n",
    "        #     count = word_count\n",
    "        # else:\n",
    "        #     print(\"Error! Wrong type!\")\n",
    "    \n",
    "    if text_tak == text_tak:\n",
    "        word_count = round(word_count / 2) \n",
    "    # fre = round(word_count * 10 / length_text, 2)\n",
    "    # print(keywords_count)\n",
    "    return word_count\n",
    "# --------------------start of test code--------------------\n",
    "# text_tak = 'Vision for action: thalamic and cortical inputs to the macaque superior tract neural tracing, parietal lobule The dorsal visual stream, the cortical circuit that in the primate brain is mainly dedicated to the visual control of actions, is split into two routes, a lateral and a medial one, both involved in coding different aspects of sensorimotor control of actions. The lateral route, named \"lateral grasping network\", is mainly involved in the control of the distal part of prehension, namely grasping and manipulation. The medial route, named \"reach-to-grasp network\", is involved in the control of the full deployment of prehension act, from the direction of arm movement to the shaping of the hand according to the object to be grasped. In macaque monkeys, the reach-to-grasp network (the target of this review) includes areas of the superior parietal lobule (SPL) that hosts visual and somatosensory neurons well suited to control goal-directed limb movements toward stationary as well as moving objects. After a brief summary of the neuronal functional properties of these areas, we will analyze their cortical and thalamic inputs thanks to retrograde neuronal tracers separately injected into the SPL areas V6, V6A, PEc, and PE. These areas receive visual and somatosensory information distributed in a caudorostral, visuosomatic trend, and some of them are directly connected with the dorsal premotor cortex. This review is particularly focused on the origin and type of visual information reaching the SPL, and on the functional role this information can play in guiding limb interaction with objects in structured and dynamic environments. Area PEc; Area V6; Area V6A; Dorsal visual stream; Goal-directed arm movement; Sensorimotor integration.'\n",
    "# text_txt = \"Distribution of the dopamine innervation in the macaque and human thalamus Miguel Ángel García-Cabezas,aBeatriz Rico,a,b Miguel Ángel Sánchez-González,aand Carmen Cavadaa,⁎ aDepartamento de Anatomía, Histología y Neurociencia, Facultad de Medicina, Universidad Autónoma de Madrid, C/Arzobispo Morcillo s/n, 28029 Madrid, Spain bInstituto de Neurociencias de Alicante, Universidad Miguel Hernández-CSIC, 03550 Sant Joan d ’Alacant, Spain Received 19 April 2006; revised 8 June 2006; accepted 11 July 2006 Available online 30 November 2006 We recently defined the thalamic dopaminergic system in primates; it arises from numerous dopaminergic cell groups and selectively targetsnumerous thalamic nuclei. Given the central position of the thalamus in subcortical and cortical interplay, and the functional relevance of dopamine neuromodulation in the brain, detailing dopamine dis-tribution in the thalamus should supply important information. Tothis end we performed immunohistochemistry for dopamine and the dopamine transporter in the thalamus of macaque monkeys and humans to generate maps, in the stereotaxic coronal plane, of thedistribution of dopaminergic axons. The dopamine innervation of the thalamus follows the same pattern in both species and is most dense in midline limbic nuclei, the mediodorsal and lateral posteriorassociation nuclei, and in the ventral lateral and ventral anteriormotor nuclei. This distribution suggests that thalamic dopamine has a prominent role in emotion, attention, cognition and complex somatosensory and visual processing, as well as in motor control.Most thalamic dopaminergic axons are thin and varicose and targetboth the neuropil and small blood vessels, suggesting that, besides neuronal modulation, thalamic dopamine may have a direct influence on microcirculation. The maps provided here should be a usefulreference in future experimental and neuroimaging studies aiming atclarifying the role of the thalamic dopaminergic system in health and in conditions involving brain dopamine, including Parkinson ’s disease, drug addiction and schizophrenia.© 2006 Elsevier Inc. All rights reserved. Keywords: Dopamine; Thalamus; Monkey; Human; Primate; Dopamine transporter; Parkinson; Schizophrenia; AddictionIntroduction The thalamus is made up of multiple nuclei relaying information from subcortical centers or from other cortices to the cerebral cortex (Sherman and Guillery, 2005 ), as well as the striatum, the nucleus accumbens and the amygdala ( Steriade et al., 1997 ). In addition to specific subcortical and cortical afferents, the primate thalamus receives axons containing the neuromodulators acetylcholine (Heckers et al., 1992 ), histamine ( Manning et al., 1996 ), serotonin (Morrison and Foote, 1986; Lavoie and Parent, 1991 ), and the catecholamines adrenaline ( Rico and Cavada, 1998a ), noradrenaline (Morrison and Foote, 1986; Ginsberg et al., 1993 ) and dopamine (Sánchez-González et al., 2005 ). Until recently, the existence of significant dopamine innervation in the primate thalamus has been largely ignored, probably becausedopamine innervation of the rodent thalamus is very scant(Groenewegen, 1988; Papadopoulos and Parnavelas, 1990 ). However, fragmentary data scattered through the literature endorse the presence of dopamine innervation in the primate thalamus.Postmortem biochemical studies showed the presence of dopamine in the thalamus of macaques ( Brown et al., 1979; Goldman-Rakic and Brown, 1981; Pifl et al., 1990, 1991 ) and human subjects ( Oke and Adams, 1987 ). Later, receptor binding and in situ hybridization analyses detected the presence of dopamine D2-like ( Joyce et al., 1991; Kessler et al., 1993; Hall et al., 1996; Langer et al., 1999;Rieck et al., 2004 ) and D3-like receptors ( Gurevich and Joyce, 1999 ) in several human thalamic nuclei. Positron emission tomography (PET) radioligand studies have also demonstratedthe presence of the dopamine transporter (DAT) ( Wang et al., 1995; Halldin et al., 1996; Helfenbein et al., 1999; Brownell et al., 2003 ) and of D2-like receptors ( Farde et al., 1997; Langer et al., 1999; Okubo et al., 1999; Brownell et al., 2003; Rieck et al., 2004 ) in the human and macaque thalamus. In the course of PET studies focusing on schizophrenia, D2- and D3-like radioligand binding was also found in the thalamus of control subjects ( Talvik et al., 2003; Yasuno et al., 2004 ). Finally, an immunohistochemical study using anti-DAT antibodies detected the presence of dopaminergic www.elsevier.com/locate/ynimg NeuroImage 34 (2007) 965 –984 ⁎Corresponding author. Fax: +34 91 497 53 15. E-mail address: carmen.cavada@uam.es (C. Cavada). Available online on ScienceDirect (www.sciencedirect.com). 1053-8119/$ - see front matter © 2006 Elsevier Inc. All rights reserved. doi:10.1016/j.neuroimage.2006.07.032\"\n",
    "# on_topic_kws = params.ranking_params\n",
    "# keywords_count = count_on_topic_kws_from_liter(text_tak, text_txt, on_topic_kws, 'count')\n",
    "# print(keywords_count)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02fb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul_index(keywords_count, on_topic_kws_weights):\n",
    "    index = 0\n",
    "    for key in keywords_count.keys():\n",
    "        index += keywords_count[key] * on_topic_kws_weights[key]\n",
    "    return index\n",
    "# --------------------start of test code--------------------\n",
    "# keywords_count_or_fre = {}\n",
    "# index = calcul_related(keywords_count_or_fre, params.on_topic_kws_weights)\n",
    "# print(index)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e08f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_and_count(input_path, output_path, ranking_params, ranking_params_weights):\n",
    "    df = pd.read_csv(input_path, header=None, sep=\",\")\n",
    "    df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "    for ind in df.index:\n",
    "        index = str(int(df.at[ind, \"INDEX\"]))\n",
    "        txt_file_name = str(index) + \".txt\"\n",
    "        txt_path = os.path.join(fpath.text_folder, txt_file_name)\n",
    "\n",
    "        text_tak = \"\"\n",
    "        text_txt = \"\"\n",
    "\n",
    "        if df.at[ind, \"ABSTRACT\"] == df.at[ind, \"ABSTRACT\"]:\n",
    "            text_tak = text_tak + \" \" + df.at[ind, \"ABSTRACT\"]\n",
    "            if df.at[ind, \"TITLE\"] == df.at[ind, \"TITLE\"]:\n",
    "                text_tak = text_tak + \" \" + df.at[ind, \"TITLE\"]\n",
    "            \n",
    "            if df.at[ind, \"KEYWORDS\"] == df.at[ind, \"KEYWORDS\"]:\n",
    "                text_tak = text_tak + \" \" + df.at[ind, \"KEYWORDS\"]\n",
    "        else:\n",
    "            text_tak = np.nan\n",
    "        # print(text_tak)\n",
    "\n",
    "        if os.path.exists(txt_path):\n",
    "            with open(txt_path, \"r\") as f:\n",
    "                text_txt = f.read()\n",
    "            f.close()\n",
    "        else:\n",
    "            pass\n",
    "            # print(\".txt file not found:\", ind, index, df.at[ind, \"PDF_URL\"])\n",
    "        \n",
    "        # on_topic_kws = [key for key in on_topic_kws_weights.keys()]\n",
    "        # # length_tak, length_txt, length_text, keywords_count = count_on_topic_kws_from_liter(text_tak, text_txt, on_topic_kws, \"count\")\n",
    "        # length_tak, length_txt, length_text, keywords_freq = count_on_topic_kws_from_liter(text_tak, text_txt, on_topic_kws, \"freq\")\n",
    "        for param in ranking_params:\n",
    "            if param == 'species':\n",
    "                species = count_on_topic_kws_from_liter(text_tak, text_txt, params.species_related, 'count')\n",
    "            elif param == 'tc_ct':\n",
    "                tc_ct = count_on_topic_kws_from_liter(text_tak, text_txt, params.tc_ct_related, 'count')\n",
    "            elif param == 'thalam':\n",
    "                thalam = count_on_topic_kws_from_liter(text_tak, text_txt, params.thalam_related, 'count')\n",
    "            elif param == 'cortex':\n",
    "                cortex = count_on_topic_kws_from_liter(text_tak, text_txt, params.cortex_related, 'count')\n",
    "            elif param == 'method':\n",
    "                method = count_on_topic_kws_from_liter(text_tak, text_txt, params.method_related, 'count')\n",
    "            elif param == 'connectivity':\n",
    "                connectivity = count_on_topic_kws_from_liter(text_tak, text_txt, params.connectivity_related, 'count')\n",
    "            else:\n",
    "                print(\"Error! Wrong param!\")\n",
    "\n",
    "\n",
    "        columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \n",
    "                   \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \n",
    "                   \"SPECIES_RELATED\", \"TC_CT_RELATED\", \"THALAM_RELATED\", \"CORTEX_RELATED\", \"METHOD_RELATED\", \"CONNECTIVITY_RELATED\"]\n",
    "        \n",
    "        # columns = columns + [key for key in keywords_count.keys()]\n",
    "\n",
    "        row = {\n",
    "            \"INDEX\": [df.at[ind, \"INDEX\"]],\n",
    "            \"DOI\": [df.at[ind, \"DOI\"]],\n",
    "            \"PMID\": [df.at[ind, \"PMID\"]],\n",
    "            \"PMCID\": [df.at[ind, \"PMCID\"]],\n",
    "            \"FULL_TEXT_URL\": [df.at[ind, \"FULL_TEXT_URL\"]],\n",
    "            \"FULL_TEXT_SOURCE\": [df.at[ind, \"FULL_TEXT_SOURCE\"]],\n",
    "            \"PDF_URL\": [df.at[ind, \"PDF_URL\"]],\n",
    "            \"PDF_SOURCE\": [df.at[ind, \"PDF_SOURCE\"]],\n",
    "            \"TITLE\": [df.at[ind, \"TITLE\"]],\n",
    "            \"ABSTRACT\": [df.at[ind, \"ABSTRACT\"]],\n",
    "            \"KEYWORDS\": [df.at[ind, \"KEYWORDS\"]],\n",
    "            \"SPECIES_RELATED\": [species],\n",
    "            \"TC_CT_RELATED\": [tc_ct],\n",
    "            \"THALAM_RELATED\": [thalam],\n",
    "            \"CORTEX_RELATED\": [cortex],\n",
    "            \"METHOD_RELATED\": [method],\n",
    "            \"CONNECTIVITY_RELATED\": [connectivity]\n",
    "        }\n",
    "        \n",
    "        # for key in on_topic_kws:\n",
    "        #     row[key] = [keywords_count[key]]\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        line_number_in_csv = ind + 1\n",
    "        print(\"Line number:\", line_number_in_csv, \" INDEX:\", int(df.at[ind, \"INDEX\"]))\n",
    "# --------------------start of test code--------------------\n",
    "# input_path = fpath.poten_litera_litera_db\n",
    "# output_path = fpath.poten_litera_litera_db_ranked\n",
    "# weight_and_rank(input_path, output_path)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edde53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rank():\n",
    "#     # relev_index = calcul_index(keywords_count, on_topic_kws_weights)\n",
    "#     relev_index = calcul_index(keywords_freq, on_topic_kws_weights)\n",
    "#     # print(index)\n",
    "\n",
    "#     columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \n",
    "#                    \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \n",
    "#                    \"LENGTH_TAK\", \"LENGTH_TXT\", \"LENGTH_TEXT\",\"RELEVANCE_INDEX\"]\n",
    "        \n",
    "#     # columns = columns + [key for key in keywords_count.keys()]\n",
    "\n",
    "#     row = {\n",
    "#         \"INDEX\": [df.at[ind, \"INDEX\"]],\n",
    "#         \"DOI\": [df.at[ind, \"DOI\"]],\n",
    "#         \"PMID\": [df.at[ind, \"PMID\"]],\n",
    "#         \"PMCID\": [df.at[ind, \"PMCID\"]],\n",
    "#         \"FULL_TEXT_URL\": [df.at[ind, \"FULL_TEXT_URL\"]],\n",
    "#         \"FULL_TEXT_SOURCE\": [df.at[ind, \"FULL_TEXT_SOURCE\"]],\n",
    "#         \"PDF_URL\": [df.at[ind, \"PDF_URL\"]],\n",
    "#         \"PDF_SOURCE\": [df.at[ind, \"PDF_SOURCE\"]],\n",
    "#         \"TITLE\": [df.at[ind, \"TITLE\"]],\n",
    "#         \"ABSTRACT\": [df.at[ind, \"ABSTRACT\"]],\n",
    "#         \"KEYWORDS\": [df.at[ind, \"KEYWORDS\"]],\n",
    "#         \"LENGTH_TAK\": [length_tak],\n",
    "#         \"LENGTH_TXT\": [length_txt],\n",
    "#         \"LENGTH_TEXT\": [length_text],\n",
    "#         \"RELEVANCE_INDEX\": [relev_index]\n",
    "#     }\n",
    "\n",
    "#     # rank\n",
    "#     df_ranked = pd.read_csv(output_path, header=0, sep=\",\")\n",
    "#     df_ranked.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \n",
    "#                    \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \n",
    "#                    \"LENGTH_TAK\", \"LENGTH_TXT\", \"LENGTH_TEXT\",\"RELEVANCE_INDEX\"]\n",
    "#     df_ranked = df_ranked.sort_values(by=\"RELEVANCE_INDEX\", ascending=False)\n",
    "#     df_ranked.reset_index(drop=True, inplace=True)\n",
    "#     df_ranked.to_csv(output_path, header=True, index=False)\n",
    "#     print(\"Weighting and ranking the potentially related literature succeded!\")\n",
    "#     print(\"Enjoy reading!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e562971",
   "metadata": {},
   "source": [
    "<h3> Main program: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dffcab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # process, correct the INDEX in potential_related_literature_testing_set_300_read.csv\n",
    "# # input_path = fpath.poten_litera_testing_set_300_read\n",
    "# # output_path = fpath.poten_litera_testing_set_300_read_index_corrected\n",
    "# # plib.clear_file(output_path)\n",
    "# # db_path = fpath.poten_litera_litera_db\n",
    "\n",
    "# # df_input = pd.read_csv(input_path, header=0, sep=',')\n",
    "# # df_input.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANCE\"]\n",
    "# # df_db = pd.read_csv(db_path, header=None, sep=',')\n",
    "# # df_db.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "# # df_db = df_db.fillna(0)\n",
    "# # df_db = df_db.astype({\"PMID\": int})\n",
    "# # # print(df_input.shape)\n",
    "# # # print(df_input.head(5))\n",
    "# # # (300, 12)\n",
    "# # # print(df_db.shape)\n",
    "# # # print(df_db.head(10))\n",
    "# # # (10776, 11)\n",
    "\n",
    "# # for ind in df_input.index:\n",
    "# #     index = df_input.at[ind, \"INDEX\"]\n",
    "# #     doi = df_input.at[ind, \"DOI\"]\n",
    "# #     pmid = df_input.at[ind, \"PMID\"]\n",
    "# #     # print(pmid, df_db.at[ind, \"PMID\"])\n",
    "# #     # print(pmid.type(), df_db.at[ind, \"PMID\"].type())\n",
    "# #     pmcid = df_input.at[ind, \"PMCID\"]\n",
    "# #     full_text_url = df_input.at[ind, \"FULL_TEXT_URL\"]\n",
    "# #     full_text_source = df_input.at[ind, \"FULL_TEXT_SOURCE\"]\n",
    "# #     title = df_input.at[ind, \"TITLE\"].lower()\n",
    "\n",
    "# #     if doi == doi:\n",
    "# #         try:\n",
    "# #             index = df_db.loc[df_db[\"DOI\"] == doi, 'INDEX'].values[0]\n",
    "# #             df_input.at[ind, \"INDEX\"] = index\n",
    "# #         except:\n",
    "# #             print(\"DOI not found in db:\", df_input.at[ind, \"INDEX\"], df_input.at[ind, \"RELEVANCE\"])\n",
    "# #             df_input.drop(ind, inplace=True)\n",
    "# #     elif pmid == pmid:\n",
    "# #         try:\n",
    "# #             index = df_db.loc[df_db[\"PMID\"]==int(pmid), 'INDEX'].values[0]\n",
    "# #             df_input.at[ind, \"INDEX\"] = index\n",
    "# #         except:\n",
    "# #             print(\"PMID not found in db:\", df_input.at[ind, \"INDEX\"], df_input.at[ind, \"RELEVANCE\"])\n",
    "# #             df_input.drop(ind, inplace=True)\n",
    "# #     elif pmcid == pmcid:\n",
    "# #         index = df_db.loc[df_db[\"PMCID\"] == pmcid, 'INDEX'].values[0]\n",
    "# #         df_input.at[ind, \"INDEX\"] = index\n",
    "# #     elif title.lower() == title.lower():\n",
    "# #         index = df_db.loc[df_db[\"TITLE\"].str.lower() == title, 'INDEX'].values[0]\n",
    "# #         df_input.at[ind, \"INDEX\"] = index\n",
    "# #     else:\n",
    "# #         print(\"ALL 4 identifiers and title are missing:\", df_input.at[ind, \"INDEX\"], df_input.at[ind, \"RELEVANCE\"])\n",
    "    \n",
    "# # df_input.reset_index(drop=True, inplace=True)\n",
    "# # df_input.to_csv(output_path, header=True, index=False)\n",
    "\n",
    "# corrected = fpath.poten_litera_testing_set_300_read_index_corrected\n",
    "# df_input = pd.read_csv(corrected, header=0, sep=',')\n",
    "# print(df_input.shape)\n",
    "# # (292, 12)\n",
    "# print(df_input.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b073ef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test if the result matches the db\n",
    "# result_path = fpath.poten_litera_testing_set_300_read_index_corrected\n",
    "# db_path = fpath.poten_litera_litera_db\n",
    "\n",
    "# df_result= pd.read_csv(result_path, header=0, sep=',')\n",
    "# df_result.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANCE\"]\n",
    "# df_db = pd.read_csv(db_path, header=None, sep=',')\n",
    "# df_db.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "# # print(df_result.shape)\n",
    "# # print(df_result.head(5))\n",
    "# # (292, 12)\n",
    "# # print(df_db.shape)\n",
    "# # print(df_db.head(10))\n",
    "# # (10776, 11)\n",
    "\n",
    "# for ind in df_result.index:\n",
    "#     index = int(df_result.at[ind, \"INDEX\"])\n",
    "#     title = ''.join([char for char in df_result.at[ind, \"TITLE\"].lower() if re.match(r'[a-z\\s]', char)])\n",
    "#     cleaned_title = re.sub(r'\\s+', ' ', title).strip()\n",
    "#     title_db = ''.join([char for char in df_db.loc[df_db[\"INDEX\"].astype(int) == index, 'TITLE'].values[0].lower() if re.match(r'[a-z\\s]', char)])\n",
    "#     cleaned_title_db = re.sub(r'\\s+', ' ', title_db).strip()\n",
    "    \n",
    "#     if cleaned_title == cleaned_title_db:\n",
    "#         pass\n",
    "#     else:\n",
    "#         # pass\n",
    "#         print(index)\n",
    "#         print(cleaned_title)\n",
    "#         print(cleaned_title_db)\n",
    "#         print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2836c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract and filling info to construct litera_db\n",
    "# input_path = fpath.poten_litera_ids_ftl_filled_filtered\n",
    "# output_path = fpath.poten_litera_litera_db\n",
    "\n",
    "# # clear file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# info_filling(input_path, output_path, 0, 10980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0e62a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # process the resuls: remove duplicates and reset index\n",
    "# input_path = fpath.poten_litera_litera_db\n",
    "# output_path = fpath.poten_litera_litera_db\n",
    "\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "# identifiers = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\"]\n",
    "\n",
    "# for identifier in identifiers:\n",
    "#     remove_dup_by = identifier\n",
    "#     df = df[df[remove_dup_by].isnull() | ~df[df[remove_dup_by].notnull()].duplicated(subset=remove_dup_by, keep='last')]\n",
    "\n",
    "# # reset index\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# df.to_csv(output_path, header=False, index=False)\n",
    "# print(\"Duplication removed.\")\n",
    "\n",
    "# input_path = fpath.poten_litera_litera_db\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# print(df.shape)\n",
    "# # (10776, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55901fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # process the db so that no text contains no \",\" and are identified as seperators and if keywords starts with \"nan\", remove it\n",
    "# input_path = fpath.poten_litera_db\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "# for ind in df.index:\n",
    "#     if df.at[ind, \"TITLE\"] == df.at[ind, \"TITLE\"]:\n",
    "#         df.at[ind, \"TITLE\"] = df.at[ind, \"TITLE\"].replace(\",\", \";\").strip()\n",
    "#     if df.at[ind, \"ABSTRACT\"] == df.at[ind, \"ABSTRACT\"]:\n",
    "#         df.at[ind, \"ABSTRACT\"] = df.at[ind, \"ABSTRACT\"].replace(\",\", \";\").strip()\n",
    "#     if df.at[ind, \"KEYWORDS\"] == df.at[ind, \"KEYWORDS\"]:\n",
    "#         k = df.at[ind, \"KEYWORDS\"].replace(\",\", \";\").strip()\n",
    "#         if k.startswith(\"nan\"):\n",
    "#             k = k.split(\"nan;\")[1].strip()\n",
    "#         df.at[ind, \"KEYWORDS\"] = k\n",
    "\n",
    "# df.to_csv(input_path, header=False, index=False)\n",
    "\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# print(df.shape)\n",
    "# print(df.head(5))\n",
    "# # (10776, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5870a3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # list all pdf source\n",
    "# input_path = fpath.poten_litera_litera_db\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "# pdf_source_set = set(df['PDF_SOURCE'].tolist())\n",
    "# print(pdf_source_set)\n",
    "# # {'www.ahajournals.org', 'anatomypubs.onlinelibrary.wiley.com', 'citeseerx.ist.psu.edu', 'www.nature.com', \n",
    "# # 'iovs.arvojournals.org', 'www.microbiologyresearch.org', 'nyaspubs.onlinelibrary.wiley.com', 'ahuman.org', \n",
    "# # 'karger.com', 'www.imrpress.com', 'www.researchsquare.com', 'link.springer.com', 'www.ijpp.com', \n",
    "# # 'europepmc.org', nan, 'www.cell.com', 'www.bu.edu', 'www.ncbi.nlm.nih.gov', 'jamanetwork.com', \n",
    "# # 'www.thieme-connect.de', 'www.science.org', 'physoc.onlinelibrary.wiley.com', 'deepblue.lib.umich.edu', \n",
    "# # 'bpb-us-e1.wpmucdn.com', 'www.researchgate.net', 'ieeexplore.ieee.org', 'zsp.com.pk', 'journals.biologists.com', \n",
    "# # 'journals.aps.org', 'papers.ssrn.com', 'academic.oup.com', 'onlinelibrary.wiley.com', 'www.hifo.uzh.ch', \n",
    "# # 'royalsocietypublishing.org', 'www.biorxiv.org', 'www.ingentaconnect.com', 'ujms.net', 'enpubs.faculty.ucdavis.edu', \n",
    "# # 'ajp.psychiatryonline.org', 'n.neurology.org', 'www.annualreviews.org', 'ruor.uottawa.ca', 'neuro.psychiatryonline.org', \n",
    "# # 'www.jstage.jst.go.jp', 'synapse.koreamed.org', 'journals.physiology.org', 'linkinghub.elsevier.com', \n",
    "# # 'www.tandfonline.com', 'www.jneurosci.org', 'analyticalsciencejournals.onlinelibrary.wiley.com', 'pubs.asahq.org', \n",
    "# # 'thejns.org', 'biomedical-engineering-online.biomedcentral.com', 'journals.sagepub.com', 'direct.mit.edu', \n",
    "# # 'pubs.acs.org', 'pharmrev.aspetjournals.org', 'journals.lww.com', 'jnm.snmjournals.org', 'jpet.aspetjournals.org', \n",
    "# # 'movementdisorders.onlinelibrary.wiley.com'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3533e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # list the pdf_source and prepare the test code for downloading the pdfs\n",
    "# pdf_source_set = [\n",
    "#     'www.ahajournals.org', 'wiley.com', 'citeseerx.ist.psu.edu', 'www.nature.com', \n",
    "#     'iovs.arvojournals.org', 'www.microbiologyresearch.org', 'ahuman.org', \n",
    "#     'karger.com', 'www.imrpress.com', 'www.researchsquare.com', 'link.springer.com', 'www.ijpp.com', \n",
    "#     'europepmc.org', 'www.cell.com', 'www.bu.edu', 'www.ncbi.nlm.nih.gov', 'jamanetwork.com', \n",
    "#     'www.thieme-connect.de', 'www.science.org', 'deepblue.lib.umich.edu', \n",
    "#     'bpb-us-e1.wpmucdn.com', 'www.researchgate.net', 'ieeexplore.ieee.org', 'zsp.com.pk', 'journals.biologists.com', \n",
    "#     'journals.aps.org', 'papers.ssrn.com', 'academic.oup.com', 'www.hifo.uzh.ch', \n",
    "#     'royalsocietypublishing.org', 'www.biorxiv.org', 'www.ingentaconnect.com', 'ujms.net', 'enpubs.faculty.ucdavis.edu', \n",
    "#     'psychiatryonline.org', 'n.neurology.org', 'www.annualreviews.org', 'ruor.uottawa.ca', \n",
    "#     'www.jstage.jst.go.jp', 'synapse.koreamed.org', 'journals.physiology.org', 'linkinghub.elsevier.com', \n",
    "#     'www.tandfonline.com', 'www.jneurosci.org', 'pubs.asahq.org', \n",
    "#     'thejns.org', 'biomedcentral.com', 'journals.sagepub.com', 'direct.mit.edu', \n",
    "#     'pubs.acs.org', 'aspetjournals.org', 'journals.lww.com', 'jnm.snmjournals.org'\n",
    "# ]\n",
    "\n",
    "# input_path = fpath.poten_litera_litera_db\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "# for website in pdf_source_set:\n",
    "#     for ind in df.index:\n",
    "#         if df.at[ind, \"PDF_SOURCE\"] != df.at[ind, \"PDF_SOURCE\"]:\n",
    "#             continue\n",
    "#         if website in df.at[ind, \"PDF_SOURCE\"]:\n",
    "#             print(\"# \" + website)\n",
    "#             print(\"\\\"\" + df.at[ind, \"PDF_URL\"] + \"\\\"\")\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f88efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_path = fpath.poten_litera_litera_db\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# print(df.shape)\n",
    "# print(df.head(5))\n",
    "# # (10776, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daccd342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download pdfs and rename them to build a database\n",
    "# input_path = fpath.poten_litera_db\n",
    "# pdf_folder = fpath.pdf_folder\n",
    "# download_pdf(input_path, pdf_folder, 0, 10776)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ce9f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the downloaded pdfs and jsons\n",
    "def test_pdf(pdf_path):       \n",
    "    # opens the file for reading\n",
    "    with open(pdf_path, 'rb') as p:\n",
    "        txt = (p.readlines())\n",
    "\n",
    "    actual_line = len(txt)\n",
    "    \n",
    "    for i, x in enumerate(txt[::-1]):\n",
    "        if b'%%EOF' in x:\n",
    "            actual_line = len(txt)-i\n",
    "            # print(f'EOF found at line position {-i} = actual {actual_line}, with value {x}')\n",
    "            break\n",
    "    \n",
    "    if actual_line != len(txt):\n",
    "        # get the new list terminating correctly\n",
    "        txtx = txt[:actual_line]\n",
    "\n",
    "        # write to new pdf\n",
    "        with open(pdf_path, 'wb') as f:\n",
    "            f.writelines(txtx)\n",
    "        f.close()\n",
    "\n",
    "    fixed_pdf = PyPDF2.PdfReader(pdf_path)\n",
    "\n",
    "    page_max = len(fixed_pdf.pages)\n",
    "\n",
    "    if page_max < 5:\n",
    "        print(page_max)\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "# --------------------start of test code--------------------\n",
    "# input_path = fpath.poten_litera_db\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "# pdf_folder = fpath.pdf_folder\n",
    "\n",
    "# start = 0\n",
    "# end = 10776\n",
    "\n",
    "# for ind in range(start, end):\n",
    "    # index = str(int(df.at[ind, \"INDEX\"]))\n",
    "    # pdf_file_name = str(index) + \".pdf\"\n",
    "    # json_file_name = str(index) + \".json\"\n",
    "    # pdf_path = os.path.join(pdf_folder, pdf_file_name)\n",
    "    # json_path = os.path.join(pdf_folder, json_file_name)\n",
    "\n",
    "    # try:\n",
    "    #     if os.path.exists(json_path):\n",
    "    #         print(ind, index)\n",
    "    #         continue\n",
    "    #     elif os.path.exists(pdf_path):\n",
    "    #         if test_pdf(pdf_path):\n",
    "    #             print(ind, index)\n",
    "    #         else:\n",
    "    #             print(\"\\n\")\n",
    "    #             print(\"PDF LEGNTH < 3\")\n",
    "    #             print(df.at[ind, \"INDEX\"], df.at[ind, \"DOI\"], df.at[ind, \"PMID\"], df.at[ind, \"PMCID\"])\n",
    "    #             print(df.at[ind, \"FULL_TEXT_URL\"], df.at[ind, \"FULL_TEXT_SOURCE\"])\n",
    "    #             print(df.at[ind, \"PDF_URL\"], df.at[ind, \"PDF_SOURCE\"])\n",
    "    #             print(ind, index)\n",
    "    #             print(\"\\n\")\n",
    "    #     else:\n",
    "    #         print(\"\\n\")\n",
    "    #         print(\"PDF NOT AVAILABLE\")\n",
    "    #         print(df.at[ind, \"INDEX\"], df.at[ind, \"DOI\"], df.at[ind, \"PMID\"], df.at[ind, \"PMCID\"])\n",
    "    #         print(df.at[ind, \"FULL_TEXT_URL\"], df.at[ind, \"FULL_TEXT_SOURCE\"])\n",
    "    #         print(df.at[ind, \"PDF_URL\"], df.at[ind, \"PDF_SOURCE\"])\n",
    "    #         print(ind, index)\n",
    "    #         print(\"\\n\")\n",
    "    # except:\n",
    "    #     print(\"\\n\")\n",
    "    #     print(\"PDF Corrupted\")\n",
    "    #     print(df.at[ind, \"INDEX\"], df.at[ind, \"DOI\"], df.at[ind, \"PMID\"], df.at[ind, \"PMCID\"])\n",
    "    #     print(df.at[ind, \"FULL_TEXT_URL\"], df.at[ind, \"FULL_TEXT_SOURCE\"])\n",
    "    #     print(df.at[ind, \"PDF_URL\"], df.at[ind, \"PDF_SOURCE\"])\n",
    "    #     print(ind, index)\n",
    "    #     print(\"\\n\")\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310f2489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # copy the relevant and not relevant pdfs and jsons from \"pdfs\" to repective folders \"relevant_pdfs\" and \"not_relevant_pdfs\"\n",
    "# test_path = fpath.poten_litera_testing_set_300_read_index_corrected\n",
    "# destination1 = \"/media/hou/DIDIHOU/relevant_pdfs\"\n",
    "# destination2 = \"/media/hou/DIDIHOU/not_relevant_pdfs\"\n",
    "\n",
    "# df_test = pd.read_csv(test_path, header=0, sep=',')\n",
    "# df_test.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANT\"]\n",
    "\n",
    "# for ind in df_test.index:\n",
    "#     index = df_test.at[ind, \"INDEX\"]\n",
    "#     if df_test.at[ind, \"RELEVANT\"] == \"YES\": # relevant\n",
    "#         flag = False\n",
    "#         print(ind, index)\n",
    "        \n",
    "#         json_path = os.path.join(fpath.pdf_folder, str(index) + \".json\")\n",
    "#         pdf_path = os.path.join(fpath.pdf_folder, str(index) + \".pdf\")\n",
    "        \n",
    "#         if os.path.exists(json_path):\n",
    "#             shutil.copy(json_path, destination1)\n",
    "#             flag = True\n",
    "#         if os.path.exists(pdf_path):\n",
    "#             shutil.copy(pdf_path, destination1)\n",
    "#             flag = True\n",
    "        \n",
    "#         if not flag:\n",
    "#             print(\"No file found for index: \", index)\n",
    "#     else: # not relevant\n",
    "#         json_path = os.path.join(fpath.pdf_folder, str(index) + \".json\")\n",
    "#         pdf_path = os.path.join(fpath.pdf_folder, str(index) + \".pdf\")\n",
    "\n",
    "#         if os.path.exists(json_path):\n",
    "#             shutil.copy(json_path, destination2)\n",
    "#         if os.path.exists(pdf_path):\n",
    "#             shutil.copy(pdf_path, destination2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa946879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_pdf_300(db_path, test_path, pdf_folder, start, end):\n",
    "#     df_db = pd.read_csv(db_path, header=None, sep=',')\n",
    "#     df_db.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "#     df_test = pd.read_csv(test_path, header=0, sep=',')\n",
    "#     df_test.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANT\"]\n",
    "\n",
    "#     for ind in range(start, end):\n",
    "#         # time.sleep(2)\n",
    "#         flag = False\n",
    "#         index = df_test.at[ind, \"INDEX\"]\n",
    "#         pdf_url = df_db.loc[df_db['INDEX'] == index, \"PDF_URL\"].values[0]\n",
    "        \n",
    "#         # pdf_url found\n",
    "#         if pdf_url != pdf_url:\n",
    "#             flag = False\n",
    "#         else:\n",
    "#             doi = df_test.at[ind, \"DOI\"]\n",
    "#             if doi == doi:\n",
    "#                 doi = doi.lower().strip()\n",
    "#             else:\n",
    "#                 pass\n",
    "#             if dpp.download_and_rename_pdf(pdf_url, doi, index, pdf_folder):\n",
    "#                 flag = True\n",
    "#             else:\n",
    "#                 flag = False\n",
    "        \n",
    "#         if not flag:\n",
    "#             print(\"\\n\")\n",
    "#             print(\"PDF_URL not found or PDF not successfully downloaded for:\")\n",
    "#             print(index)\n",
    "#             print(df_db.loc[df_db['INDEX'] == index, \"FULL_TEXT_URL\"].values[0])\n",
    "#             print(df_db.loc[df_db['INDEX'] == index, \"PDF_URL\"].values[0])\n",
    "#             print(df_db.loc[df_db['INDEX'] == index, \"TITLE\"].values[0])\n",
    "\n",
    "#         line_number_in_csv = ind + 1\n",
    "#         print(ind, \" Line number:\", line_number_in_csv, \" INDEX:\", index)\n",
    "#         print(\"\\n\")\n",
    "\n",
    "# # # download the pdfs of 300 test papers\n",
    "# db_path = fpath.poten_litera_db\n",
    "# test_path = fpath.poten_litera_testing_set_300_read_index_corrected\n",
    "# pdf_folder = fpath.pdf_folder\n",
    "# download_pdf_300(db_path, test_path, pdf_folder, 0, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67041512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # count the average and maximum number of words in the title + keywords + abstract\n",
    "# input_path = fpath.poten_litera_db\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "# count = 0\n",
    "# max = 0\n",
    "# average = 0\n",
    "\n",
    "# for ind in df.index:\n",
    "#     if df.at[ind, \"TITLE\"] == df.at[ind, \"TITLE\"]:\n",
    "#         title = df.at[ind, \"TITLE\"]\n",
    "#     else:\n",
    "#         title = \"\"\n",
    "#     if df.at[ind, \"ABSTRACT\"] == df.at[ind, \"ABSTRACT\"]:\n",
    "#         abstract = df.at[ind, \"ABSTRACT\"]\n",
    "#     else:\n",
    "#         abstract = \"\"\n",
    "#         continue\n",
    "#     if df.at[ind, \"KEYWORDS\"] == df.at[ind, \"KEYWORDS\"]:\n",
    "#         keywords = df.at[ind, \"KEYWORDS\"]\n",
    "#     else:\n",
    "#         keywords = \"\"\n",
    "\n",
    "#     count += 1\n",
    "#     text = title + \" \" + abstract + \" \" + keywords\n",
    "#     text = text.lower()\n",
    "#     # replace any non-alphabetical character space, but keep - and space\n",
    "#     text = re.sub(r'[^a-z\\s\\-]', '', text)\n",
    "#     text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "#     num_words = len(text.split(\" \"))\n",
    "#     average += num_words\n",
    "#     if num_words > max:\n",
    "#         max = num_words\n",
    "\n",
    "# average = average / count\n",
    "\n",
    "# print(\"count:\", count)\n",
    "# print(\"max:\", max)\n",
    "# print(\"average:\", average)\n",
    "# # count: 9892\n",
    "# # max: 1307\n",
    "# # average: 235.05327537403963"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb6f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract text to store to a text file, record the literatures whose pdfs or jsons are not available\n",
    "# input_path = fpath.poten_litera_db\n",
    "# df = pd.read_csv(input_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "# # poten_litera_pdf_not_available = fpath.poten_litera_pdf_not_available\n",
    "# # plib.clear_file(poten_litera_pdf_not_available)\n",
    "\n",
    "# pdf_folder = fpath.pdf_folder\n",
    "# text_folder = fpath.text_folder\n",
    "# start_page = 0\n",
    "# text_length_to_extract = params.text_length_to_extract\n",
    "\n",
    "# for ind in df.index:\n",
    "#     time.sleep(2)\n",
    "#     index = str(int(df.at[ind, \"INDEX\"]))\n",
    "#     pdf_file_name = str(index) + \".pdf\"\n",
    "#     json_file_name = str(index) + \".json\"\n",
    "#     pdf_path = os.path.join(pdf_folder, pdf_file_name)\n",
    "#     json_path = os.path.join(pdf_folder, json_file_name)\n",
    "#     text_path = os.path.join(text_folder, str(index) + \".txt\")\n",
    "    \n",
    "#     if os.path.exists(json_path):\n",
    "#         json2text(json_path, text_path, text_length_to_extract)\n",
    "#     elif os.path.exists(pdf_path):\n",
    "#         pdf2text(pdf_path, text_path, start_page, text_length_to_extract)\n",
    "#     else:\n",
    "#         selected_row = df.iloc[[ind]]\n",
    "#         # selected_row.to_csv(poten_litera_pdf_not_available, mode='a', header=False, index=False)\n",
    "#         print(\"\\n\")\n",
    "#         # print(df.at[ind, \"INDEX\"], df.at[ind, \"DOI\"], df.at[ind, \"PMID\"], df.at[ind, \"PMCID\"])\n",
    "#         print(df.at[ind, \"FULL_TEXT_URL\"], df.at[ind, \"FULL_TEXT_SOURCE\"])\n",
    "#         print(df.at[ind, \"PDF_URL\"], df.at[ind, \"PDF_SOURCE\"])\n",
    "#         # print(df.at[ind, \"TITLE\"], df.at[ind, \"ABSTRACT\"], df.at[ind, \"KEYWORDS\"])\n",
    "#         print(\"\\n\")\n",
    "    \n",
    "#     print(ind, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9493b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in columns containing information of the number of times keywords appear and assign index to each literature and rank them\n",
    "input_path = fpath.poten_litera_db\n",
    "output_path = fpath.poten_litera_db_final\n",
    "\n",
    "# clear file\n",
    "plib.clear_file(output_path)\n",
    "\n",
    "weight_and_count(input_path, output_path, params.ranking_params, params.ranking_params_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa55d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # obtain the relevance_index of YESs and NOs of the test data set and draw a violin plot, and calculate the difference between the two distributions\n",
    "# # the difference is defined as 1. t-statistic 2. \n",
    "# db_ranked_path = fpath.poten_litera_db_ranked\n",
    "# test_path = fpath.poten_litera_testing_set_300_read_index_corrected\n",
    "\n",
    "# df_db_ranked = pd.read_csv(db_ranked_path, header=0, sep=',')\n",
    "# df_db_ranked.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \n",
    "#                         \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \n",
    "#                         \"LENGTH_TAK\", \"LENGTH_TXT\", \"LENGTH_TEXT\",\"RELEVANCE_INDEX\"]\n",
    "\n",
    "# df_test = pd.read_csv(test_path, header=0, sep=',')\n",
    "# df_test.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANT\"]\n",
    "\n",
    "# relvant_index = []\n",
    "# relevant_relevance_index_list = []\n",
    "\n",
    "# non_relevant_index = []\n",
    "# non_relevant_relevance_index_list = []\n",
    "\n",
    "# for ind in df_test.index:\n",
    "#     index = int(df_test.at[ind, \"INDEX\"])\n",
    "#     # print(ind, index)\n",
    "#     # print(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "\n",
    "#     if df_test.at[ind, \"RELEVANT\"] == \"YES\" and df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"LENGTH_TEXT\"].values[0] > 100:\n",
    "#         relvant_index.append(index)\n",
    "#         relevant_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "#     elif df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"LENGTH_TEXT\"].values[0] > 100:\n",
    "#         non_relevant_index.append(index)\n",
    "#         non_relevant_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "#     else:\n",
    "#         pass\n",
    "\n",
    "# # print(len(relvant_index))\n",
    "# print(\"Numer of relevant literature:\", len(relevant_relevance_index_list))\n",
    "# # print(len(non_relevant_index))\n",
    "# print(\"Number of not relevant literature:\", len(non_relevant_relevance_index_list))\n",
    "\n",
    "# print()\n",
    "\n",
    "# # draw violin plot\n",
    "\n",
    "# # Combine the data for plotting\n",
    "# data = relevant_relevance_index_list + non_relevant_relevance_index_list\n",
    "# labels = ['Relevant'] * len(relevant_relevance_index_list) + ['Not Relevant'] * len(non_relevant_relevance_index_list)\n",
    "\n",
    "# # Create a DataFrame\n",
    "# df = pd.DataFrame({'Relevance Index': data, 'Label': labels})\n",
    "\n",
    "# # Plot using seaborn\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.violinplot(x='Label', y='Relevance Index', data=df)\n",
    "# plt.title('Distribution of Relevance Index')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc9ccf",
   "metadata": {},
   "source": [
    "<h3> Next step: manually read papers and find all actually related literature </h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
