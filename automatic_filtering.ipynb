{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea1cce5-4f07-4bd7-8ca3-5f6fa51254d0",
   "metadata": {},
   "source": [
    "<h2> Automatic filtering </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5a61c5-f8c6-418a-b450-cdea0378ddab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "from nltk import ngrams\n",
    "import PyPDF2\n",
    "import json\n",
    "import time\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import shutil\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726b12c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib\n",
    "import extract_info\n",
    "import parameters as params\n",
    "import download_and_process_pdf as dpp\n",
    "import dataframe_columns as df_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489f0451",
   "metadata": {},
   "source": [
    "<h3> Predefined fucntions: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce32f7b8-96b7-4f5c-a2da-3ff931cdeaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_from_text(text, word): \n",
    "    # process text\n",
    "    text = plib.process_text(text, lower=True)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    len_word = len(word.split())\n",
    "    \n",
    "    # get the words of length len_word from text\n",
    "    ng = list(ngrams(tokens, len_word))\n",
    "    words = [' '.join(gram) for gram in ng]\n",
    "    # print(words)\n",
    "\n",
    "    # count word\n",
    "    if word in params.exact_match_kw_list: # if word is in exact_match_kw_list, use exact match\n",
    "        count = 0\n",
    "        for w in words:\n",
    "            if word == w:\n",
    "                count += 1\n",
    "    else: # if word is not in exact_match_kw_list, use fuzzy match\n",
    "        count = text.count(word)\n",
    "    return count\n",
    "# --------------------Start of test code--------------------\n",
    "# # text = 'This all.6 apple 6i7s very_tasty？、 apple, 6i7s 2but-the banana this is not delicious at Is all.6'\n",
    "# text = \"cat of the dopcate innervaton in innervation the macaque and human thalamus Miguel Ángel García-Cabezas,aBeatriz Rico,a,b Miguel Ángel Sánchez-González,aand Carmen Cavadaa,⁎ aDepartamento de Anatomía, Histología y Neurociencia, Facultad de Medicina, Universidad Autónoma de Madrid, C/Arzobispo Morcillo s/n, 28029 Madrid, Spain bInstituto de Neurociencias de Alicante, Universidad Miguel Hernández-CSIC, 03550 Sant Joan d ’Alacant, Spain Received 19 April 2006; revised 8 June 2006; accepted 11 July 2006 Available online 30 November 2006 We recently defined the thalamic dopaminergic system in primates; it arises from numerous dopaminergic cell groups and selectively targetsnumerous thalamic nuclei. Given the central position of the thalamus in subcortical and cortical interplay, and the functional relevance of dopamine neuromodulation in the brain, detailing dopamine dis-tribution in the thalamus should supply important information. Tothis end we performed immunohistochemistry for dopamine and the dopamine transporter in the thalamus of macaque monkeys and humans to generate maps, in the stereotaxic coronal plane, of thedistribution of dopaminergic axons. The dopamine innervation of the thalamus follows the same pattern in both species and is most dense in midline limbic nuclei, the mediodorsal and lateral posteriorassociation nuclei, and in the ventral lateral and ventral anteriormotor nuclei. This distribution suggests that thalamic dopamine has a prominent role in emotion, attention, cognition and complex somatosensory and visual processing, as well as in motor control.Most thalamic dopaminergic axons are thin and varicose and targetboth the neuropil and small blood vessels, suggesting that, besides neuronal modulation, thalamic dopamine may have a direct influence on microcirculation. The maps provided here should be a usefulreference in future experimental and neuroimaging studies aiming atclarifying the role of the thalamic dopaminergic system in health and in conditions involving brain dopamine, including Parkinson ’s disease, drug addiction and schizophrenia.© 2006 Elsevier Inc. All rights reserved. Keywords: Dopamine; Thalamus; Monkey; Human; Primate; Dopamine transporter; Parkinson; Schizophrenia; AddictionIntroduction The thalamus is made up of multiple nuclei relaying information from subcortical centers or from other cortices to the cerebral cortex (Sherman and Guillery, 2005 ), as well as the striatum, the nucleus accumbens and the amygdala ( Steriade et al., 1997 ). In addition to specific subcortical and cortical afferents, the primate thalamus receives axons containing the neuromodulators acetylcholine (Heckers et al., 1992 ), histamine ( Manning et al., 1996 ), serotonin (Morrison and Foote, 1986; Lavoie and Parent, 1991 ), and the catecholamines adrenaline ( Rico and Cavada, 1998a ), noradrenaline (Morrison and Foote, 1986; Ginsberg et al., 1993 ) and dopamine (Sánchez-González et al., 2005 ). Until recently, the existence of significant dopamine innervation in the primate thalamus has been largely ignored, probably becausedopamine innervation of the rodent thalamus is very scant(Groenewegen, 1988; Papadopoulos and Parnavelas, 1990 ). However, fragmentary data scattered through the literature endorse the presence of dopamine innervation in the primate thalamus.Postmortem biochemical studies showed the presence of dopamine in the thalamus of macaques ( Brown et al., 1979; Goldman-Rakic and Brown, 1981; Pifl et al., 1990, 1991 ) and human subjects ( Oke and Adams, 1987 ). Later, receptor binding and in situ hybridization analyses detected the presence of dopamine D2-like ( Joyce et al., 1991; Kessler et al., 1993; Hall et al., 1996; Langer et al., 1999;Rieck et al., 2004 ) and D3-like receptors ( Gurevich and Joyce, 1999 ) in several human thalamic nuclei. Positron emission tomography (PET) radioligand studies have also demonstratedthe presence of the dopamine transporter (DAT) ( Wang et al., 1995; Halldin et al., 1996; Helfenbein et al., 1999; Brownell et al., 2003 ) and of D2-like receptors ( Farde et al., 1997; Langer et al., 1999; Okubo et al., 1999; Brownell et al., 2003; Rieck et al., 2004 ) in the human and macaque thalamus. In the course of PET studies focusing on schizophrenia, D2- and D3-like radioligand binding was also found in the thalamus of control subjects ( Talvik et al., 2003; Yasuno et al., 2004 ). Finally, an immunohistochemical study using anti-DAT antibodies detected the presence of dopaminergic www.elsevier.com/locate/ynimg NeuroImage 34 (2007) 965 –984 ⁎Corresponding author. Fax: +34 91 497 53 15. E-mail address: carmen.cavada@uam.es (C. Cavada). Available online on ScienceDirect (www.sciencedirect.com). 1053-8119/$ - see front matter © 2006 Elsevier Inc. All rights reserved. doi:10.1016/j.neuroimage.2006.07.032\"\n",
    "# keyword = 'innervat'\n",
    "# count = count_word_from_text(text, keyword)\n",
    "# print(count)\n",
    "# ---------------------End of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b488729c-2e38-43e8-bba2-d81126f38847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_kw_group_from_text(text_tak, text_txt, keyword_group):\n",
    "    if text_txt == text_txt and text_txt != \"\": # if full text is available, use full text\n",
    "        text = text_txt\n",
    "    elif text_tak == text_tak and text_tak != \"\": # if full text is not available, use tak (title + abstract + keywords)\n",
    "        text = text_tak\n",
    "    else:\n",
    "        raise ValueError('text_tak and text_txt cannot both be np.nan.')\n",
    "\n",
    "    # process the text\n",
    "    text = plib.process_text(text, lower=True)\n",
    "\n",
    "    # length_text = len(text.split())\n",
    "\n",
    "    # count the number of keywords\n",
    "    word_count = 0\n",
    "    for word in keyword_group:\n",
    "        word_count = word_count + count_word_from_text(text, word)\n",
    "\n",
    "    return word_count\n",
    "# --------------------start of test code--------------------\n",
    "# text_tak = 'Virhesussiorhesuscat macaqua Cat Rhesus fRhesusor rhesus action: thalamic and cortical inputs to the macaque superior tract neural tracing, parietal lobule The dorsal visual stream, the cortical circuit that in the primate brain is mainly dedicated to the visual control of actions, is split into two routes, a lateral and a medial one, both involved in coding different aspects of sensorimotor control of actions. The lateral route, named \"lateral grasping network\", is mainly involved in the control of the distal part of prehension, namely grasping and manipulation. The medial route, named \"reach-to-grasp network\", is involved in the control of the full deployment of prehension act, from the direction of arm movement to the shaping of the hand according to the object to be grasped. In macaque monkeys, the reach-to-grasp network (the target of this review) includes areas of the superior parietal lobule (SPL) that hosts visual and somatosensory neurons well suited to control goal-directed limb movements toward stationary as well as moving objects. After a brief summary of the neuronal functional properties of these areas, we will analyze their cortical and thalamic inputs thanks to retrograde neuronal tracers separately injected into the SPL areas V6, V6A, PEc, and PE. These areas receive visual and somatosensory information distributed in a caudorostral, visuosomatic trend, and some of them are directly connected with the dorsal premotor cortex. This review is particularly focused on the origin and type of visual information reaching the SPL, and on the functional role this information can play in guiding limb interaction with objects in structured and dynamic environments. Area PEc; Area V6; Area V6A; Dorsal visual stream; Goal-directed arm movement; Sensorimotor integration.'\n",
    "# # text_txt = \"rhesusSDistrirhesusbution rhesus of the dopamine innervation in the macaque and human thalamus Miguel Ángel García-Cabezas,aBeatriz Rico,a,b Miguel Ángel Sánchez-González,aand Carmen Cavadaa,⁎ aDepartamento de Anatomía, Histología y Neurociencia, Facultad de Medicina, Universidad Autónoma de Madrid, C/Arzobispo Morcillo s/n, 28029 Madrid, Spain bInstituto de Neurociencias de Alicante, Universidad Miguel Hernández-CSIC, 03550 Sant Joan d ’Alacant, Spain Received 19 April 2006; revised 8 June 2006; accepted 11 July 2006 Available online 30 November 2006 We recently defined the thalamic dopaminergic system in primates; it arises from numerous dopaminergic cell groups and selectively targetsnumerous thalamic nuclei. Given the central position of the thalamus in subcortical and cortical interplay, and the functional relevance of dopamine neuromodulation in the brain, detailing dopamine dis-tribution in the thalamus should supply important information. Tothis end we performed immunohistochemistry for dopamine and the dopamine transporter in the thalamus of macaque monkeys and humans to generate maps, in the stereotaxic coronal plane, of thedistribution of dopaminergic axons. The dopamine innervation of the thalamus follows the same pattern in both species and is most dense in midline limbic nuclei, the mediodorsal and lateral posteriorassociation nuclei, and in the ventral lateral and ventral anteriormotor nuclei. This distribution suggests that thalamic dopamine has a prominent role in emotion, attention, cognition and complex somatosensory and visual processing, as well as in motor control.Most thalamic dopaminergic axons are thin and varicose and targetboth the neuropil and small blood vessels, suggesting that, besides neuronal modulation, thalamic dopamine may have a direct influence on microcirculation. The maps provided here should be a usefulreference in future experimental and neuroimaging studies aiming atclarifying the role of the thalamic dopaminergic system in health and in conditions involving brain dopamine, including Parkinson ’s disease, drug addiction and schizophrenia.© 2006 Elsevier Inc. All rights reserved. Keywords: Dopamine; Thalamus; Monkey; Human; Primate; Dopamine transporter; Parkinson; Schizophrenia; AddictionIntroduction The thalamus is made up of multiple nuclei relaying information from subcortical centers or from other cortices to the cerebral cortex (Sherman and Guillery, 2005 ), as well as the striatum, the nucleus accumbens and the amygdala ( Steriade et al., 1997 ). In addition to specific subcortical and cortical afferents, the primate thalamus receives axons containing the neuromodulators acetylcholine (Heckers et al., 1992 ), histamine ( Manning et al., 1996 ), serotonin (Morrison and Foote, 1986; Lavoie and Parent, 1991 ), and the catecholamines adrenaline ( Rico and Cavada, 1998a ), noradrenaline (Morrison and Foote, 1986; Ginsberg et al., 1993 ) and dopamine (Sánchez-González et al., 2005 ). Until recently, the existence of significant dopamine innervation in the primate thalamus has been largely ignored, probably becausedopamine innervation of the rodent thalamus is very scant(Groenewegen, 1988; Papadopoulos and Parnavelas, 1990 ). However, fragmentary data scattered through the literature endorse the presence of dopamine innervation in the primate thalamus.Postmortem biochemical studies showed the presence of dopamine in the thalamus of macaques ( Brown et al., 1979; Goldman-Rakic and Brown, 1981; Pifl et al., 1990, 1991 ) and human subjects ( Oke and Adams, 1987 ). Later, receptor binding and in situ hybridization analyses detected the presence of dopamine D2-like ( Joyce et al., 1991; Kessler et al., 1993; Hall et al., 1996; Langer et al., 1999;Rieck et al., 2004 ) and D3-like receptors ( Gurevich and Joyce, 1999 ) in several human thalamic nuclei. Positron emission tomography (PET) radioligand studies have also demonstratedthe presence of the dopamine transporter (DAT) ( Wang et al., 1995; Halldin et al., 1996; Helfenbein et al., 1999; Brownell et al., 2003 ) and of D2-like receptors ( Farde et al., 1997; Langer et al., 1999; Okubo et al., 1999; Brownell et al., 2003; Rieck et al., 2004 ) in the human and macaque thalamus. In the course of PET studies focusing on schizophrenia, D2- and D3-like radioligand binding was also found in the thalamus of control subjects ( Talvik et al., 2003; Yasuno et al., 2004 ). Finally, an immunohistochemical study using anti-DAT antibodies detected the presence of dopaminergic www.elsevier.com/locate/ynimg NeuroImage 34 (2007) 965 –984 ⁎Corresponding author. Fax: +34 91 497 53 15. E-mail address: carmen.cavada@uam.es (C. Cavada). Available online on ScienceDirect (www.sciencedirect.com). 1053-8119/$ - see front matter © 2006 Elsevier Inc. All rights reserved. doi:10.1016/j.neuroimage.2006.07.032\"\n",
    "# text_txt = \"\"\n",
    "# keyword_group = ['cat', 'macaque', 'macaca']\n",
    "# keywords_count = count_kw_group_from_text(text_tak, text_txt, keyword_group)\n",
    "# print(keywords_count)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc22165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sent_from_text(text, kw_group):\n",
    "    words = word_tokenize(text) # Tokenize the text\n",
    "    # print(words)\n",
    "    sent_end_punct = ['.', '!', '?'] # Sentence ending punctuations\n",
    "     \n",
    "    sents = \"\"\n",
    "\n",
    "    for keyword in kw_group:\n",
    "        for idx, word in enumerate(words):\n",
    "            # If the keyword is in exact match keyword list, then the keyword must be matched exactly\n",
    "            # If the keyword is not in exact match keyword list, then the keyword can be matched in a word\n",
    "            if (keyword in params.exact_match_kw_list and keyword == word) or (keyword not in params.exact_match_kw_list and keyword in word):\n",
    "                # Extract words before the keyword until stopped by sent_end_punct\n",
    "                before = []\n",
    "                before_idx = idx - 1\n",
    "                while before_idx >= 0 and words[before_idx] not in sent_end_punct:\n",
    "                    before.insert(0, words[before_idx])\n",
    "                    before_idx -= 1\n",
    "                \n",
    "                # Extract words after the keyword until stopped by sent_end_punct\n",
    "                after = []\n",
    "                after_idx = idx + 1\n",
    "                while after_idx < len(words) and words[after_idx] not in sent_end_punct:\n",
    "                    after.append(words[after_idx])\n",
    "                    after_idx += 1\n",
    "                \n",
    "                # Combine before, keyword, and after\n",
    "                extracted = ' '.join(before + [word] + after)\n",
    "                sents = sents + extracted + \". \"\n",
    "    sents = sents.strip()\n",
    "\n",
    "    return sents\n",
    "# --------------------Start of test code--------------------\n",
    "# text = \"Effect of Attentive CAT cato Fixation in Macaqueq tThalamis and Cortex. Effect of Attentive CAT cato Fixation in Macaque Thalamus and Cortex.\"\n",
    "# text = plib.process_text(text, lower=True)\n",
    "# kw_group = ['macaque', 'thalam']\n",
    "# sents = extract_sent_from_text(text, kw_group)\n",
    "# print(sents)\n",
    "# ---------------------End of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e749e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sents_and_record(train_test_1000_path, db_path, db_text_extract_path):\n",
    "    df_1000 = pd.read_csv(train_test_1000_path, header=0, sep=',')\n",
    "    df_1000.columns = df_col.train_test_1000_path_columns\n",
    "\n",
    "    df_db = pd.read_csv(db_path, header=None, sep=',')\n",
    "    df_db.columns = df_col.db_columns\n",
    "\n",
    "    for ind in df_1000.index:\n",
    "        index = int(df_1000.at[ind, \"INDEX\"])\n",
    "        # print(\"ind:\", ind, \"index:\", index)\n",
    "\n",
    "        # get the text from the full text\n",
    "        txt_file_name = str(index) + \".txt\"\n",
    "        txt_path = os.path.join(fpath.text_folder, txt_file_name)\n",
    "\n",
    "        # extract text_tak and text_500\n",
    "        if os.path.exists(txt_path): # text from full text\n",
    "            with open(txt_path, 'r', encoding='ascii') as f:\n",
    "                text_txt = f.read()\n",
    "            f.close()\n",
    "        else:\n",
    "            text_txt = \"\"\n",
    "\n",
    "        text_tak = \"\" # text from title, abstract, and keywords\n",
    "        if df_1000.at[ind, \"TITLE\"] == df_1000.at[ind, \"TITLE\"]:\n",
    "            text_tak = text_tak + df_1000.at[ind, \"TITLE\"] + \" \"\n",
    "        else:\n",
    "            pass\n",
    "        if df_1000.at[ind, \"ABSTRACT\"] == df_1000.at[ind, \"ABSTRACT\"]:\n",
    "            text_tak = text_tak + df_1000.at[ind, \"ABSTRACT\"] + \" \"\n",
    "        else:\n",
    "            pass\n",
    "        if df_1000.at[ind, \"KEYWORDS\"] == df_1000.at[ind, \"KEYWORDS\"]:\n",
    "            text_tak = text_tak + df_1000.at[ind, \"KEYWORDS\"] + \" \"\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # process the text\n",
    "        text_txt = plib.process_text(text_txt, lower=True)\n",
    "        text_tak = plib.process_text(text_tak, lower=True)\n",
    "        text_500 = ' '.join([word for word in text_txt.split()[:params.text_length_to_extract]])\n",
    "\n",
    "        # print(text_tak)\n",
    "        # print(text_500)\n",
    "        \n",
    "        # count keywords from text\n",
    "        text_column = params.text_column_to_add\n",
    "        # print(text_column)\n",
    "        count_list = [0] * len(params.ranking_kw_groups.keys())\n",
    "        keys_list = list(params.ranking_kw_groups.keys())\n",
    "        for i in range(len(count_list)):\n",
    "            count_list[i] = count_kw_group_from_text(text_tak, text_500, params.ranking_kw_groups[keys_list[i]])\n",
    "        # print(count_list)\n",
    "        # exatrct columns we want to add\n",
    "        text_column_count_list = []\n",
    "        for i in range(len(text_column)):\n",
    "            # get the index of the text_column[i] in keys_list\n",
    "            j = keys_list.index(text_column[i])\n",
    "            text_column_count_list.append(count_list[j])\n",
    "        # print(text_column_count_list)\n",
    "\n",
    "        # extract sentences from text\n",
    "        if text_txt == text_txt and text_txt != \"\": # if full text is available, use full text\n",
    "            text = text_txt\n",
    "        else: # otherwise, use tak (title + abstract + keywords)\n",
    "            text = text_tak\n",
    "\n",
    "        text_list = []\n",
    "        for key in params.ranking_kw_groups.keys():\n",
    "            sents = extract_sent_from_text(text, params.ranking_kw_groups[key])\n",
    "            text_list.append(sents)\n",
    "        \n",
    "        text_column_text_list = []\n",
    "        for i in range(len(text_column)):\n",
    "            j = keys_list.index(text_column[i])\n",
    "            text_column_text_list.append(text_list[j])\n",
    "        # print(text_column_text_list)\n",
    "        \n",
    "        # csv columns\n",
    "        columns = [\n",
    "            \"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"TITLTE\", \"FULL_TEXT_URL\", \"PDF_URL\", \n",
    "            \"TEXT_TAK\", \"TEXT_500\"\n",
    "        ]\n",
    "        columns = columns + [key+\"_TEXT\" for key in text_column] # add keyword group text\n",
    "        columns = columns + [key+\"_COUNT\" for key in text_column] # add keyword group count\n",
    "        columns_to_add = [\"TT?(Y/N/MB/NA)\", \"MACAQUE?(Y/N/MB/NA)\", \"THALAMUS?(Y/N/MB/NA)\", \"RELEVANT(Y/N/MB/NA)\", \"READ_BY(A/D/R)\", \"COMMENT\"] # add columns for documenting labels\n",
    "        columns = columns + columns_to_add\n",
    "\n",
    "        # specify rows\n",
    "        row = {\n",
    "            \"INDEX\": [df_1000.at[ind, \"INDEX\"]],\n",
    "            \"DOI\": [df_1000.at[ind, \"DOI\"]],\n",
    "            \"PMID\": [df_1000.at[ind, \"PMID\"]],\n",
    "            \"PMCID\": [df_1000.at[ind, \"PMCID\"]],\n",
    "            \"TITLTE\": [df_1000.at[ind, \"TITLE\"]],\n",
    "            \"FULL_TEXT_URL\": [df_1000.at[ind, \"FULL_TEXT_URL\"]],\n",
    "            \"PDF_URL\": [df_1000.at[ind, \"PDF_URL\"]],\n",
    "            \"TEXT_TAK\": [text_tak],\n",
    "            \"TEXT_500\": [text_500]\n",
    "        }\n",
    "\n",
    "        i = 0\n",
    "        for key in text_column: # add key value pair of ranking_kw_groups and values in text_group\n",
    "            text_value = text_column_text_list[i]\n",
    "            row[key+\"_TEXT\"] = [text_value]\n",
    "            count_value = text_column_count_list[i]\n",
    "            row[key+\"_COUNT\"] = [count_value]\n",
    "            i += 1\n",
    "\n",
    "        for column in columns_to_add:\n",
    "            row[column] = [np.nan]\n",
    "        # print(row)\n",
    "\n",
    "        # save to csv\n",
    "        if ind == 0: # add the first row with header\n",
    "            df_new_row = pd.DataFrame(data=row, columns=columns)\n",
    "            df_new_row.to_csv(db_text_extract_path, mode='w', index=False, header=True, escapechar='\\\\')\n",
    "            continue\n",
    "\n",
    "        if not plib.add_row_to_csv(db_text_extract_path, row, columns):  # add the rest rows without header and with mode \"a\"\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(\"ind:\", ind, \"index:\", index)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e08f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_and_record(db_path, db_kw_count, ranking_kw_groups):\n",
    "    df = pd.read_csv(db_path, header=None, sep=\",\")\n",
    "    df.columns = df_col.db_columns\n",
    "\n",
    "    for ind in df.index:\n",
    "        index = int(df.at[ind, \"INDEX\"])\n",
    "        txt_file_name = str(index) + \".txt\"\n",
    "        txt_path = os.path.join(fpath.text_folder, txt_file_name)\n",
    "        \n",
    "        # extract text\n",
    "        text_tak = \"\" # text from title, abstract, and keywords\n",
    "        text_txt = \"\" # text from full text\n",
    "        # from title, abstract, and keywords\n",
    "        if df.at[ind, \"TITLE\"] == df.at[ind, \"TITLE\"]:\n",
    "            text_tak = text_tak + \" \" + df.at[ind, \"TITLE\"]\n",
    "        else:\n",
    "            pass  \n",
    "        if df.at[ind, \"ABSTRACT\"] == df.at[ind, \"ABSTRACT\"]:\n",
    "            text_tak = text_tak + \" \" + df.at[ind, \"ABSTRACT\"]\n",
    "        else:\n",
    "            pass\n",
    "        if df.at[ind, \"KEYWORDS\"] == df.at[ind, \"KEYWORDS\"]:\n",
    "            text_tak = text_tak + \" \" + df.at[ind, \"KEYWORDS\"]\n",
    "        else:\n",
    "            pass\n",
    "        # from full text\n",
    "        if os.path.exists(txt_path):\n",
    "            with open(txt_path, \"r\", encoding='ascii') as f:\n",
    "                text_txt = f.read()\n",
    "            f.close()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # extract first 500 words from text_txt and text_tak, if they are longer than 500 words\n",
    "        # if they are shorter than 500 words, expand them to 500 words by repeating\n",
    "        text_tak = plib.process_text(text_tak, lower=True)\n",
    "        while len(text_tak.split()) < params.text_length_to_extract:\n",
    "            text_tak = text_tak + \" \" + text_tak\n",
    "        text_tak = ' '.join(text_tak.split()[:params.text_length_to_extract])\n",
    "        text_txt = plib.process_text(text_txt, lower=True)\n",
    "        # print(text_tak)\n",
    "        # print(len(text_tak.split()))\n",
    "\n",
    "        text_txt = plib.process_text(text_txt, lower=True)\n",
    "        while len(text_txt.split()) < params.text_length_to_extract:\n",
    "            text_txt = text_txt + \" \" + text_txt\n",
    "        text_txt = ' '.join(text_txt.split()[:params.text_length_to_extract])\n",
    "        text_txt = plib.process_text(text_txt, lower=True)\n",
    "        # print(text_txt)\n",
    "        # print(len(text_txt.split()))\n",
    "\n",
    "        # count keywords from text\n",
    "        count_list = [0] * len(ranking_kw_groups)\n",
    "        keys_list = list(ranking_kw_groups.keys())\n",
    "        for i in range(len(count_list)):\n",
    "            count_list[i] = count_kw_group_from_text(text_tak, text_txt, ranking_kw_groups[keys_list[i]])\n",
    "        # print(count_list)\n",
    "\n",
    "        columns = df_col.db_ranked_columns\n",
    "\n",
    "        # specify rows\n",
    "        row = {\n",
    "            \"INDEX\": [df.at[ind, \"INDEX\"]],\n",
    "            \"DOI\": [df.at[ind, \"DOI\"]],\n",
    "            \"PMID\": [df.at[ind, \"PMID\"]],\n",
    "            \"PMCID\": [df.at[ind, \"PMCID\"]],\n",
    "            \"TITLE\": [df.at[ind, \"TITLE\"]]\n",
    "        }\n",
    "        # add key, value pair of keyword group counts\n",
    "        i = 0\n",
    "        for key in ranking_kw_groups.keys():\n",
    "            value = count_list[i]\n",
    "            row[key+\"_COUNT\"] = [value]\n",
    "            i += 1\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(db_kw_count, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        line_number_in_csv = ind + 1\n",
    "        print(\"Line number:\", line_number_in_csv, \" INDEX:\", int(df.at[ind, \"INDEX\"]))\n",
    "# --------------------start of test code--------------------\n",
    "# input_path = fpath.poten_litera_db\n",
    "# output_path = fpath.poten_litera_db_kw_count\n",
    "# count_and_record(input_path, output_path, params.ranking_kw_groups)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02fb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul_index(counts_dict, ranking_params_weights):\n",
    "    for key in counts_dict.keys():\n",
    "        index += math.log(1 + counts_dict[key]) * (ranking_params_weights[key])\n",
    "    return index\n",
    "# --------------------start of test code--------------------\n",
    "# keywords_count_or_fre = {}\n",
    "# index = calcul_related(keywords_count_or_fre, params.on_topic_kws_weights)\n",
    "# print(index)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edde53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(db_path, db_ranked_path, ranking_params_weights):\n",
    "    df = pd.read_csv(db_path, header=None, sep=\",\")\n",
    "    df.columns = df_col.db_columns\n",
    "    \n",
    "    for ind in df.index:\n",
    "        count_dict = {}\n",
    "        for key in params.ranking_kw_groups.keys():\n",
    "            count_dict[ind, key+\"_COUNT\"] = int(df.at[ind, key+\"_COUNT\"])\n",
    "        print(count_dict)\n",
    "\n",
    "        relev_index = calcul_index(count_dict, ranking_params_weights)\n",
    "        \n",
    "        # csv column names\n",
    "        columns = df_col.db_ranked_columns\n",
    "\n",
    "        # specify rows\n",
    "        row = {\n",
    "            \"INDEX\": [df.at[ind, \"INDEX\"]],\n",
    "            \"DOI\": [df.at[ind, \"DOI\"]],\n",
    "            \"PMID\": [df.at[ind, \"PMID\"]],\n",
    "            \"PMCID\": [df.at[ind, \"PMCID\"]],\n",
    "            \"TITLE\": [df.at[ind, \"TITLE\"]],\n",
    "        }\n",
    "        # merge dicts row and count_dict into one\n",
    "        row = {**row, **count_dict}\n",
    "        row[\"RELEVANCE_INDEX\"] = [relev_index]\n",
    "        print(row)\n",
    "\n",
    "        # save to csv\n",
    "        if not plib.add_row_to_csv(db_ranked_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        line_number_in_csv = ind + 1\n",
    "        print(\"Line number:\", line_number_in_csv, \" INDEX:\", int(df.at[ind, \"INDEX\"]))\n",
    "    \n",
    "    # sort and save to csv\n",
    "    df_to_rank = df.read_csv(db_ranked_path, header=None, sep=\",\")\n",
    "    df_to_rank.columns = df_col.db_ranked_columns\n",
    "\n",
    "    df_ranked = df_to_rank.sort_values(by=\"RELEVANCE_INDEX\", ascending=False)\n",
    "    df_ranked.reset_index(drop=True, inplace=True)\n",
    "    df_ranked.to_csv(db_ranked_path, header=True, index=False)\n",
    "    print(\"Weighting and ranking the potentially related literature succeded!\")\n",
    "    print(\"Enjoy reading!\")\n",
    "# --------------------start of test code--------------------\n",
    "# input_path = fpath.poten_litera_db_kw_count\n",
    "# output_path = fpath.poten_litera_db_ranked\n",
    "# rank(input_path, output_path, params.ranking_params_weights)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e562971",
   "metadata": {},
   "source": [
    "<h3> Main program: </h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b3dd0",
   "metadata": {},
   "source": [
    "#### 1. Data processing and candidate articles ranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67041512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of text length in the title + keywords + abstract\n",
    "input_path = fpath.poten_litera_db\n",
    "df = pd.read_csv(input_path, header=None, sep=',')\n",
    "df.columns = [\n",
    "    \"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \n",
    "    \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"\n",
    "    ]\n",
    "\n",
    "len_list = []\n",
    "\n",
    "for ind in df.index:\n",
    "    text = \"\"\n",
    "    num_words = 0\n",
    "\n",
    "    if df.at[ind, \"ABSTRACT\"] == df.at[ind, \"ABSTRACT\"]: # if abstract is available\n",
    "        abstract = df.at[ind, \"ABSTRACT\"]\n",
    "    else: # skip this article if abstract is not available\n",
    "        continue\n",
    "    if df.at[ind, \"TITLE\"] == df.at[ind, \"TITLE\"]:\n",
    "        title = df.at[ind, \"TITLE\"]\n",
    "    else:\n",
    "        title = \"\"\n",
    "    if df.at[ind, \"KEYWORDS\"] == df.at[ind, \"KEYWORDS\"]:\n",
    "        keywords = df.at[ind, \"KEYWORDS\"]\n",
    "    else:\n",
    "        keywords = \"\"\n",
    "\n",
    "    text = title + \" \" + abstract + \" \" + keywords\n",
    "    \n",
    "    # process the text\n",
    "    text = plib.process_text(text, lower=True)\n",
    "\n",
    "    num_words = len(text.split())\n",
    "\n",
    "    len_list.append(num_words)\n",
    "\n",
    "# calculate the average and maximum number of words in the text\n",
    "print(\"The number of articles considered:\", len(len_list))\n",
    "print(\"Max of length:\", max(len_list))\n",
    "print(\"Average length:\", np.mean(len_list))\n",
    "print(\"Median length:\", np.median(len_list))\n",
    "print(\"Std of length:\", np.std(len_list))\n",
    "\n",
    "# sort the len_list and draw a histogram\n",
    "len_list.sort()\n",
    "plt.hist(len_list, bins=20)\n",
    "plt.xlabel(\"Text legth of tak (title + abstract + keywords) text\")\n",
    "plt.ylabel(\"Number of articles\")\n",
    "plt.show()\n",
    "# The number of articles considered: 9892\n",
    "# Max of length: 1307\n",
    "# Average length: 235.13121714516782\n",
    "# Median length: 230.0\n",
    "# Std of length: 79.72543466983504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77873c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the train_test_1000 set, extract sentences and counts, save to poten_litera_testing_set_1000_text_extract_and_count\n",
    "input_path = fpath.poten_litera_testing_set_1000\n",
    "db_path = fpath.poten_litera_db\n",
    "output_path = fpath.poten_litera_testing_set_1000_text_extract_and_count\n",
    "# clear the file\n",
    "plib.clear_file(output_path)\n",
    "\n",
    "# extract_sents_and_record(input_path, db_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5f7f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read the db_final.csv and scan the rows and count the number of np.nans in the columns Macaque?(Y/N), Thalamus?(Y/N), Inject?(Y/N)\n",
    "# input_path = fpath.poten_litera_db_text_extract\n",
    "# df = pd.read_csv(input_path, header=0, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \n",
    "#                \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \n",
    "#                \"SPIECIES_TEXT\", \"THALAM_TEXT\", \"INJECT_TEXT\", \n",
    "#                \"Macaque?(Y/N)\", \"Thalamus?(Y/N)\", \"Inject?(Y/N)\"]\n",
    "\n",
    "# macaque = 0\n",
    "# thalamus = 0\n",
    "# inject = 0\n",
    "\n",
    "# for ind in df.index:\n",
    "#     if df.at[ind, \"SPIECIES_TEXT\"] != df.at[ind, \"SPIECIES_TEXT\"]:\n",
    "#         macaque += 1\n",
    "#         print(\"No macaque in text!\")\n",
    "#         print(df.at[ind, \"INDEX\"])\n",
    "#         print(df.at[ind, \"FULL_TEXT_URL\"])\n",
    "#         print(\"\\n\")\n",
    "#     if df.at[ind, \"THALAM_TEXT\"] != df.at[ind, \"THALAM_TEXT\"]:\n",
    "#         thalamus += 1\n",
    "#         print(\"No thalamus in text!\")\n",
    "#         print(df.at[ind, \"INDEX\"])\n",
    "#         print(df.at[ind, \"FULL_TEXT_URL\"])\n",
    "#         print(\"\\n\")\n",
    "#     if df.at[ind, \"INJECT_TEXT\"] != df.at[ind, \"INJECT_TEXT\"]:\n",
    "#         inject += 1\n",
    "\n",
    "# print(\"Macaque:\", macaque)\n",
    "# print(\"Thalamus:\", thalamus)\n",
    "# print(\"Inject:\", inject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9493b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read from poten_litera_db, count keywords, save to poten_litera_db_kw_count\n",
    "# input_path = fpath.poten_litera_db\n",
    "# output_path = fpath.poten_litera_db_kw_count\n",
    "\n",
    "# # clear file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# count_and_record(input_path, output_path, params.ranking_kw_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bef318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from poten_litera_db_kw_count, rank the candidate articles, save to poten_litera_db_ranked\n",
    "input_path = fpath.poten_litera_db_kw_count\n",
    "output_path = fpath.poten_litera_db_ranked\n",
    "\n",
    "# clear file\n",
    "plib.clear_file(output_path)\n",
    "\n",
    "rank(input_path, output_path, params.ranking_kw_groups_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36685b74",
   "metadata": {},
   "source": [
    "#### 2. Ranking results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e384b1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read the ranked database and obtain the relevance_index of YESs and NOs \n",
    "# # of the test data set and draw a violin plot, and calculate the difference between the two distributions\n",
    "# # the difference is defined as 1. t-statistic 2. \n",
    "# db_ranked_path = fpath.poten_litera_db_ranked\n",
    "# test_path = fpath.poten_litera_testing_set_1000_read\n",
    "\n",
    "# df_db_ranked = pd.read_csv(db_ranked_path, header=0, sep=',')\n",
    "# df_db_ranked.columns = [\n",
    "#     \"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \n",
    "#     \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \n",
    "#     \"SPECIES_RELATED\", \"TC_CT_RELATED\", \"THALAM_RELATED\", \"CORTEX_RELATED\", \"METHOD_RELATED\", \"CONNECTIVITY_RELATED\",\n",
    "#     \"RELEVANCE_INDEX\"]\n",
    "\n",
    "# df_test = pd.read_csv(test_path, header=0, sep=',')\n",
    "# df_test.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa55d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the counts of the keywords in the respective lists\n",
    "# relevant_species = []\n",
    "# relevant_tc_ct = []\n",
    "# relevant_thalam = []\n",
    "# relevant_cortex = []\n",
    "# relevant_method = []\n",
    "# relevant_connectivity = []\n",
    "\n",
    "# non_relevant_species = []\n",
    "# non_relevant_tc_ct = []\n",
    "# non_relevant_thalam = []\n",
    "# non_relevant_cortex = []\n",
    "# non_relevant_method = []\n",
    "# non_relevant_connectivity = []\n",
    "\n",
    "# relvant_index = []\n",
    "# relevant_relevance_index_list = []\n",
    "\n",
    "# non_relevant_index = []\n",
    "# non_relevant_relevance_index_list = []\n",
    "\n",
    "# for ind in df_test.index:\n",
    "#     index = int(df_test.at[ind, \"INDEX\"])\n",
    "#     # print(ind, index)\n",
    "#     # print(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "\n",
    "#     # if df_test.at[ind, \"RELEVANT\"] == \"YES\" and df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"LENGTH_TEXT\"].values[0] > 100:\n",
    "#     if df_test.at[ind, \"RELEVANT\"] == \"YES\":\n",
    "#         relvant_index.append(index)\n",
    "#         relevant_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "#         relevant_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"SPECIES_RELATED\"].values[0])\n",
    "#         relevant_tc_ct.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"TC_CT_RELATED\"].values[0])\n",
    "#         relevant_thalam.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"THALAM_RELATED\"].values[0])\n",
    "#         relevant_cortex.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CORTEX_RELATED\"].values[0])\n",
    "#         relevant_method.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"METHOD_RELATED\"].values[0])\n",
    "#         relevant_connectivity.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CONNECTIVITY_RELATED\"].values[0])\n",
    "#     # elif df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"LENGTH_TEXT\"].values[0] > 100:\n",
    "#     else:\n",
    "#         non_relevant_index.append(index)\n",
    "#         non_relevant_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "#         non_relevant_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"SPECIES_RELATED\"].values[0])\n",
    "#         non_relevant_tc_ct.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"TC_CT_RELATED\"].values[0])\n",
    "#         non_relevant_thalam.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"THALAM_RELATED\"].values[0])\n",
    "#         non_relevant_cortex.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CORTEX_RELATED\"].values[0])\n",
    "#         non_relevant_method.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"METHOD_RELATED\"].values[0])\n",
    "#         non_relevant_connectivity.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CONNECTIVITY_RELATED\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1821b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(relvant_index)):\n",
    "#     print(relvant_index[i], relevant_relevance_index_list[i])\n",
    "#     # print(relevant_relevance_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d5dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the dot plot of the relevance_index of YESs and NOs of the test data set\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(relvant_index, relevant_relevance_index_list, 'ro', label=\"YES\")\n",
    "# plt.plot(non_relevant_index, non_relevant_relevance_index_list, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Relevance Index\")\n",
    "# plt.legend()\n",
    "\n",
    "# # add labels for the relevant index points\n",
    "# for i, index in enumerate(relvant_index):\n",
    "#     plt.text(index, relevant_relevance_index_list[i]+0.1, str(index), color='black', fontsize=10)\n",
    "\n",
    "# # # add labels for the relevant index points\n",
    "# # for i, index in enumerate(non_relevant_index):\n",
    "# #     plt.text(index, non_relevant_relevance_index_list[i], str(index), color='black', fontsize=10)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed550a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the 6 dot plots of the species_related, tc_ct_related, thalam_related, cortex_related, method_related, connectivity_related of YESs and NOs of the test data set in 2 rows in the same figure\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.subplot(3, 2, 1)\n",
    "# plt.plot(relvant_index, relevant_species, 'ro', label=\"YES\")\n",
    "# plt.plot(non_relevant_index, non_relevant_species, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Species Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(3, 2, 2)\n",
    "# plt.plot(relvant_index, relevant_tc_ct, 'ro', label=\"YES\")\n",
    "# plt.plot(non_relevant_index, non_relevant_tc_ct, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"TC_CT Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(3, 2, 3)\n",
    "# plt.plot(relvant_index, relevant_thalam, 'ro', label=\"YES\")\n",
    "# plt.plot(non_relevant_index, non_relevant_thalam, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Thalam Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(3, 2, 4)\n",
    "# plt.plot(relvant_index, relevant_cortex, 'ro', label=\"YES\")\n",
    "# plt.plot(non_relevant_index, non_relevant_cortex, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Cortex Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(3, 2, 5)\n",
    "# plt.plot(relvant_index, relevant_method, 'ro', label=\"YES\")\n",
    "# plt.plot(non_relevant_index, non_relevant_method, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Method Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(3, 2, 6)\n",
    "# plt.plot(relvant_index, relevant_connectivity, 'ro', label=\"YES\")\n",
    "# plt.plot(non_relevant_index, non_relevant_connectivity, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Connectivity Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6260d21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pick_values_uniformly(data, n):\n",
    "#     \"\"\"Pick up `n` values uniformly from `data`.\"\"\"\n",
    "#     if n <= 0:\n",
    "#         return []\n",
    "\n",
    "#     # Determine the range of the data\n",
    "#     min_val, max_val = min(data), max(data)\n",
    "\n",
    "#     threshold = (max_val - min_val) / n / 2\n",
    "\n",
    "#     # If n is 1, just return the midpoint\n",
    "#     if n == 1:\n",
    "#         return [(min_val + max_val) / 2]\n",
    "\n",
    "#     # Calculate the interval size\n",
    "#     interval = (max_val - min_val) / (n - 1)\n",
    "\n",
    "#     # Get the uniform values\n",
    "#     return [min_val + i * interval for i in range(n)], threshold\n",
    "\n",
    "# # data = [1, 3, 5, 2, 8, 10, 2]\n",
    "# n = 5\n",
    "# density_display_index, thres = pick_values_uniformly(relevant_relevance_index_list + non_relevant_relevance_index_list, n)\n",
    "# print(density_display_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73da3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Printing the length of lists\n",
    "# print(\"Numer of relevant literature:\", len(relevant_relevance_index_list))\n",
    "# print(\"Number of not relevant literature:\", len(non_relevant_relevance_index_list))\n",
    "# print()\n",
    "\n",
    "# # Create a DataFrame for plotting\n",
    "# df = pd.DataFrame({'Relevance Index': relevant_relevance_index_list + non_relevant_relevance_index_list, \n",
    "#                    'Label': ['Relevant'] * len(relevant_relevance_index_list) + ['Not Relevant'] * len(non_relevant_relevance_index_list)})\n",
    "\n",
    "# # Draw the violin plot\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# ax = sns.violinplot(x='Label', y='Relevance Index', data=df, bw='scott', cut=0)\n",
    "\n",
    "# relevance_indices = density_display_index  # Replace with your relevance indices\n",
    "\n",
    "# threshold = thres  # Adjust this based on your desired range around the relevance index\n",
    "\n",
    "# for index in relevance_indices:\n",
    "#     ax.axhline(index, color='gray', linestyle='--')\n",
    "    \n",
    "#     for i, label in enumerate(df['Label'].unique()):\n",
    "#         # Filter data points close to the current relevance index\n",
    "#         close_points = df[(df['Label'] == label) & (np.abs(df['Relevance Index'] - index) < threshold)]\n",
    "#         density = len(close_points)\n",
    "        \n",
    "#         ax.text(i, index + 0.1, str(density), ha='center', va='center', color='red', fontsize=9)  # adjust the vertical offset (0.1 here) as necessary\n",
    "\n",
    "# plt.title('Distribution of Relevance Index')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.stats as stats\n",
    "\n",
    "# # Check the equality of variances\n",
    "# var_relevant = np.var(relevant_relevance_index_list)\n",
    "# var_non_relevant = np.var(non_relevant_relevance_index_list)\n",
    "# print('Variance of relevant:', var_relevant)\n",
    "# print('Variance of non-relevant:', var_non_relevant)\n",
    "# print(var_relevant/var_non_relevant)\n",
    "# # statistic, p_value = stats.levene(relevant_relevance_index_list, non_relevant_relevance_index_list)\n",
    "\n",
    "# # # Print the results\n",
    "# # print('Levene test statistic:', statistic)\n",
    "# # print('p-value:', p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0804ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the t-statistic and p-value\n",
    "# t_statistic, p_value = stats.ttest_ind(relevant_relevance_index_list, non_relevant_relevance_index_list)\n",
    "\n",
    "# # Print the results\n",
    "# print('t-statistic:', t_statistic)\n",
    "# print('p-value:', p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc9ccf",
   "metadata": {},
   "source": [
    "<h3> Next step: manually read papers and find all actually related literature </h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
