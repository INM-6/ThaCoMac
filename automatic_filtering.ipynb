{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea1cce5-4f07-4bd7-8ca3-5f6fa51254d0",
   "metadata": {},
   "source": [
    "<h2> Automatic filtering </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e51706-2334-49bb-8c1e-4939b52c60b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import internal modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib\n",
    "import extract_info\n",
    "import parameters as params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad5a61c5-f8c6-418a-b450-cdea0378ddab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489f0451",
   "metadata": {},
   "source": [
    "<h3> Predefined fucntions: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93f93d7d-bbbc-4afe-94f7-f12cca267a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_filling(input_path, output_path, start, end):\n",
    "    # scan each row in the potential related literature and extract information\n",
    "    df = pd.read_csv(input_path, header=None, sep=\",\")\n",
    "    df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # info = {\n",
    "        #     \"doi\": np.nan,\n",
    "        #     \"pmid\": np.nan,\n",
    "        #     \"pmcid\": np.nan,\n",
    "        #     \"title\": np.nan,\n",
    "        #     \"abstract\": np.nan,\n",
    "        #     \"keywords\": np.nan,\n",
    "        #     \"pdf_link\": np.nan\n",
    "        # }\n",
    "\n",
    "        # initialzie\n",
    "        index = df.at[ind, \"INDEX\"]\n",
    "        doi = df.at[ind, \"DOI\"]\n",
    "        pmid = df.at[ind, \"PMID\"]\n",
    "        pmcid = df.at[ind, \"PMCID\"]\n",
    "        full_text_url = df.at[ind, \"FULL_TEXT_URL\"]\n",
    "        full_text_source = df.at[ind, \"FULL_TEXT_SOURCE\"]\n",
    "        pdf_url = df.at[ind, \"PDF_URL\"]\n",
    "        # pdf_source = df.at[ind, \"PDF_SOURCE\"]\n",
    "        title = df.at[ind, \"TITLE\"]\n",
    "        abstract = df.at[ind, \"ABSTRACT\"]\n",
    "        keywords = df.at[ind, \"KEYWORDS\"]\n",
    "\n",
    "        if full_text_url != full_text_url: # full text url not found\n",
    "            if pdf_url == pdf_url:\n",
    "                url = str(pdf_url).strip()\n",
    "                try:\n",
    "                    url1, status_code = plib.get_final_redirected_url(url)\n",
    "                    if status_code == 200:\n",
    "                        pdf_url = url\n",
    "                        pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                    elif status_code == 403:\n",
    "                        print(status_code, \"when getting final redirected url: \", url)\n",
    "                        pdf_url = url\n",
    "                        pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                    else:\n",
    "                        print(status_code, \"when getting final redirected url: \", url)\n",
    "                        continue\n",
    "                except:\n",
    "                    continue    \n",
    "            else:\n",
    "                print(\"full text url and pdf url are not available!\")\n",
    "                continue\n",
    "        else: # full text url found\n",
    "            \n",
    "            # try:\n",
    "            #     full_text_url, status_code = plib.get_final_redirected_url(full_text_url)\n",
    "            # if status_code == 200 or status_code == 403:\n",
    "            #     full_text_url = full_text_url\n",
    "            #     full_text_source = full_text_url.split(\"://\")[1].split(\"/\")[0]\n",
    "            # elif doi == doi:\n",
    "            #     doi_url = \"https://doi.org/\" + str(doi).strip()\n",
    "            #     full_text_url, status_code = plib.get_final_redirected_url(doi_url)\n",
    "            \n",
    "            # check if the full_text_link is one of our websites\n",
    "            flag = False\n",
    "            for website in params.websites:\n",
    "                if website in df.at[ind, \"FULL_TEXT_SOURCE\"]:\n",
    "                    flag = True\n",
    "                    break\n",
    "            if not flag:\n",
    "                continue\n",
    "\n",
    "            url = str(full_text_url).strip()\n",
    "\n",
    "            try:\n",
    "                info = extract_info.extract_info_from_webpage(url, params.websites)\n",
    "            except:\n",
    "                raise Exception(\"Error! Cannot extract information from the webpage: \" + url)\n",
    "            \n",
    "            # doi\n",
    "            if info['doi'] == info['doi'] and doi == doi and info['doi'] != doi:\n",
    "                print(doi)\n",
    "                print(info['doi'])\n",
    "\n",
    "            if info['doi'] == info['doi']:\n",
    "                doi = info['doi'].lower()\n",
    "            else:\n",
    "                doi = doi\n",
    "            \n",
    "            # pmid\n",
    "            if info['pmid'] == info['pmid'] and df.at[ind, \"PMID\"] == df.at[ind, \"PMID\"] and str(int(info['pmid'])) != str(int(df.at[ind, \"PMID\"])):\n",
    "                print(str(int(df.at[ind, \"PMID\"]))) \n",
    "                print(str(int(info['pmid'])))      \n",
    "\n",
    "            if info['pmid'] == info['pmid']:\n",
    "                pmid = str(int(info['pmid']))\n",
    "            elif pmid == pmid:\n",
    "                pmid = str(int(pmid)).strip()\n",
    "            else:\n",
    "                pmid = np.nan\n",
    "            \n",
    "            # pmcid\n",
    "            if info['pmcid'] == info['pmcid'] and pmcid == pmcid and info['pmcid'] != pmcid:\n",
    "                print(pmcid)\n",
    "                print(info['pmcid'])\n",
    "\n",
    "            if info['pmcid'] == info['pmcid']:\n",
    "                pmcid = info['pmcid']\n",
    "            else:\n",
    "                pmcid = df.at[ind, \"PMCID\"]\n",
    "            \n",
    "            # full_text_url, full_text_surce\n",
    "            if full_text_url == full_text_url:\n",
    "                full_text_source = full_text_url.split(\"://\")[1].split(\"/\")[0]\n",
    "            else:\n",
    "                print(\"full text url is not available\")\n",
    "                full_text_url = np.nan\n",
    "                full_text_source = np.nan\n",
    "                # raise Exception(\"Error! Full text link is not available!\")\n",
    "            \n",
    "            # pdf_url, pdf_source\n",
    "            if info['pdf_link'] == info['pdf_link']:\n",
    "                pdf_url = info['pdf_link']\n",
    "                pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "            elif pdf_url == pdf_url:\n",
    "                url = pdf_url\n",
    "                source = url.split(\"://\")[1].split(\"/\")[0]\n",
    "                if \"elsevier\" in source and doi == doi:\n",
    "                    pdf_url = doi.lower()\n",
    "                    pdf_source = \"ELSEVIER\"\n",
    "                elif \"springer\" in source and full_text_url != full_text_url:\n",
    "                    pdf_url = full_text_url\n",
    "                    pdf_source = \"SPRINGER\"\n",
    "                else:\n",
    "                    print(\"PDF_URL not extracted from info: , but existed already: \", pdf_url)\n",
    "                    pass\n",
    "            else:\n",
    "                print(\"PDF_URL not found for: \", full_text_url)\n",
    "                pdf_url = np.nan\n",
    "                pdf_source = np.nan\n",
    "                # raise Exception(\"Error! pdf link is not available!\")\n",
    "                \n",
    "            # title\n",
    "            if info['title'] == info['title']:\n",
    "                title = info['title']\n",
    "                title = title.replace(\";\", \",\")\n",
    "            else:\n",
    "                title = title\n",
    "            \n",
    "            # abstract\n",
    "            if info['abstract'] == info['abstract']:\n",
    "                abstract = info['abstract']\n",
    "                abstract = ''.join(e for e in abstract if (e.isalpha() or e == \" \" or e == \"-\"))\n",
    "            else:\n",
    "                # print(\"Warning! Abstract is not available!\", full_text_link)\n",
    "                abstract = abstract\n",
    "            \n",
    "            # keywords\n",
    "            if info['keywords'] == info['keywords']:\n",
    "                keywords = info['keywords']\n",
    "                keywords = keywords.replace(\";\", \",\")\n",
    "                keywords = ''.join(e for e in keywords if (e.isalpha() or e == \" \" or e == \"-\" or e == \",\"))\n",
    "            else:\n",
    "                keywords = keywords\n",
    "        \n",
    "        columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "        \n",
    "        row = {\n",
    "            \"INDEX\": [index],\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"FULL_TEXT_URL\": [full_text_url],\n",
    "            \"FULL_TEXT_SOURCE\": [full_text_source],\n",
    "            \"PDF_URL\": [pdf_url],\n",
    "            \"PDF_SOURCE\": [pdf_source],\n",
    "            \"TITLE\": [title],\n",
    "            \"ABSTRACT\": [abstract],\n",
    "            \"KEYWORDS\": [keywords]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# input_path = fpath.poten_litera_ids_ftl_filled\n",
    "# output_path = fpath.poten_litera_litera_db\n",
    "\n",
    "# # clear file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# info_filling(input_path, output_path)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce32f7b8-96b7-4f5c-a2da-3ff931cdeaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_keyword(text, keyword, keyword_length):\n",
    "    # remove non-alphabetic characters but keep spaces and \"-\"\n",
    "    text = ''.join(e for e in text if (e.isalpha() or e == \" \" or e == \"-\"))\n",
    "    # print(text)\n",
    "    text = text.strip().lower()\n",
    "    # print(text)\n",
    "    \n",
    "    words = []\n",
    "    # sentence = 'I have a laptop case and a laptop bag'\n",
    "    n = keyword_length\n",
    "    n_grams = ngrams(text.split(), n)\n",
    "    for gram in n_grams:\n",
    "        word = gram[0]\n",
    "        if n > 0:\n",
    "            for i in range(1, n):\n",
    "                word = word + \" \" + gram[i]\n",
    "        words.append(word)\n",
    "    \n",
    "    # print(words)\n",
    "    \n",
    "    word_count = 0\n",
    "    for word in words:\n",
    "        # print(word)\n",
    "        if word == keyword:\n",
    "            word_count += 1\n",
    "    return word_count\n",
    "# --------------------start of test code--------------------\n",
    "# text = 'This apple 6i7s very tasty？、  2but th&e banana this is not delicious at Is all.6'\n",
    "# keyword = 'this apple'\n",
    "# count = count_keyword(text, keyword, 2)\n",
    "# print(count)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b488729c-2e38-43e8-bba2-d81126f38847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_freq_from_liter(text, on_topic_kws, type):\n",
    "    text = ''.join(e for e in text if (e.isalpha() or e == \" \" or e == \"-\"))\n",
    "    # print(text)\n",
    "    text = text.strip().lower()\n",
    "    # print(text)\n",
    "\n",
    "    text_length = len(text.split(\" \"))\n",
    "    keywords_count = dict()\n",
    "    keywords_fre = dict()\n",
    "\n",
    "    # count the on-topic keywords\n",
    "    for i in range(len(on_topic_kws)):\n",
    "        word_count = count_keyword(text, on_topic_kws[i], len(on_topic_kws[i].split(\" \")))\n",
    "        print(word_count)\n",
    "        if type == \"count\":\n",
    "            keywords_count[on_topic_kws[i]] = word_count\n",
    "        elif type == \"frequency\":\n",
    "            keywords_fre[on_topic_kws[i]] = math.ceil((word_count*100/text_length))/100\n",
    "        else:\n",
    "            raise Exception(\"Error! The only two options for type are 'count' or 'frequency'!\")\n",
    "    \n",
    "    if type == \"count\":\n",
    "        return keywords_count\n",
    "    elif type == \"frequency\":\n",
    "        return keywords_fre\n",
    "    else:\n",
    "        raise Exception(\"Error! The only two options for type are 'count' or 'frequency'!\")\n",
    "# --------------------start of test code--------------------\n",
    "# text = 'Vision for action: thalamic and cortical inputs to the macaque superior tract neural tracing, parietal lobule The dorsal visual stream, the cortical circuit that in the primate brain is mainly dedicated to the visual control of actions, is split into two routes, a lateral and a medial one, both involved in coding different aspects of sensorimotor control of actions. The lateral route, named \"lateral grasping network\", is mainly involved in the control of the distal part of prehension, namely grasping and manipulation. The medial route, named \"reach-to-grasp network\", is involved in the control of the full deployment of prehension act, from the direction of arm movement to the shaping of the hand according to the object to be grasped. In macaque monkeys, the reach-to-grasp network (the target of this review) includes areas of the superior parietal lobule (SPL) that hosts visual and somatosensory neurons well suited to control goal-directed limb movements toward stationary as well as moving objects. After a brief summary of the neuronal functional properties of these areas, we will analyze their cortical and thalamic inputs thanks to retrograde neuronal tracers separately injected into the SPL areas V6, V6A, PEc, and PE. These areas receive visual and somatosensory information distributed in a caudorostral, visuosomatic trend, and some of them are directly connected with the dorsal premotor cortex. This review is particularly focused on the origin and type of visual information reaching the SPL, and on the functional role this information can play in guiding limb interaction with objects in structured and dynamic environments. Area PEc; Area V6; Area V6A; Dorsal visual stream; Goal-directed arm movement; Sensorimotor integration.'\n",
    "# keywords_count_fre = count_freq_from_liter(text, params.on_topic_kws, type=\"count\")\n",
    "# print(keywords_count_fre)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f02fb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul_index(keywords_count_or_fre, on_topic_kws_weights):\n",
    "    index = 0\n",
    "    for key in keywords_count_or_fre.keys():\n",
    "        index += keywords_count_or_fre[key] * on_topic_kws_weights[key]\n",
    "    return index\n",
    "# --------------------start of test code--------------------\n",
    "# keywords_count_or_fre = {}\n",
    "# index = calcul_related(keywords_count_or_fre, params.on_topic_kws_weights)\n",
    "# print(index)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84e08f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_and_rank(input_path, output_path, on_topic_kws_weights):\n",
    "    df = pd.read_csv(input_path, header=None, sep=\",\")\n",
    "    df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_link\", \"full_text_source\", \"pdf_link\", \"Title\", \"Abstract\", \"Keywords\", \"index\"]\n",
    "\n",
    "    for ind in df.index:\n",
    "        text = \"\"\n",
    "        if df.at[ind, \"Title\"] == df.at[ind, \"Title\"]:\n",
    "            text += df.at[ind, \"Title\"] + \" \"\n",
    "        if df.at[ind, \"Abstract\"] == df.at[ind, \"Abstract\"]:\n",
    "            text += df.at[ind, \"Abstract\"] + \" \"\n",
    "        if df.at[ind, \"Keywords\"] == df.at[ind, \"Keywords\"]:\n",
    "            text += df.at[ind, \"Keywords\"] + \" \"\n",
    "        # print(text)\n",
    "        # type = \"count\"\n",
    "        type = \"frequency\"\n",
    "        keywords_count_or_fre = count_freq_from_liter(text, params.on_topic_kws, type)\n",
    "        index = calcul_index(keywords_count_or_fre, on_topic_kws_weights)\n",
    "        # print(index)\n",
    "        df.at[ind, \"index\"] = index\n",
    "        # print(ind)\n",
    "    \n",
    "    # rank\n",
    "    df = df.sort_values(by=[\"index\"], ascending=False)\n",
    "    df.to_csv(output_path, header=True, index=False)\n",
    "    print(\"Weighting and ranking the potentially related literature succeded!\")\n",
    "    print(\"Enjoy reading!\")\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e562971",
   "metadata": {},
   "source": [
    "<h3> Main program: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b0387fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "PDF_URL not found for:  https://linkinghub.elsevier.com/retrieve/pii/S0165-0270(17)30363-1\n",
      "81\n",
      "82\n",
      "83\n",
      "403 when getting final redirected url:  https://journals.physiology.org/doi/pdf/10.1152/jn.1944.7.3.171\n",
      "84\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "PDF_URL not extracted from info: , but existed already:  https://dukespace.lib.duke.edu/dspace/bitstream/handle/10161/11747/Sommer2003-RoleOfThalamusInMotorControl.pdf?sequence=1&isAllowed=y\n",
      "94\n",
      "96\n",
      "PDF_URL not extracted from info: , but existed already:  http://brainvis.wustl.edu/resources/VE_PBR05.pdf\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error! Cannot extract information from the webpage: https://linkinghub.elsevier.com/retrieve/pii/0304-3940(93)90180-S",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 73\u001b[0m, in \u001b[0;36minfo_filling\u001b[0;34m(input_path, output_path, start, end)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 73\u001b[0m     info \u001b[39m=\u001b[39m extract_info\u001b[39m.\u001b[39;49mextract_info_from_webpage(url, params\u001b[39m.\u001b[39;49mwebsites)\n\u001b[1;32m     74\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/myProjects/thalamocortical_connectivity_in_macaque/extract_info.py:49\u001b[0m, in \u001b[0;36mextract_info_from_webpage\u001b[0;34m(url, websites)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[39m# print(func)\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     info \u001b[39m=\u001b[39m func(url)\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/myProjects/thalamocortical_connectivity_in_macaque/extract_info.py:212\u001b[0m, in \u001b[0;36mfunc_elsevier_com\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m    211\u001b[0m options\u001b[39m.\u001b[39madd_argument(\u001b[39m'\u001b[39m\u001b[39m--headless\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 212\u001b[0m driver \u001b[39m=\u001b[39m webdriver\u001b[39m.\u001b[39;49mFirefox(options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m    214\u001b[0m \u001b[39m# load the webpage\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/selenium/webdriver/firefox/webdriver.py:68\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[0;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     63\u001b[0m executor \u001b[39m=\u001b[39m FirefoxRemoteConnection(\n\u001b[1;32m     64\u001b[0m     remote_server_addr\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mservice\u001b[39m.\u001b[39mservice_url,\n\u001b[1;32m     65\u001b[0m     ignore_proxy\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39m_ignore_local_proxy,\n\u001b[1;32m     66\u001b[0m     keep_alive\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_alive,\n\u001b[1;32m     67\u001b[0m )\n\u001b[0;32m---> 68\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(command_executor\u001b[39m=\u001b[39;49mexecutor, options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions)\n\u001b[1;32m     70\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_remote \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:206\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[0;34m(self, command_executor, keep_alive, file_detector, options)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_client()\n\u001b[0;32m--> 206\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstart_session(capabilities)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:291\u001b[0m, in \u001b[0;36mWebDriver.start_session\u001b[0;34m(self, capabilities)\u001b[0m\n\u001b[1;32m    290\u001b[0m caps \u001b[39m=\u001b[39m _create_caps(capabilities)\n\u001b[0;32m--> 291\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mNEW_SESSION, caps)[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    292\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession_id \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39msessionId\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:344\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    342\u001b[0m         params[\u001b[39m\"\u001b[39m\u001b[39msessionId\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession_id\n\u001b[0;32m--> 344\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcommand_executor\u001b[39m.\u001b[39;49mexecute(driver_command, params)\n\u001b[1;32m    345\u001b[0m \u001b[39mif\u001b[39;00m response:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/selenium/webdriver/remote/remote_connection.py:290\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    289\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_url\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 290\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(command_info[\u001b[39m0\u001b[39;49m], url, body\u001b[39m=\u001b[39;49mdata)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/selenium/webdriver/remote/remote_connection.py:311\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 311\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conn\u001b[39m.\u001b[39;49mrequest(method, url, body\u001b[39m=\u001b[39;49mbody, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m    312\u001b[0m     statuscode \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/urllib3/request.py:78\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_encode_body(\n\u001b[1;32m     79\u001b[0m         method, url, fields\u001b[39m=\u001b[39;49mfields, headers\u001b[39m=\u001b[39;49mheaders, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49murlopen_kw\n\u001b[1;32m     80\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/urllib3/request.py:170\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    168\u001b[0m extra_kw\u001b[39m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 170\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/urllib3/poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(method, u\u001b[39m.\u001b[39;49mrequest_uri, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    378\u001b[0m redirect_location \u001b[39m=\u001b[39m redirect \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mget_redirect_location()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m     httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    462\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m     \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m     \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m     response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1376\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m \u001b[39mexcept\u001b[39;00m timeout:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m output_path \u001b[39m=\u001b[39m fpath\u001b[39m.\u001b[39mpoten_litera_litera_db\n\u001b[1;32m      5\u001b[0m \u001b[39m# clear file\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# plib.clear_file(output_path)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m info_filling(input_path, output_path, \u001b[39m75\u001b[39;49m, \u001b[39m10980\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[6], line 75\u001b[0m, in \u001b[0;36minfo_filling\u001b[0;34m(input_path, output_path, start, end)\u001b[0m\n\u001b[1;32m     73\u001b[0m     info \u001b[39m=\u001b[39m extract_info\u001b[39m.\u001b[39mextract_info_from_webpage(url, params\u001b[39m.\u001b[39mwebsites)\n\u001b[1;32m     74\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mError! Cannot extract information from the webpage: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m url)\n\u001b[1;32m     77\u001b[0m \u001b[39m# doi\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[39mif\u001b[39;00m info[\u001b[39m'\u001b[39m\u001b[39mdoi\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m info[\u001b[39m'\u001b[39m\u001b[39mdoi\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mand\u001b[39;00m doi \u001b[39m==\u001b[39m doi \u001b[39mand\u001b[39;00m info[\u001b[39m'\u001b[39m\u001b[39mdoi\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m!=\u001b[39m doi:\n",
      "\u001b[0;31mException\u001b[0m: Error! Cannot extract information from the webpage: https://linkinghub.elsevier.com/retrieve/pii/0304-3940(93)90180-S"
     ]
    }
   ],
   "source": [
    "# step 1: extract and filling text\n",
    "input_path = fpath.poten_litera_ids_ftl_filled\n",
    "output_path = fpath.poten_litera_litera_db\n",
    "\n",
    "# clear file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "info_filling(input_path, output_path, 75, 10980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daccd342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pdfs and rename them to build a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9493b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # step 2: assign index to each literature and rank them\n",
    "# input_path = fpath.poten_litera_litera_db\n",
    "# output_path = fpath.poten_litera_litera_db_ranked\n",
    "\n",
    "# # clear file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# weight_and_rank(input_path, output_path, on_topic_kws_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc9ccf",
   "metadata": {},
   "source": [
    "<h3> Next step: manually read papers and find all actually related literature </h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
