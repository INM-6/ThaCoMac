{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea1cce5-4f07-4bd7-8ca3-5f6fa51254d0",
   "metadata": {},
   "source": [
    "<h2> Automatic filtering </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad5a61c5-f8c6-418a-b450-cdea0378ddab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "from nltk import ngrams\n",
    "import PyPDF2\n",
    "import json\n",
    "import time\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import shutil\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "726b12c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 13:08:49 GM01X680 metapub.config[349754] WARNING NCBI_API_KEY was not set.\n"
     ]
    }
   ],
   "source": [
    "# import internal modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib\n",
    "import extract_info\n",
    "import parameters as params\n",
    "import download_and_process_pdf as dpp\n",
    "import dataframe_columns as df_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489f0451",
   "metadata": {},
   "source": [
    "<h3> Predefined fucntions: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce32f7b8-96b7-4f5c-a2da-3ff931cdeaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def count_word_from_text(text, word): \n",
    "    word = word.lower()\n",
    "\n",
    "    # process text\n",
    "    text = plib.process_text(text, lower=True)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    len_word = len(word.split())\n",
    "    \n",
    "    # get the words of length len_word from text\n",
    "    ng = list(ngrams(tokens, len_word))\n",
    "    words = [' '.join(gram) for gram in ng]\n",
    "    # print(words)\n",
    "\n",
    "    # count word\n",
    "    if word in params.exact_match_kw_list: # if word is in exact_match_kw_list, use exact match\n",
    "        count = 0\n",
    "        for w in words:\n",
    "            if word == w:\n",
    "                count += 1\n",
    "    else: # if word is not in exact_match_kw_list, use fuzzy match\n",
    "        count = text.count(word)\n",
    "    return count\n",
    "# --------------------Start of test code--------------------\n",
    "# # text = 'This all.6 apple 6i7s very_tasty？、 apple, 6i7s 2but-the banana this is not delicious at Is all.6'\n",
    "# # text = \"cat of the dopcate innervaton in innervation the macaque and human thalamus Miguel Ángel García-Cabezas,aBeatriz Rico,a,b Miguel Ángel Sánchez-González,aand Carmen Cavadaa,⁎ aDepartamento de Anatomía, Histología y Neurociencia, Facultad de Medicina, Universidad Autónoma de Madrid, C/Arzobispo Morcillo s/n, 28029 Madrid, Spain bInstituto de Neurociencias de Alicante, Universidad Miguel Hernández-CSIC, 03550 Sant Joan d ’Alacant, Spain Received 19 April 2006; revised 8 June 2006; accepted 11 July 2006 Available online 30 November 2006 We recently defined the thalamic dopaminergic system in primates; it arises from numerous dopaminergic cell groups and selectively targetsnumerous thalamic nuclei. Given the central position of the thalamus in subcortical and cortical interplay, and the functional relevance of dopamine neuromodulation in the brain, detailing dopamine dis-tribution in the thalamus should supply important information. Tothis end we performed immunohistochemistry for dopamine and the dopamine transporter in the thalamus of macaque monkeys and humans to generate maps, in the stereotaxic coronal plane, of thedistribution of dopaminergic axons. The dopamine innervation of the thalamus follows the same pattern in both species and is most dense in midline limbic nuclei, the mediodorsal and lateral posteriorassociation nuclei, and in the ventral lateral and ventral anteriormotor nuclei. This distribution suggests that thalamic dopamine has a prominent role in emotion, attention, cognition and complex somatosensory and visual processing, as well as in motor control.Most thalamic dopaminergic axons are thin and varicose and targetboth the neuropil and small blood vessels, suggesting that, besides neuronal modulation, thalamic dopamine may have a direct influence on microcirculation. The maps provided here should be a usefulreference in future experimental and neuroimaging studies aiming atclarifying the role of the thalamic dopaminergic system in health and in conditions involving brain dopamine, including Parkinson ’s disease, drug addiction and schizophrenia.© 2006 Elsevier Inc. All rights reserved. Keywords: Dopamine; Thalamus; Monkey; Human; Primate; Dopamine transporter; Parkinson; Schizophrenia; AddictionIntroduction The thalamus is made up of multiple nuclei relaying information from subcortical centers or from other cortices to the cerebral cortex (Sherman and Guillery, 2005 ), as well as the striatum, the nucleus accumbens and the amygdala ( Steriade et al., 1997 ). In addition to specific subcortical and cortical afferents, the primate thalamus receives axons containing the neuromodulators acetylcholine (Heckers et al., 1992 ), histamine ( Manning et al., 1996 ), serotonin (Morrison and Foote, 1986; Lavoie and Parent, 1991 ), and the catecholamines adrenaline ( Rico and Cavada, 1998a ), noradrenaline (Morrison and Foote, 1986; Ginsberg et al., 1993 ) and dopamine (Sánchez-González et al., 2005 ). Until recently, the existence of significant dopamine innervation in the primate thalamus has been largely ignored, probably becausedopamine innervation of the rodent thalamus is very scant(Groenewegen, 1988; Papadopoulos and Parnavelas, 1990 ). However, fragmentary data scattered through the literature endorse the presence of dopamine innervation in the primate thalamus.Postmortem biochemical studies showed the presence of dopamine in the thalamus of macaques ( Brown et al., 1979; Goldman-Rakic and Brown, 1981; Pifl et al., 1990, 1991 ) and human subjects ( Oke and Adams, 1987 ). Later, receptor binding and in situ hybridization analyses detected the presence of dopamine D2-like ( Joyce et al., 1991; Kessler et al., 1993; Hall et al., 1996; Langer et al., 1999;Rieck et al., 2004 ) and D3-like receptors ( Gurevich and Joyce, 1999 ) in several human thalamic nuclei. Positron emission tomography (PET) radioligand studies have also demonstratedthe presence of the dopamine transporter (DAT) ( Wang et al., 1995; Halldin et al., 1996; Helfenbein et al., 1999; Brownell et al., 2003 ) and of D2-like receptors ( Farde et al., 1997; Langer et al., 1999; Okubo et al., 1999; Brownell et al., 2003; Rieck et al., 2004 ) in the human and macaque thalamus. In the course of PET studies focusing on schizophrenia, D2- and D3-like radioligand binding was also found in the thalamus of control subjects ( Talvik et al., 2003; Yasuno et al., 2004 ). Finally, an immunohistochemical study using anti-DAT antibodies detected the presence of dopaminergic www.elsevier.com/locate/ynimg NeuroImage 34 (2007) 965 –984 ⁎Corresponding author. Fax: +34 91 497 53 15. E-mail address: carmen.cavada@uam.es (C. Cavada). Available online on ScienceDirect (www.sciencedirect.com). 1053-8119/$ - see front matter © 2006 Elsevier Inc. All rights reserved. doi:10.1016/j.neuroimage.2006.07.032\"\n",
    "# text = \"Effect of sCate Attentive  ca t Fixation in Macaque asThalamusss andc Cortexs D. B. BENDER AND M. YOUAKIM Department of Physiology and Biophysics, School of Medicine and Biomedical Sciences, University at Buffalo, State University of New York, Buffalo, New York 14214 Received 29 December 1999; accepted in final form 21 September 2000 Bender, D. B. and M. Youakim. Effect of attentive fixation in macaque thalamus and cortex. J Neurophysiol 85: 219234, 2001. Attentional modulation of neuronal responsiveness is common in many areas of visual cortex. We examined whether attentional modulation in the visual thalamus was quantitatively similar to that in cortex. Identical procedures and apparatus were used to compare attentional modulation of single neurons in seven different areas of the visual system: the lateral geniculate, three visual subdivisions of the pulvinar [inferior, lateral, dorsomedial part of lateral pulvinar (Pdm)], and three areas of extrastriate cortex representing early, intermediate, and late stages of cortical processing (V2, V4/PM, area 7a). A simple fixation task controlled transitions among three attentive states. The animal waited for a fixation point to appear (ready state), fixated the point until it dimmed (fixation state), and then waited idly to begin the next trial (idle state). Attentional modulation was estimated by flashing an identical, irrelevant stimulus in a neurons receptive field during each of the three states; the three responses defined a response vector whose deviation from the line of equal response in all three states (the main diagonal) indicated the character and magnitude of attentional modulation. Attentional modulation was present in all visual areas except the lateral geniculate, indicating that modulation was of central origin. Prevalence of modulation was modest (26%) in pulvinar, and increased from 21% in V2 to 43% in 7a. Modulation had a push-pull character (as many cells facilitated as suppressed) with respect to the fixation state in all areas except Pdm where all cells were suppressed during fixation. The absolute magnitude of attentional modulation, measured by the angle between response vector and main diagonal expressed as a percent of the maximum possible angle, differed among brain areas. Magnitude of modulation was modest in the pulvinar (1926%), and increased from 22% in V2 to 41% in 7a. However, average trial-to-trial variability of response, measured by the coefficient of variation, also increased across brain areas so that its difference among areas accounted for more than 90% of the difference in modulation magnitude among areas. We also measured attentional modulation by the ratio of cell discharge due to attention divided by discharge variability. The resulting signal-tonoise ratio of attention was small and constant, 1.3 6 10%, across all areas of pulvinar and cortex. We conclude that the pulvinar, but not the lateral geniculate, is as strongly affected by attentional state as any area of visual cortex we studied and that attentional modulation amplitude is closely tied to intrinsic variability of response. INTRODUCTION It is now clear that attention can affect the responsiveness of neurons throughout visual cortex. Visually responsive cortex includes a number of distinct areas beyond striate cortex, or V1. Beginning with V2, these extrastriate areas are organized into two partially segregated, roughly hierarchical systems (reviews in Felleman and Van Essen 1991; Maunsell and Newsome 1987; Ungerleider and Mishkin 1982; Van Essen 1985). One includes dorsally located areas such as V3A, MT, and MST and leads into area 7a in the inferior parietal lobule. The other includes more ventrally located areas such as V4 and TEO and leads into area TE in the temporal lobe. Recordings from single neurons in many of these areas show that neuronal excitability depends on the animals attentive state (reviews in Colby 1991; Desimone and Duncan 1995; Lock and Bender 1999; Maunsell 1995; Motter 1998). Typically the effect of attention is modest: a small increase or decrease in magnitude of response to a visual stimulus relative to a control condition. Such modulation can be found at virtually every level of the cortical hierarchy, including V1. A variety of behavioral paradigms have been used to manipulate attention, and these show that the prevalence and magnitude of attentional modulation can depend substantially on both the behavioral paradigm and the cortical area in which its effects are measured. Furthermore factors such as task difficulty, the extent to which a task engages the functions of an area, and whether multiple stimuli compete for attention all can affect the modulation (Luck et al. 1997; Motter 1993; Richmond and Sato 1987). To what extent does the thalamus contribute to, or participate in, the attentional modulation that is so widespread throughout visual cortex? Three thalamic nuclei are closely interrelated with visual cortex: the lateral geniculate nucleus, the pulvinar, and the reticular nucleus of the thalamus. All have been thought to be involved in one form of attention or another (e.g., Guillery et al. 1998; Koch and Ullman 1985; Olshausen et al. 1993). The lateral geniculate projects almost exclusively to V1 with little or no output to extrastriate cortex. Layer 6 of both extrastriate and striate cortex project back to the geniculate, potentially modulating transmission through it. The pulvinar has at least three distinct visual subdivisions. The inferior (PI) and lateral pulvinar (PL) contain two separate visuotopic maps (Bender 1981). PI is driven by input from V1 (Bender 1983) but also receives input from extrastriate cortex and the superior colliculus. It projects mainly to V2, V3, V3A, and MT. PL likewise receives input from V1 and extrastriate cortex, but may have a particular affinity\"\n",
    "# keyword = 'sThalamusss andc cortex'\n",
    "# count = count_word_from_text(text, keyword)\n",
    "# print(count)\n",
    "# ---------------------End of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b488729c-2e38-43e8-bba2-d81126f38847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "def count_kw_group_from_text(text_tak, text_txt, keyword_group):\n",
    "    if text_txt == text_txt and text_txt != \"\": # if full text is available, use full text\n",
    "        text = text_txt\n",
    "    elif text_tak == text_tak and text_tak != \"\": # if full text is not available, use tak (title + abstract + keywords)\n",
    "        text = text_tak\n",
    "    else:\n",
    "        raise ValueError('text_tak and text_txt cannot both be np.nan.')\n",
    "\n",
    "    # process the text\n",
    "    text = plib.process_text(text, lower=True)\n",
    "\n",
    "    # count the number of keywords\n",
    "    word_count = 0\n",
    "    for word in keyword_group:\n",
    "        word_count += count_word_from_text(text, word)\n",
    "\n",
    "    return word_count\n",
    "# --------------------start of test code--------------------\n",
    "# text_tak = 'Virhesussiorhesuscat macaqua Cat Rhesus fRhesusor rhesus action: thalamic and cortical inputs to the macaque superior tract neural tracing, parietal lobule The dorsal visual stream, the cortical circuit that in the primate brain is mainly dedicated to the visual control of actions, is split into two routes, a lateral and a medial one, both involved in coding different aspects of sensorimotor control of actions. The lateral route, named \"lateral grasping network\", is mainly involved in the control of the distal part of prehension, namely grasping and manipulation. The medial route, named \"reach-to-grasp network\", is involved in the control of the full deployment of prehension act, from the direction of arm movement to the shaping of the hand according to the object to be grasped. In macaque monkeys, the reach-to-grasp network (the target of this review) includes areas of the superior parietal lobule (SPL) that hosts visual and somatosensory neurons well suited to control goal-directed limb movements toward stationary as well as moving objects. After a brief summary of the neuronal functional properties of these areas, we will analyze their cortical and thalamic inputs thanks to retrograde neuronal tracers separately injected into the SPL areas V6, V6A, PEc, and PE. These areas receive visual and somatosensory information distributed in a caudorostral, visuosomatic trend, and some of them are directly connected with the dorsal premotor cortex. This review is particularly focused on the origin and type of visual information reaching the SPL, and on the functional role this information can play in guiding limb interaction with objects in structured and dynamic environments. Area PEc; Area V6; Area V6A; Dorsal visual stream; Goal-directed arm movement; Sensorimotor integration.'\n",
    "# # text_txt = \"rhesusSDistrirhesusbution rhesus of the dopamine innervation in the macaque and human thalamus Miguel Ángel García-Cabezas,aBeatriz Rico,a,b Miguel Ángel Sánchez-González,aand Carmen Cavadaa,⁎ aDepartamento de Anatomía, Histología y Neurociencia, Facultad de Medicina, Universidad Autónoma de Madrid, C/Arzobispo Morcillo s/n, 28029 Madrid, Spain bInstituto de Neurociencias de Alicante, Universidad Miguel Hernández-CSIC, 03550 Sant Joan d ’Alacant, Spain Received 19 April 2006; revised 8 June 2006; accepted 11 July 2006 Available online 30 November 2006 We recently defined the thalamic dopaminergic system in primates; it arises from numerous dopaminergic cell groups and selectively targetsnumerous thalamic nuclei. Given the central position of the thalamus in subcortical and cortical interplay, and the functional relevance of dopamine neuromodulation in the brain, detailing dopamine dis-tribution in the thalamus should supply important information. Tothis end we performed immunohistochemistry for dopamine and the dopamine transporter in the thalamus of macaque monkeys and humans to generate maps, in the stereotaxic coronal plane, of thedistribution of dopaminergic axons. The dopamine innervation of the thalamus follows the same pattern in both species and is most dense in midline limbic nuclei, the mediodorsal and lateral posteriorassociation nuclei, and in the ventral lateral and ventral anteriormotor nuclei. This distribution suggests that thalamic dopamine has a prominent role in emotion, attention, cognition and complex somatosensory and visual processing, as well as in motor control.Most thalamic dopaminergic axons are thin and varicose and targetboth the neuropil and small blood vessels, suggesting that, besides neuronal modulation, thalamic dopamine may have a direct influence on microcirculation. The maps provided here should be a usefulreference in future experimental and neuroimaging studies aiming atclarifying the role of the thalamic dopaminergic system in health and in conditions involving brain dopamine, including Parkinson ’s disease, drug addiction and schizophrenia.© 2006 Elsevier Inc. All rights reserved. Keywords: Dopamine; Thalamus; Monkey; Human; Primate; Dopamine transporter; Parkinson; Schizophrenia; AddictionIntroduction The thalamus is made up of multiple nuclei relaying information from subcortical centers or from other cortices to the cerebral cortex (Sherman and Guillery, 2005 ), as well as the striatum, the nucleus accumbens and the amygdala ( Steriade et al., 1997 ). In addition to specific subcortical and cortical afferents, the primate thalamus receives axons containing the neuromodulators acetylcholine (Heckers et al., 1992 ), histamine ( Manning et al., 1996 ), serotonin (Morrison and Foote, 1986; Lavoie and Parent, 1991 ), and the catecholamines adrenaline ( Rico and Cavada, 1998a ), noradrenaline (Morrison and Foote, 1986; Ginsberg et al., 1993 ) and dopamine (Sánchez-González et al., 2005 ). Until recently, the existence of significant dopamine innervation in the primate thalamus has been largely ignored, probably becausedopamine innervation of the rodent thalamus is very scant(Groenewegen, 1988; Papadopoulos and Parnavelas, 1990 ). However, fragmentary data scattered through the literature endorse the presence of dopamine innervation in the primate thalamus.Postmortem biochemical studies showed the presence of dopamine in the thalamus of macaques ( Brown et al., 1979; Goldman-Rakic and Brown, 1981; Pifl et al., 1990, 1991 ) and human subjects ( Oke and Adams, 1987 ). Later, receptor binding and in situ hybridization analyses detected the presence of dopamine D2-like ( Joyce et al., 1991; Kessler et al., 1993; Hall et al., 1996; Langer et al., 1999;Rieck et al., 2004 ) and D3-like receptors ( Gurevich and Joyce, 1999 ) in several human thalamic nuclei. Positron emission tomography (PET) radioligand studies have also demonstratedthe presence of the dopamine transporter (DAT) ( Wang et al., 1995; Halldin et al., 1996; Helfenbein et al., 1999; Brownell et al., 2003 ) and of D2-like receptors ( Farde et al., 1997; Langer et al., 1999; Okubo et al., 1999; Brownell et al., 2003; Rieck et al., 2004 ) in the human and macaque thalamus. In the course of PET studies focusing on schizophrenia, D2- and D3-like radioligand binding was also found in the thalamus of control subjects ( Talvik et al., 2003; Yasuno et al., 2004 ). Finally, an immunohistochemical study using anti-DAT antibodies detected the presence of dopaminergic www.elsevier.com/locate/ynimg NeuroImage 34 (2007) 965 –984 ⁎Corresponding author. Fax: +34 91 497 53 15. E-mail address: carmen.cavada@uam.es (C. Cavada). Available online on ScienceDirect (www.sciencedirect.com). 1053-8119/$ - see front matter © 2006 Elsevier Inc. All rights reserved. doi:10.1016/j.neuroimage.2006.07.032\"\n",
    "# text_txt = \"Effect of cats Attentive Fixation in Macaque Thalamus and Cortex D. B. BENDER AND M. YOUAKIM Department of Physiology and Biophysics, School of Medicine and Biomedical Sciences, University at Buffalo, State University of New York, Buffalo, New York 14214 Received 29 December 1999; accepted in final form 21 September 2000 Bender, D. B. and M. Youakim. Effect of attentive fixation in macaque thalamus and cortex. J Neurophysiol 85: 219234, 2001. Attentional modulation of neuronal responsiveness is common in many areas of visual cortex. We examined whether attentional modulation in the visual thalamus was quantitatively similar to that in cortex. Identical procedures and apparatus were used to compare attentional modulation of single neurons in seven different areas of the visual system: the lateral geniculate, three visual subdivisions of the pulvinar [inferior, lateral, dorsomedial part of lateral pulvinar (Pdm)], and three areas of extrastriate cortex representing early, intermediate, and late stages of cortical processing (V2, V4/PM, area 7a). A simple fixation task controlled transitions among three attentive states. The animal waited for a fixation point to appear (ready state), fixated the point until it dimmed (fixation state), and then waited idly to begin the next trial (idle state). Attentional modulation was estimated by flashing an identical, irrelevant stimulus in a neurons receptive field during each of the three states; the three responses defined a response vector whose deviation from the line of equal response in all three states (the main diagonal) indicated the character and magnitude of attentional modulation. Attentional modulation was present in all visual areas except the lateral geniculate, indicating that modulation was of central origin. Prevalence of modulation was modest (26%) in pulvinar, and increased from 21% in V2 to 43% in 7a. Modulation had a push-pull character (as many cells facilitated as suppressed) with respect to the fixation state in all areas except Pdm where all cells were suppressed during fixation. The absolute magnitude of attentional modulation, measured by the angle between response vector and main diagonal expressed as a percent of the maximum possible angle, differed among brain areas. Magnitude of modulation was modest in the pulvinar (1926%), and increased from 22% in V2 to 41% in 7a. However, average trial-to-trial variability of response, measured by the coefficient of variation, also increased across brain areas so that its difference among areas accounted for more than 90% of the difference in modulation magnitude among areas. We also measured attentional modulation by the ratio of cell discharge due to attention divided by discharge variability. The resulting signal-tonoise ratio of attention was small and constant, 1.3 6 10%, across all areas of pulvinar and cortex. We conclude that the pulvinar, but not the lateral geniculate, is as strongly affected by attentional state as any area of visual cortex we studied and that attentional modulation amplitude is closely tied to intrinsic variability of response. INTRODUCTION It is now clear that attention can affect the responsiveness of neurons throughout visual cortex. Visually responsive cortex includes a number of distinct areas beyond striate cortex, or V1. Beginning with V2, these extrastriate areas are organized into two partially segregated, roughly hierarchical systems (reviews in Felleman and Van Essen 1991; Maunsell and Newsome 1987; Ungerleider and Mishkin 1982; Van Essen 1985). One includes dorsally located areas such as V3A, MT, and MST and leads into area 7a in the inferior parietal lobule. The other includes more ventrally located areas such as V4 and TEO and leads into area TE in the temporal lobe. Recordings from single neurons in many of these areas show that neuronal excitability depends on the animals attentive state (reviews in Colby 1991; Desimone and Duncan 1995; Lock and Bender 1999; Maunsell 1995; Motter 1998). Typically the effect of attention is modest: a small increase or decrease in magnitude of response to a visual stimulus relative to a control condition. Such modulation can be found at virtually every level of the cortical hierarchy, including V1. A variety of behavioral paradigms have been used to manipulate attention, and these show that the prevalence and magnitude of attentional modulation can depend substantially on both the behavioral paradigm and the cortical area in which its effects are measured. Furthermore factors such as task difficulty, the extent to which a task engages the functions of an area, and whether multiple stimuli compete for attention all can affect the modulation (Luck et al. 1997; Motter 1993; Richmond and Sato 1987). To what extent does the thalamus contribute to, or participate in, the attentional modulation that is so widespread throughout visual cortex? Three thalamic nuclei are closely interrelated with visual cortex: the lateral geniculate nucleus, the pulvinar, and the reticular nucleus of the thalamus. All have been thought to be involved in one form of attention or another (e.g., Guillery et al. 1998; Koch and Ullman 1985; Olshausen et al. 1993). The lateral geniculate projects almost exclusively to V1 with little or no output to extrastriate cortex. Layer 6 of both extrastriate and striate cortex project back to the geniculate, potentially modulating transmission through it. The pulvinar has at least three distinct visual subdivisions. The inferior (PI) and lateral pulvinar (PL) contain two separate visuotopic maps (Bender 1981). PI is driven by input from V1 (Bender 1983) but also receives input from extrastriate cortex and the superior colliculus. It projects mainly to V2, V3, V3A, and MT. PL likewise receives input from V1 and extrastriate cortex, but may have a particular affinity\"\n",
    "# keyword_group = ['cat', 'macaque Thalamus', 'macaca']\n",
    "# keywords_count = count_kw_group_from_text(text_tak, text_txt, keyword_group)\n",
    "# print(keywords_count)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bdc22165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effect of scat attentive fixation sin acaques shalamus and cortex. effect of attentive fixation in macaque thalamus and cortex. j neurophysiol 85: 219234, 2001. attentional modulation of neuronal responsiveness is common in many areas of visual cortex. we examined whether attentional modulation in the visual thalamus was quantitatively similar to that in cortex. identical procedures and apparatus were used to compare attentional modulation of single neurons in seven different areas of the visual system: the lateral geniculate, three visual subdivisions of the pulvinar [inferior, lateral, dorsomedial part of lateral pulvinar (pdm)], and three areas of extrastriate cortex representing early, intermediate, and late stages of cortical processing (v2, v4/pm, area 7a). the resulting signal-tonoise ratio of attention was small and constant, 1.3 6 10%, across all areas of pulvinar and cortex. we conclude that the pulvinar, but not the lateral geniculate, is as strongly affected by attentional state as any area of visual cortex we studied and that attentional modulation amplitude is closely tied to intrinsic variability of response. introduction it is now clear that attention can affect the responsiveness of neurons throughout visual cortex. visually responsive cortex includes a number of distinct areas beyond striate cortex, or v1. to what extent does the thalamus contribute to, or participate in, the attentional modulation that is so widespread throughout visual cortex?. three thalamic nuclei are closely interrelated with visual cortex: the lateral geniculate nucleus, the pulvinar, and the reticular nucleus of the thalamus. the lateral geniculate projects almost exclusively to v1 with little or no output to extrastriate cortex. layer 6 of both extrastriate and striate cortex project back to the geniculate, potentially modulating transmission through it. pi is driven by input from v1 (bender 1983) but also receives input from extrastriate cortex and the superior colliculus. pl likewise receives input from v1 and extrastriate cortex, but may have a particular affinity.\n"
     ]
    }
   ],
   "source": [
    "def extract_sent_from_text(text, kw_group):\n",
    "    text = plib.process_text(text, lower=True) \n",
    "    sents = []\n",
    "\n",
    "    all_sentences = sent_tokenize(text)\n",
    "    # print the sentences\n",
    "    # for sent in all_sentences:\n",
    "    #     print(sent)\n",
    "\n",
    "    for sent in all_sentences:\n",
    "        flag = False # this sentence is not found\n",
    "        if not sent.endswith('.'):\n",
    "            sent += '.'\n",
    "        # print(sent)\n",
    "        # print(sent)\n",
    "        for keyword in kw_group:\n",
    "            keyword = keyword.lower()\n",
    "            # print(keyword)\n",
    "            # If the keyword is in exact match keyword list, then the keyword must be matched exactly\n",
    "            if keyword in params.exact_match_kw_list:\n",
    "                words = word_tokenize(sent)\n",
    "                print(words)\n",
    "                for word in words:\n",
    "                    if word == keyword and sent not in sents:\n",
    "                        sents.append(sent)\n",
    "                        flag = True\n",
    "                        break\n",
    "                if flag:\n",
    "                    break\n",
    "            # If the keyword is not in exact match keyword list, then the keyword can be matched in a word\n",
    "            elif (keyword not in params.exact_match_kw_list) and (keyword in sent) and (sent not in sents):\n",
    "                sents.append(sent)\n",
    "                # print(sents)\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    # print(list(sents))\n",
    "    # convert set to string\n",
    "    sents = ' '.join(sent for sent in sents)\n",
    "    sents = sents.strip()\n",
    "\n",
    "    return sents\n",
    "# --------------------Start of test code--------------------\n",
    "# text = \"Effect of scat Attentive Fixation sin acaques shalamus and Cortex. D. B. BENDER AND M. YOUAKIM Department of Physiology and Biophysics, School of Medicine and Biomedical Sciences, University at Buffalo, State University of New York, Buffalo, New York 14214 Received 29 December 1999; accepted in final form 21 September 2000 Bender, D. B. and M. Youakim. Effect of attentive fixation in macaque thalamus and cortex. J Neurophysiol 85: 219234, 2001. Attentional modulation of neuronal responsiveness is common in many areas of visual cortex. We examined whether attentional modulation in the visual thalamus was quantitatively similar to that in cortex. Identical procedures and apparatus were used to compare attentional modulation of single neurons in seven different areas of the visual system: the lateral geniculate, three visual subdivisions of the pulvinar [inferior, lateral, dorsomedial part of lateral pulvinar (Pdm)], and three areas of extrastriate cortex representing early, intermediate, and late stages of cortical processing (V2, V4/PM, area 7a). A simple fixation task controlled transitions among three attentive states. The animal waited for a fixation point to appear (ready state), fixated the point until it dimmed (fixation state), and then waited idly to begin the next trial (idle state). Attentional modulation was estimated by flashing an identical, irrelevant stimulus in a neurons receptive field during each of the three states; the three responses defined a response vector whose deviation from the line of equal response in all three states (the main diagonal) indicated the character and magnitude of attentional modulation. Attentional modulation was present in all visual areas except the lateral geniculate, indicating that modulation was of central origin. Prevalence of modulation was modest (26%) in pulvinar, and increased from 21% in V2 to 43% in 7a. Modulation had a push-pull character (as many cells facilitated as suppressed) with respect to the fixation state in all areas except Pdm where all cells were suppressed during fixation. The absolute magnitude of attentional modulation, measured by the angle between response vector and main diagonal expressed as a percent of the maximum possible angle, differed among brain areas. Magnitude of modulation was modest in the pulvinar (1926%), and increased from 22% in V2 to 41% in 7a. However, average trial-to-trial variability of response, measured by the coefficient of variation, also increased across brain areas so that its difference among areas accounted for more than 90% of the difference in modulation magnitude among areas. We also measured attentional modulation by the ratio of cell discharge due to attention divided by discharge variability. The resulting signal-tonoise ratio of attention was small and constant, 1.3 6 10%, across all areas of pulvinar and cortex. We conclude that the pulvinar, but not the lateral geniculate, is as strongly affected by attentional state as any area of visual cortex we studied and that attentional modulation amplitude is closely tied to intrinsic variability of response. INTRODUCTION It is now clear that attention can affect the responsiveness of neurons throughout visual cortex. Visually responsive cortex includes a number of distinct areas beyond striate cortex, or V1. Beginning with V2, these extrastriate areas are organized into two partially segregated, roughly hierarchical systems (reviews in Felleman and Van Essen 1991; Maunsell and Newsome 1987; Ungerleider and Mishkin 1982; Van Essen 1985). One includes dorsally located areas such as V3A, MT, and MST and leads into area 7a in the inferior parietal lobule. The other includes more ventrally located areas such as V4 and TEO and leads into area TE in the temporal lobe. Recordings from single neurons in many of these areas show that neuronal excitability depends on the animals attentive state (reviews in Colby 1991; Desimone and Duncan 1995; Lock and Bender 1999; Maunsell 1995; Motter 1998). Typically the effect of attention is modest: a small increase or decrease in magnitude of response to a visual stimulus relative to a control condition. Such modulation can be found at virtually every level of the cortical hierarchy, including V1. A variety of behavioral paradigms have been used to manipulate attention, and these show that the prevalence and magnitude of attentional modulation can depend substantially on both the behavioral paradigm and the cortical area in which its effects are measured. Furthermore factors such as task difficulty, the extent to which a task engages the functions of an area, and whether multiple stimuli compete for attention all can affect the modulation (Luck et al. 1997; Motter 1993; Richmond and Sato 1987). To what extent does the thalamus contribute to, or participate in, the attentional modulation that is so widespread throughout visual cortex? Three thalamic nuclei are closely interrelated with visual cortex: the lateral geniculate nucleus, the pulvinar, and the reticular nucleus of the thalamus. All have been thought to be involved in one form of attention or another (e.g., Guillery et al. 1998; Koch and Ullman 1985; Olshausen et al. 1993). The lateral geniculate projects almost exclusively to V1 with little or no output to extrastriate cortex. Layer 6 of both extrastriate and striate cortex project back to the geniculate, potentially modulating transmission through it. The pulvinar has at least three distinct visual subdivisions. The inferior (PI) and lateral pulvinar (PL) contain two separate visuotopic maps (Bender 1981). PI is driven by input from V1 (Bender 1983) but also receives input from extrastriate cortex and the superior colliculus. It projects mainly to V2, V3, V3A, and MT. PL likewise receives input from V1 and extrastriate cortex, but may have a particular affinity\"\n",
    "# text = plib.process_text(text, lower=True)\n",
    "# kw_group = ['Cortex', 'Thalamus', 'Macaques']\n",
    "# sents = extract_sent_from_text(text, kw_group)\n",
    "# print(sents)\n",
    "# ---------------------End of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88e749e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sents_and_record(train_test_1000_path, db_path, db_text_extract_path):\n",
    "    df_1000 = pd.read_csv(train_test_1000_path, header=0, sep=',')\n",
    "    df_1000.columns = df_col.train_test_1000_path_columns\n",
    "\n",
    "    df_db = pd.read_csv(db_path, header=None, sep=',')\n",
    "    df_db.columns = df_col.db_columns\n",
    "\n",
    "    for ind in df_1000.index:\n",
    "        index = int(df_1000.at[ind, \"INDEX\"])\n",
    "\n",
    "        # get the text from the full text\n",
    "        txt_file_name = str(index) + \".txt\"\n",
    "        txt_path = os.path.join(fpath.text_folder, txt_file_name)\n",
    "        txt_500_path = os.path.join(fpath.processed_texts_of_length_500_folder, txt_file_name)\n",
    "\n",
    "        # extract text_tak and text_500\n",
    "        if os.path.exists(txt_path): # text from full text\n",
    "            with open(txt_path, 'r', encoding='ascii') as f:\n",
    "                text_txt = f.read()\n",
    "            f.close()\n",
    "\n",
    "            with open(txt_500_path, 'r', encoding='ascii') as f:\n",
    "                text_500 = f.read()\n",
    "            f.close()\n",
    "        else:\n",
    "            text_txt = \"\"\n",
    "\n",
    "        text_tak = \"\" # text from title, abstract, and keywords\n",
    "        if df_1000.at[ind, \"TITLE\"] == df_1000.at[ind, \"TITLE\"]:\n",
    "            text_tak = text_tak + df_1000.at[ind, \"TITLE\"] + \" \"\n",
    "        else:\n",
    "            pass\n",
    "        if df_1000.at[ind, \"ABSTRACT\"] == df_1000.at[ind, \"ABSTRACT\"]:\n",
    "            text_tak = text_tak + df_1000.at[ind, \"ABSTRACT\"] + \" \"\n",
    "        else:\n",
    "            pass\n",
    "        if df_1000.at[ind, \"KEYWORDS\"] == df_1000.at[ind, \"KEYWORDS\"]:\n",
    "            text_tak = text_tak + df_1000.at[ind, \"KEYWORDS\"] + \" \"\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # process the text\n",
    "        text_txt = plib.process_text(text_txt, lower=True)\n",
    "        text_500 = plib.process_text(text_500, lower=True)\n",
    "        text_tak = plib.process_text(text_tak, lower=True)\n",
    "        \n",
    "        # print(text_txt)\n",
    "        # print(text_500)\n",
    "        # print(text_tak)\n",
    "        \n",
    "        # count keywords from text\n",
    "        text_column = params.text_column_to_add\n",
    "        count_list = [0] * len(params.ranking_kw_groups.keys())\n",
    "        keys_list = list(params.ranking_kw_groups.keys())\n",
    "        for i in range(len(count_list)):\n",
    "            count_list[i] = count_kw_group_from_text(text_tak, text_500, params.ranking_kw_groups[keys_list[i]])\n",
    "        # print(count_list)\n",
    "\n",
    "        text_column_count_list = []\n",
    "        for i in range(len(text_column)):\n",
    "            # get the index of the text_column[i] in keys_list\n",
    "            j = keys_list.index(text_column[i])\n",
    "            text_column_count_list.append(count_list[j])\n",
    "        # print(text_column_count_list)\n",
    "\n",
    "        # extract sentences from text\n",
    "        if text_txt == text_txt and text_txt != \"\": # if full text is available, use full text\n",
    "            text = text_txt\n",
    "        else: # otherwise, use tak (title + abstract + keywords)\n",
    "            text = text_tak\n",
    "\n",
    "        text_list = []\n",
    "        for key in params.ranking_kw_groups.keys():\n",
    "            sents = extract_sent_from_text(text, params.ranking_kw_groups[key])\n",
    "            text_list.append(sents)\n",
    "        \n",
    "        text_column_text_list = []\n",
    "        for i in range(len(text_column)):\n",
    "            j = keys_list.index(text_column[i])\n",
    "            text_column_text_list.append(text_list[j])\n",
    "        # print(text_column_text_list)\n",
    "        \n",
    "        # csv columns\n",
    "        columns = [\n",
    "            \"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"TITLTE\", \"FULL_TEXT_URL\", \"PDF_URL\", \n",
    "            \"TEXT_TAK\", \"TEXT_500\"\n",
    "        ]\n",
    "        columns = columns + [key+\"_TEXT\" for key in text_column] # add keyword group text\n",
    "        columns = columns + [key+\"_COUNT\" for key in text_column] # add keyword group count\n",
    "        columns_to_add = [\"TT?(Y/N/MB/NA)\", \"MACAQUE?(Y/N/MB/NA)\", \"TC_OR_CT?(Y/N/MB/NA)\", \"RELEVANT?(Y/N/MB/NA)\", \"READ_BY(A/D/R)\", \"COMMENT\"] # add columns for documenting labels\n",
    "        columns = columns + columns_to_add\n",
    "\n",
    "        # specify rows\n",
    "        row = {\n",
    "            \"INDEX\": [df_1000.at[ind, \"INDEX\"]],\n",
    "            \"DOI\": [df_1000.at[ind, \"DOI\"]],\n",
    "            \"PMID\": [df_1000.at[ind, \"PMID\"]],\n",
    "            \"PMCID\": [df_1000.at[ind, \"PMCID\"]],\n",
    "            \"TITLTE\": [df_1000.at[ind, \"TITLE\"]],\n",
    "            \"FULL_TEXT_URL\": [df_1000.at[ind, \"FULL_TEXT_URL\"]],\n",
    "            \"PDF_URL\": [df_1000.at[ind, \"PDF_URL\"]],\n",
    "            \"TEXT_TAK\": [text_tak],\n",
    "            \"TEXT_500\": [text_500]\n",
    "        }\n",
    "\n",
    "        i = 0\n",
    "        for key in text_column: # add key value pair of ranking_kw_groups and values in text_group\n",
    "            text_value = text_column_text_list[i]\n",
    "            row[key+\"_TEXT\"] = [text_value]\n",
    "\n",
    "            count_value = text_column_count_list[i]\n",
    "            row[key+\"_COUNT\"] = [count_value]\n",
    "            i += 1\n",
    "\n",
    "        for column in columns_to_add:\n",
    "            row[column] = [np.nan]\n",
    "        # print(row)\n",
    "\n",
    "        # save to csv\n",
    "        if not plib.add_row_to_csv(db_text_extract_path, row, columns):  # add the rest rows without header and with mode \"a\"\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(\"ind:\", ind, \"index:\", index)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    # read the csv file and reset index and add header\n",
    "    df = pd.read_csv(db_text_extract_path, header=None, sep=',')\n",
    "    df.columns = columns\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.to_csv(db_text_extract_path, index=False, header=True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84e08f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_and_record_db(db_path, db_kw_count, ranking_kw_groups):\n",
    "    df = pd.read_csv(db_path, header=None, sep=\",\")\n",
    "    df.columns = df_col.db_columns\n",
    "\n",
    "    for ind in df.index:\n",
    "        index = int(df.at[ind, \"INDEX\"])\n",
    "        txt_file_name = str(index) + \".txt\"\n",
    "        txt_500_path = os.path.join(fpath.processed_texts_of_length_500_folder, txt_file_name)\n",
    "        \n",
    "        # extract text\n",
    "        text_tak = \"\" # text from title, abstract, and keywords\n",
    "        text_txt_500 = \"\" # text from full text\n",
    "\n",
    "        # from title, abstract, and keywords\n",
    "        if df.at[ind, \"TITLE\"] == df.at[ind, \"TITLE\"]:\n",
    "            text_tak = text_tak + \" \" + df.at[ind, \"TITLE\"]\n",
    "        else:\n",
    "            pass  \n",
    "        if df.at[ind, \"ABSTRACT\"] == df.at[ind, \"ABSTRACT\"]:\n",
    "            text_tak = text_tak + \" \" + df.at[ind, \"ABSTRACT\"]\n",
    "        else:\n",
    "            pass\n",
    "        if df.at[ind, \"KEYWORDS\"] == df.at[ind, \"KEYWORDS\"]:\n",
    "            text_tak = text_tak + \" \" + df.at[ind, \"KEYWORDS\"]\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # from full text\n",
    "        if os.path.exists(txt_500_path):\n",
    "            with open(txt_500_path, \"r\", encoding='ascii') as f:\n",
    "                text_txt_500 = f.read()\n",
    "            f.close()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # extract first 500 words from text_txt and text_tak, if they are longer than 500 words\n",
    "        # if they are shorter than 500 words, expand them to 500 words by repeating\n",
    "        text_tak = plib.process_text(text_tak, lower=True)\n",
    "        while len(text_tak.split()) < params.text_length_to_extract:\n",
    "            text_tak = text_tak + \" \" + text_tak\n",
    "        text_tak = ' '.join(text_tak.split()[:params.text_length_to_extract])\n",
    "        text_tak = plib.process_text(text_tak, lower=True)\n",
    "        # print(text_tak)\n",
    "        # print(len(text_tak.split()))\n",
    "\n",
    "        text_txt_500 = plib.process_text(text_txt_500, lower=True)\n",
    "        while len(text_txt_500.split()) < params.text_length_to_extract:\n",
    "            text_txt_500 = text_txt_500 + \" \" + text_txt_500\n",
    "        text_txt_500 = ' '.join(text_txt_500.split()[:params.text_length_to_extract])\n",
    "        text_txt_500 = plib.process_text(text_txt_500, lower=True)\n",
    "        # print(text_txt_500)\n",
    "        # print(len(text_txt_500.split()))\n",
    "\n",
    "        # count keywords from text\n",
    "        count_list = [0] * len(ranking_kw_groups)\n",
    "        keys_list = list(ranking_kw_groups.keys())\n",
    "        for i in range(len(count_list)):\n",
    "            count_list[i] = count_kw_group_from_text(text_tak, text_txt_500, ranking_kw_groups[keys_list[i]])\n",
    "        # print(count_list)\n",
    "\n",
    "        columns = df_col.db_ranked_columns\n",
    "\n",
    "        # specify rows\n",
    "        row = {\n",
    "            \"INDEX\": [df.at[ind, \"INDEX\"]],\n",
    "            \"DOI\": [df.at[ind, \"DOI\"]],\n",
    "            \"PMID\": [df.at[ind, \"PMID\"]],\n",
    "            \"PMCID\": [df.at[ind, \"PMCID\"]],\n",
    "            \"TITLE\": [df.at[ind, \"TITLE\"]]\n",
    "        }\n",
    "        # add key, value pair of keyword group counts\n",
    "        i = 0\n",
    "        for key in ranking_kw_groups.keys():\n",
    "            value = count_list[i]\n",
    "            row[key+\"_COUNT\"] = [value]\n",
    "            i += 1\n",
    "        # print(row)\n",
    "        row[\"RELEVANT\"] = [np.nan]\n",
    "\n",
    "        if not plib.add_row_to_csv(db_kw_count, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        line_number_in_csv = ind + 1\n",
    "        print(\"Line number:\", line_number_in_csv, \" INDEX:\", int(df.at[ind, \"INDEX\"]))\n",
    "# --------------------start of test code--------------------\n",
    "# input_path = fpath.poten_litera_db\n",
    "# output_path = fpath.poten_litera_db_kw_count\n",
    "# count_and_record(input_path, output_path, params.ranking_kw_groups)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f02fb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul_index(counts_dict, ranking_params_weights):\n",
    "    for key in counts_dict.keys():\n",
    "        index += math.log(1 + counts_dict[key]) * (ranking_params_weights[key])\n",
    "    return index\n",
    "# --------------------start of test code--------------------\n",
    "# keywords_count_or_fre = {}\n",
    "# index = calcul_related(keywords_count_or_fre, params.on_topic_kws_weights)\n",
    "# print(index)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5edde53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(db_path, db_ranked_path, ranking_params_weights):\n",
    "    df = pd.read_csv(db_path, header=None, sep=\",\")\n",
    "    df.columns = df_col.db_columns\n",
    "    \n",
    "    for ind in df.index:\n",
    "        count_dict = {}\n",
    "        for key in params.ranking_kw_groups.keys():\n",
    "            count_dict[ind, key+\"_COUNT\"] = int(df.at[ind, key+\"_COUNT\"])\n",
    "        # print(count_dict)\n",
    "\n",
    "        relev_index = calcul_index(count_dict, ranking_params_weights)\n",
    "        \n",
    "        # csv column names\n",
    "        columns = df_col.db_ranked_columns\n",
    "\n",
    "        # specify rows\n",
    "        row = {\n",
    "            \"INDEX\": [df.at[ind, \"INDEX\"]],\n",
    "            \"DOI\": [df.at[ind, \"DOI\"]],\n",
    "            \"PMID\": [df.at[ind, \"PMID\"]],\n",
    "            \"PMCID\": [df.at[ind, \"PMCID\"]],\n",
    "            \"TITLE\": [df.at[ind, \"TITLE\"]],\n",
    "        }\n",
    "        # merge dicts row and count_dict into one\n",
    "        row = {**row, **count_dict}\n",
    "        row[\"RELEVANCE_INDEX\"] = [relev_index]\n",
    "        print(row)\n",
    "\n",
    "        # save to csv\n",
    "        if not plib.add_row_to_csv(db_ranked_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        line_number_in_csv = ind + 1\n",
    "        print(\"Line number:\", line_number_in_csv, \" INDEX:\", int(df.at[ind, \"INDEX\"]))\n",
    "    \n",
    "    # sort and save to csv\n",
    "    df_to_rank = df.read_csv(db_ranked_path, header=None, sep=\",\")\n",
    "    df_to_rank.columns = df_col.db_ranked_columns\n",
    "\n",
    "    df_ranked = df_to_rank.sort_values(by=\"RELEVANCE_INDEX\", ascending=False)\n",
    "    df_ranked.reset_index(drop=True, inplace=True)\n",
    "    df_ranked.to_csv(db_ranked_path, header=True, index=False)\n",
    "    print(\"Weighting and ranking the potentially related literature succeded!\")\n",
    "    print(\"Enjoy reading!\")\n",
    "# --------------------start of test code--------------------\n",
    "# input_path = fpath.poten_litera_db_kw_count\n",
    "# output_path = fpath.poten_litera_db_ranked\n",
    "# rank(input_path, output_path, params.ranking_params_weights)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e562971",
   "metadata": {},
   "source": [
    "<h3> Main program: </h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b3dd0",
   "metadata": {},
   "source": [
    "#### 1. Data processing and candidate articles ranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67041512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of articles considered: 9892\n",
      "Max of length: 1307\n",
      "Average length: 235.13121714516782\n",
      "Median length: 230.0\n",
      "Std of length: 79.72543466983504\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribution of text length in the title + keywords + abstract\n",
    "input_path = fpath.poten_litera_db\n",
    "df = pd.read_csv(input_path, header=None, sep=',')\n",
    "df.columns = [\n",
    "    \"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \n",
    "    \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"\n",
    "    ]\n",
    "\n",
    "len_list = []\n",
    "\n",
    "for ind in df.index:\n",
    "    text = \"\"\n",
    "    num_words = 0\n",
    "\n",
    "    if df.at[ind, \"ABSTRACT\"] == df.at[ind, \"ABSTRACT\"]: # if abstract is available\n",
    "        abstract = df.at[ind, \"ABSTRACT\"]\n",
    "    else: # skip this article if abstract is not available\n",
    "        continue\n",
    "    if df.at[ind, \"TITLE\"] == df.at[ind, \"TITLE\"]:\n",
    "        title = df.at[ind, \"TITLE\"]\n",
    "    else:\n",
    "        title = \"\"\n",
    "    if df.at[ind, \"KEYWORDS\"] == df.at[ind, \"KEYWORDS\"]:\n",
    "        keywords = df.at[ind, \"KEYWORDS\"]\n",
    "    else:\n",
    "        keywords = \"\"\n",
    "\n",
    "    text = title + \" \" + abstract + \" \" + keywords\n",
    "    \n",
    "    # process the text\n",
    "    text = plib.process_text(text, lower=True)\n",
    "\n",
    "    num_words = len(text.split())\n",
    "\n",
    "    len_list.append(num_words)\n",
    "\n",
    "# calculate the average and maximum number of words in the text\n",
    "print(\"The number of articles considered:\", len(len_list))\n",
    "print(\"Max of length:\", max(len_list))\n",
    "print(\"Average length:\", np.mean(len_list))\n",
    "print(\"Median length:\", np.median(len_list))\n",
    "print(\"Std of length:\", np.std(len_list))\n",
    "\n",
    "# sort the len_list and draw a histogram\n",
    "len_list.sort()\n",
    "plt.hist(len_list, bins=20)\n",
    "plt.xlabel(\"Text legth of tak (title + abstract + keywords) text\")\n",
    "plt.ylabel(\"Number of articles\")\n",
    "plt.show()\n",
    "# The number of articles considered: 9892\n",
    "# Max of length: 1307\n",
    "# Average length: 235.13121714516782\n",
    "# Median length: 230.0\n",
    "# Std of length: 79.72543466983504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77873c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the train_test_1000 set, extract sentences and counts, save to poten_litera_testing_set_1000_text_extract_and_count\n",
    "input_path = fpath.poten_litera_testing_set_1000\n",
    "db_path = fpath.poten_litera_db\n",
    "output_path = fpath.poten_litera_testing_set_1000_text_extract_and_count\n",
    "# clear the file\n",
    "plib.clear_file(output_path)\n",
    "\n",
    "extract_sents_and_record(input_path, db_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce5f7f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read the db_final.csv and scan the rows and count the number of np.nans in the columns Macaque?(Y/N), Thalamus?(Y/N), Inject?(Y/N)\n",
    "# input_path = fpath.poten_litera_db_text_extract\n",
    "# df = pd.read_csv(input_path, header=0, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \n",
    "#                \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \n",
    "#                \"SPIECIES_TEXT\", \"THALAM_TEXT\", \"INJECT_TEXT\", \n",
    "#                \"Macaque?(Y/N)\", \"Thalamus?(Y/N)\", \"Inject?(Y/N)\"]\n",
    "\n",
    "# macaque = 0\n",
    "# thalamus = 0\n",
    "# inject = 0\n",
    "\n",
    "# for ind in df.index:\n",
    "#     if df.at[ind, \"SPIECIES_TEXT\"] != df.at[ind, \"SPIECIES_TEXT\"]:\n",
    "#         macaque += 1\n",
    "#         print(\"No macaque in text!\")\n",
    "#         print(df.at[ind, \"INDEX\"])\n",
    "#         print(df.at[ind, \"FULL_TEXT_URL\"])\n",
    "#         print(\"\\n\")\n",
    "#     if df.at[ind, \"THALAM_TEXT\"] != df.at[ind, \"THALAM_TEXT\"]:\n",
    "#         thalamus += 1\n",
    "#         print(\"No thalamus in text!\")\n",
    "#         print(df.at[ind, \"INDEX\"])\n",
    "#         print(df.at[ind, \"FULL_TEXT_URL\"])\n",
    "#         print(\"\\n\")\n",
    "#     if df.at[ind, \"INJECT_TEXT\"] != df.at[ind, \"INJECT_TEXT\"]:\n",
    "#         inject += 1\n",
    "\n",
    "# print(\"Macaque:\", macaque)\n",
    "# print(\"Thalamus:\", thalamus)\n",
    "# print(\"Inject:\", inject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9493b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from poten_litera_db, count keywords, save to poten_litera_db_kw_count\n",
    "input_path = fpath.poten_litera_db\n",
    "output_path = fpath.poten_litera_db_kw_count\n",
    "\n",
    "# clear file\n",
    "plib.clear_file(output_path)\n",
    "\n",
    "count_and_record_db(input_path, output_path, params.ranking_kw_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26bef318",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 14 elements, new values have 11 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/hou/myProjects/thalamocortical_connectivity_in_macaque/automatic_filtering.ipynb Cell 18\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hou/myProjects/thalamocortical_connectivity_in_macaque/automatic_filtering.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# clear file\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hou/myProjects/thalamocortical_connectivity_in_macaque/automatic_filtering.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m plib\u001b[39m.\u001b[39mclear_file(output_path)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hou/myProjects/thalamocortical_connectivity_in_macaque/automatic_filtering.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m rank(input_path, output_path, params\u001b[39m.\u001b[39;49mranking_kw_groups_weights)\n",
      "\u001b[1;32m/home/hou/myProjects/thalamocortical_connectivity_in_macaque/automatic_filtering.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hou/myProjects/thalamocortical_connectivity_in_macaque/automatic_filtering.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrank\u001b[39m(db_path, db_ranked_path, ranking_params_weights):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hou/myProjects/thalamocortical_connectivity_in_macaque/automatic_filtering.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(db_path, header\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hou/myProjects/thalamocortical_connectivity_in_macaque/automatic_filtering.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     df\u001b[39m.\u001b[39;49mcolumns \u001b[39m=\u001b[39m df_col\u001b[39m.\u001b[39mdb_columns\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hou/myProjects/thalamocortical_connectivity_in_macaque/automatic_filtering.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mfor\u001b[39;00m ind \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39mindex:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hou/myProjects/thalamocortical_connectivity_in_macaque/automatic_filtering.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         count_dict \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/pandas/core/generic.py:6002\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   6000\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   6001\u001b[0m     \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(\u001b[39mself\u001b[39m, name)\n\u001b[0;32m-> 6002\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__setattr__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name, value)\n\u001b[1;32m   6003\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m   6004\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/pandas/_libs/properties.pyx:69\u001b[0m, in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/pandas/core/generic.py:730\u001b[0m, in \u001b[0;36mNDFrame._set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[39mThis is called from the cython code when we set the `index` attribute\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39mdirectly, e.g. `series.index = [1, 2, 3]`.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    729\u001b[0m labels \u001b[39m=\u001b[39m ensure_index(labels)\n\u001b[0;32m--> 730\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mset_axis(axis, labels)\n\u001b[1;32m    731\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/pandas/core/internals/managers.py:225\u001b[0m, in \u001b[0;36mBaseBlockManager.set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_axis\u001b[39m(\u001b[39mself\u001b[39m, axis: AxisInt, new_labels: Index) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     \u001b[39m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_set_axis(axis, new_labels)\n\u001b[1;32m    226\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis] \u001b[39m=\u001b[39m new_labels\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/pandas/core/internals/base.py:70\u001b[0m, in \u001b[0;36mDataManager._validate_set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39melif\u001b[39;00m new_len \u001b[39m!=\u001b[39m old_len:\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     71\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLength mismatch: Expected axis has \u001b[39m\u001b[39m{\u001b[39;00mold_len\u001b[39m}\u001b[39;00m\u001b[39m elements, new \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalues have \u001b[39m\u001b[39m{\u001b[39;00mnew_len\u001b[39m}\u001b[39;00m\u001b[39m elements\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 14 elements, new values have 11 elements"
     ]
    }
   ],
   "source": [
    "# read from poten_litera_db_kw_count, rank the candidate articles, save to poten_litera_db_ranked\n",
    "input_path = fpath.poten_litera_db_kw_count\n",
    "output_path = fpath.poten_litera_db_ranked\n",
    "\n",
    "# clear file\n",
    "plib.clear_file(output_path)\n",
    "\n",
    "rank(input_path, output_path, params.ranking_kw_groups_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36685b74",
   "metadata": {},
   "source": [
    "#### 2. Ranking results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e384b1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read the ranked database and obtain the relevance_index of YESs and NOs \n",
    "# # of the test data set and draw a violin plot, and calculate the difference between the two distributions\n",
    "# # the difference is defined as 1. t-statistic 2. \n",
    "# db_ranked_path = fpath.poten_litera_db_ranked\n",
    "# test_path = fpath.poten_litera_testing_set_1000_read\n",
    "\n",
    "# df_db_ranked = pd.read_csv(db_ranked_path, header=0, sep=',')\n",
    "# df_db_ranked.columns = [\n",
    "#     \"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \n",
    "#     \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \n",
    "#     \"SPECIES_RELATED\", \"TC_CT_RELATED\", \"THALAM_RELATED\", \"CORTEX_RELATED\", \"METHOD_RELATED\", \"CONNECTIVITY_RELATED\",\n",
    "#     \"RELEVANCE_INDEX\"]\n",
    "\n",
    "# df_test = pd.read_csv(test_path, header=0, sep=',')\n",
    "# df_test.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"RELEVANT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa55d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the counts of the keywords in the respective lists\n",
    "# relevant_species = []\n",
    "# relevant_tc_ct = []\n",
    "# relevant_thalam = []\n",
    "# relevant_cortex = []\n",
    "# relevant_method = []\n",
    "# relevant_connectivity = []\n",
    "\n",
    "# non_relevant_species = []\n",
    "# non_relevant_tc_ct = []\n",
    "# non_relevant_thalam = []\n",
    "# non_relevant_cortex = []\n",
    "# non_relevant_method = []\n",
    "# non_relevant_connectivity = []\n",
    "\n",
    "# relvant_index = []\n",
    "# relevant_relevance_index_list = []\n",
    "\n",
    "# non_relevant_index = []\n",
    "# non_relevant_relevance_index_list = []\n",
    "\n",
    "# for ind in df_test.index:\n",
    "#     index = int(df_test.at[ind, \"INDEX\"])\n",
    "#     # print(ind, index)\n",
    "#     # print(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "\n",
    "#     # if df_test.at[ind, \"RELEVANT\"] == \"YES\" and df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"LENGTH_TEXT\"].values[0] > 100:\n",
    "#     if df_test.at[ind, \"RELEVANT\"] == \"YES\":\n",
    "#         relvant_index.append(index)\n",
    "#         relevant_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "#         relevant_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"SPECIES_RELATED\"].values[0])\n",
    "#         relevant_tc_ct.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"TC_CT_RELATED\"].values[0])\n",
    "#         relevant_thalam.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"THALAM_RELATED\"].values[0])\n",
    "#         relevant_cortex.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CORTEX_RELATED\"].values[0])\n",
    "#         relevant_method.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"METHOD_RELATED\"].values[0])\n",
    "#         relevant_connectivity.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CONNECTIVITY_RELATED\"].values[0])\n",
    "#     # elif df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"LENGTH_TEXT\"].values[0] > 100:\n",
    "#     else:\n",
    "#         non_relevant_index.append(index)\n",
    "#         non_relevant_relevance_index_list.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"RELEVANCE_INDEX\"].values[0])\n",
    "#         non_relevant_species.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"SPECIES_RELATED\"].values[0])\n",
    "#         non_relevant_tc_ct.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"TC_CT_RELATED\"].values[0])\n",
    "#         non_relevant_thalam.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"THALAM_RELATED\"].values[0])\n",
    "#         non_relevant_cortex.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CORTEX_RELATED\"].values[0])\n",
    "#         non_relevant_method.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"METHOD_RELATED\"].values[0])\n",
    "#         non_relevant_connectivity.append(df_db_ranked.loc[df_db_ranked[\"INDEX\"].apply(int) == index, \"CONNECTIVITY_RELATED\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1821b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(relvant_index)):\n",
    "#     print(relvant_index[i], relevant_relevance_index_list[i])\n",
    "#     # print(relevant_relevance_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d5dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the dot plot of the relevance_index of YESs and NOs of the test data set\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(relvant_index, relevant_relevance_index_list, 'ro', label=\"YES\")\n",
    "# plt.plot(non_relevant_index, non_relevant_relevance_index_list, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Relevance Index\")\n",
    "# plt.legend()\n",
    "\n",
    "# # add labels for the relevant index points\n",
    "# for i, index in enumerate(relvant_index):\n",
    "#     plt.text(index, relevant_relevance_index_list[i]+0.1, str(index), color='black', fontsize=10)\n",
    "\n",
    "# # # add labels for the relevant index points\n",
    "# # for i, index in enumerate(non_relevant_index):\n",
    "# #     plt.text(index, non_relevant_relevance_index_list[i], str(index), color='black', fontsize=10)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed550a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the 6 dot plots of the species_related, tc_ct_related, thalam_related, cortex_related, method_related, connectivity_related of YESs and NOs of the test data set in 2 rows in the same figure\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.subplot(3, 2, 1)\n",
    "# plt.plot(relvant_index, relevant_species, 'ro', label=\"YES\")\n",
    "# plt.plot(non_relevant_index, non_relevant_species, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Species Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(3, 2, 2)\n",
    "# plt.plot(relvant_index, relevant_tc_ct, 'ro', label=\"YES\")\n",
    "# plt.plot(non_relevant_index, non_relevant_tc_ct, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"TC_CT Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(3, 2, 3)\n",
    "# plt.plot(relvant_index, relevant_thalam, 'ro', label=\"YES\")\n",
    "# plt.plot(non_relevant_index, non_relevant_thalam, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Thalam Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(3, 2, 4)\n",
    "# plt.plot(relvant_index, relevant_cortex, 'ro', label=\"YES\")\n",
    "# plt.plot(non_relevant_index, non_relevant_cortex, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Cortex Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(3, 2, 5)\n",
    "# plt.plot(relvant_index, relevant_method, 'ro', label=\"YES\")\n",
    "# plt.plot(non_relevant_index, non_relevant_method, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Method Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(3, 2, 6)\n",
    "# plt.plot(relvant_index, relevant_connectivity, 'ro', label=\"YES\")\n",
    "# plt.plot(non_relevant_index, non_relevant_connectivity, 'bo', label=\"NO\")\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Connectivity Related\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6260d21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pick_values_uniformly(data, n):\n",
    "#     \"\"\"Pick up `n` values uniformly from `data`.\"\"\"\n",
    "#     if n <= 0:\n",
    "#         return []\n",
    "\n",
    "#     # Determine the range of the data\n",
    "#     min_val, max_val = min(data), max(data)\n",
    "\n",
    "#     threshold = (max_val - min_val) / n / 2\n",
    "\n",
    "#     # If n is 1, just return the midpoint\n",
    "#     if n == 1:\n",
    "#         return [(min_val + max_val) / 2]\n",
    "\n",
    "#     # Calculate the interval size\n",
    "#     interval = (max_val - min_val) / (n - 1)\n",
    "\n",
    "#     # Get the uniform values\n",
    "#     return [min_val + i * interval for i in range(n)], threshold\n",
    "\n",
    "# # data = [1, 3, 5, 2, 8, 10, 2]\n",
    "# n = 5\n",
    "# density_display_index, thres = pick_values_uniformly(relevant_relevance_index_list + non_relevant_relevance_index_list, n)\n",
    "# print(density_display_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73da3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Printing the length of lists\n",
    "# print(\"Numer of relevant literature:\", len(relevant_relevance_index_list))\n",
    "# print(\"Number of not relevant literature:\", len(non_relevant_relevance_index_list))\n",
    "# print()\n",
    "\n",
    "# # Create a DataFrame for plotting\n",
    "# df = pd.DataFrame({'Relevance Index': relevant_relevance_index_list + non_relevant_relevance_index_list, \n",
    "#                    'Label': ['Relevant'] * len(relevant_relevance_index_list) + ['Not Relevant'] * len(non_relevant_relevance_index_list)})\n",
    "\n",
    "# # Draw the violin plot\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# ax = sns.violinplot(x='Label', y='Relevance Index', data=df, bw='scott', cut=0)\n",
    "\n",
    "# relevance_indices = density_display_index  # Replace with your relevance indices\n",
    "\n",
    "# threshold = thres  # Adjust this based on your desired range around the relevance index\n",
    "\n",
    "# for index in relevance_indices:\n",
    "#     ax.axhline(index, color='gray', linestyle='--')\n",
    "    \n",
    "#     for i, label in enumerate(df['Label'].unique()):\n",
    "#         # Filter data points close to the current relevance index\n",
    "#         close_points = df[(df['Label'] == label) & (np.abs(df['Relevance Index'] - index) < threshold)]\n",
    "#         density = len(close_points)\n",
    "        \n",
    "#         ax.text(i, index + 0.1, str(density), ha='center', va='center', color='red', fontsize=9)  # adjust the vertical offset (0.1 here) as necessary\n",
    "\n",
    "# plt.title('Distribution of Relevance Index')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.stats as stats\n",
    "\n",
    "# # Check the equality of variances\n",
    "# var_relevant = np.var(relevant_relevance_index_list)\n",
    "# var_non_relevant = np.var(non_relevant_relevance_index_list)\n",
    "# print('Variance of relevant:', var_relevant)\n",
    "# print('Variance of non-relevant:', var_non_relevant)\n",
    "# print(var_relevant/var_non_relevant)\n",
    "# # statistic, p_value = stats.levene(relevant_relevance_index_list, non_relevant_relevance_index_list)\n",
    "\n",
    "# # # Print the results\n",
    "# # print('Levene test statistic:', statistic)\n",
    "# # print('p-value:', p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0804ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the t-statistic and p-value\n",
    "# t_statistic, p_value = stats.ttest_ind(relevant_relevance_index_list, non_relevant_relevance_index_list)\n",
    "\n",
    "# # Print the results\n",
    "# print('t-statistic:', t_statistic)\n",
    "# print('p-value:', p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc9ccf",
   "metadata": {},
   "source": [
    "<h3> Next step: manually read papers and find all actually related literature </h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
