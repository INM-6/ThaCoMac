{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Searched literature data preprocessing </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-22 01:26:27 Didis-MacBook-Pro.local metapub.config[6338] WARNING NCBI_API_KEY was not set.\n"
     ]
    }
   ],
   "source": [
    "# import internal modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib\n",
    "import extract_info_from_webpage as extra_info\n",
    "import parameters as params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Parameters: </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns of file: potential_related_literature.csv\n",
    "columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Predefined fucntions: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pubmed(source_path, output_path, start, end):\n",
    "    print(\"Starting preprocessing search results from PubMed...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, sep=',')\n",
    "    df = df[[\"DOI\", \"PMID\", \"PMCID\", \"Title\"]]\n",
    "    \n",
    "    for ind in range(start, end):\n",
    "        # sleep to avoid to be blocked\n",
    "        time.sleep(random.randint(1, 3))\n",
    "        \n",
    "        # request the webpage\n",
    "        # the columns PMID, Title don't contain np.nan\n",
    "        pmid = str(df[\"PMID\"][ind]).strip()\n",
    "        url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "        # proxies = plib.get_proxies()\n",
    "        soup = plib.request_webpage(url)\n",
    "        # print(soup)\n",
    "        \n",
    "        # get pmcid\n",
    "        if df[\"PMCID\"][ind] != df[\"PMCID\"][ind]: # PMCID is np.nan\n",
    "            try:\n",
    "                pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                pmcid = np.nan\n",
    "        else: # PMCID is not np.nan\n",
    "            pmcid = str(df[\"PMCID\"][ind]).strip()\n",
    "        # print(pmcid)\n",
    "\n",
    "        # get doi\n",
    "        if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # DOI is np.nan\n",
    "            try:\n",
    "                doi = soup.find_all(\"span\", {\"class\": \"identifier doi\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                doi  = np.nan\n",
    "        else: # DOI is not np.nan\n",
    "            doi = str(df[\"DOI\"][ind]).strip()\n",
    "        # print(doi)\n",
    "\n",
    "        # get full_text_url\n",
    "        # if pmcid == pmcid: # pmcid is not np.nan\n",
    "        #     full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "        #     full_text_source = \"PMC\"\n",
    "        # else: # pmcid is np.nan\n",
    "        #     # PMC does not include this paper\n",
    "        #     try:\n",
    "        #         full_text_url = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"href\"].strip()\n",
    "        #         full_text_source = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"data-ga-action\"].strip()\n",
    "        #     except:\n",
    "        #         full_text_url = np.nan\n",
    "        #         full_text_source = np.nan\n",
    "        full_text_url = np.nan\n",
    "        # print(full_text_url)\n",
    "        \n",
    "        # get pdf_url\n",
    "        pdf_url = np.nan\n",
    "        title = (df.at[ind, \"Title\"]).strip()\n",
    "        abstract = np.nan\n",
    "        keywords = np.nan\n",
    "        \n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"Title\": [title],\n",
    "            \"Abstract\": [abstract],\n",
    "            \"Keywords\": [keywords]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_pubmed\n",
    "# output_path = fpath.poten_litera_pubmed_processed\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, sep=',')\n",
    "# print(df.shape)\n",
    "# # (2612, 11)\n",
    "# df = df[[\"DOI\", \"PMID\", \"PMCID\", \"Title\"]]\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "\n",
    "# print(df[\"DOI\"].isnull().values.any())\n",
    "# print(df[\"PMID\"].isnull().values.any())\n",
    "# print(df[\"PMCID\"].isnull().values.any())\n",
    "# print(df[\"Title\"].isnull().values.any())\n",
    "# # True, False, True, Flase\n",
    "# # PMID, Title don't contain np.nan\n",
    "# # DOI, PMCID contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_pubmed(source_path, output_path, start, end)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_webofscience(source_path, output_path, start, end):\n",
    "    print(\"Starting preprocessing search results from Web of Science...\")\n",
    "    \n",
    "    df = pd.read_csv(source_path, sep=\",\")\n",
    "    df = df[[\"DOI\", \"Pubmed Id\", \"Article Title\", \"Abstract\", \"Author Keywords\", \"Keywords Plus\"]]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # sleep to avoid to be blocked\n",
    "        time.sleep(random.randint(1, 3))\n",
    "        \n",
    "        # the columns Article Title don't contain np.nan\n",
    "        # the columns DOI and PMID might contain np.nan\n",
    "        # get pmid, doi\n",
    "        if df[\"Pubmed Id\"][ind] != df[\"Pubmed Id\"][ind]: # Pubmed Id is np.nan\n",
    "            if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # DOI is np.nan\n",
    "                doi = np.nan\n",
    "                pmid = np.nan\n",
    "            else: # DOI is not np.nan\n",
    "                doi = str(df[\"DOI\"][ind]).strip()\n",
    "                pmid = plib.doi2pmid(doi)\n",
    "        else: # Pubmed Id is not np.nan\n",
    "            pmid = str(int(df[\"Pubmed Id\"][ind])).strip()\n",
    "            if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # DOI is not np.nan\n",
    "                doi, a = plib.pmid2doi_pmcid(pmid)\n",
    "            else: # DOI is not np.nan\n",
    "                doi = str(df[\"DOI\"][ind]).strip()\n",
    "        \n",
    "        # get pmcid\n",
    "        if pmid != pmid: # pmid is np.nan\n",
    "            pmcid = np.nan\n",
    "            # if doi != doi: # doi is np.nan\n",
    "            #     full_text_url = np.nan\n",
    "            #     full_text_source = np.nan\n",
    "            # else:\n",
    "            #     full_text_url = \"https://doi.org/\" + str(doi).strip()\n",
    "            #     full_text_source = \"DOI\"\n",
    "        else: # pmid is not np.nan\n",
    "            # request the webpage\n",
    "            url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "            soup = plib.request_webpage(url)\n",
    "            # print(soup)\n",
    "\n",
    "            # get pmcid\n",
    "            try:\n",
    "                pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                pmcid = np.nan\n",
    "            # print(pmcid)\n",
    "            \n",
    "            # get full_text_url\n",
    "            # if pmcid == pmcid:\n",
    "            #     full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "            #     full_text_source = \"PMC\"\n",
    "            # else:\n",
    "            #     try:\n",
    "            #         full_text_url = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"href\"].strip()\n",
    "            #         full_text_source = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"data-ga-action\"].strip()\n",
    "            #     except:\n",
    "            #         full_text_url = np.nan\n",
    "            #         full_text_source = np.nan\n",
    "        \n",
    "        full_text_url = np.nan\n",
    "        pdf_url = np.nan\n",
    "        title = str(df[\"Article Title\"][ind]).strip()\n",
    "        abstract = str(df[\"Abstract\"][ind]).strip()\n",
    "        keywords = str(df[\"Author Keywords\"][ind]).strip() + \"; \" + str(df[\"Keywords Plus\"][ind]).strip()\n",
    "\n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"Title\": [title],\n",
    "            \"Abstract\": [abstract],\n",
    "            \"Keywords\": [keywords]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# # source_path = fpath.poten_litera_wos\n",
    "# # output_path = fpath.poten_litera_wos_processed\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, sep=';')\n",
    "# df = df[[\"DOI\", \"Pubmed Id\", \"Article Title\", \"Abstract\", \"Author Keywords\", \"Keywords Plus\"]]\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "\n",
    "# print(df[\"DOI\"].isnull().values.any())\n",
    "# print(df[\"Pubmed Id\"].isnull().values.any())\n",
    "# print(df[\"Article Title\"].isnull().values.any())\n",
    "# print(df[\"Abstract\"].isnull().values.any())\n",
    "# print(df[\"Author Keywords\"].isnull().values.any())\n",
    "# print(df[\"Keywords Plus\"].isnull().values.any())\n",
    "# # True, True, False\n",
    "# # Article Title don't contain np.nan\n",
    "# # DOI, Pubmed Id contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code--------------------- \n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_webofscience(source_path, output_path, 0, 10)\n",
    "# ---------------------end of test code--------------------- \n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=';')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_eupmc(source_path, output_path, start, end):\n",
    "    print(\"Starting preprocessing search results from Europe PMC...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, sep=\",\")\n",
    "    df = df[[\"SOURCE\", \"DOI\", \"EXTERNAL_ID\", \"PMCID\", \"TITLE\"]]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # sleep to avoid to be blocked\n",
    "        time.sleep(random.randint(1, 3))\n",
    "\n",
    "        # get pmid, doi\n",
    "        # SOURCE = {'PMC', 'MED', 'ETH', 'PPR'}\n",
    "        if df[\"SOURCE\"][ind] != \"MED\": # SOURCE is not \"MED\" \n",
    "            if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # doi is np.nan\n",
    "                doi = np.nan\n",
    "                pmid = np.nan\n",
    "            else:\n",
    "                doi = str(df[\"DOI\"][ind]).strip()\n",
    "                pmid = plib.doi2pmid(doi)\n",
    "        else: # SOURCE is \"MED\"\n",
    "            # get doi, pmid\n",
    "            if df[\"EXTERNAL_ID\"][ind] != df[\"EXTERNAL_ID\"][ind]: # EXTERNAL_ID is np.nan\n",
    "                if df[\"DOI\"][ind] != df[\"DOI\"][ind](): # DOI is np.nan\n",
    "                    doi = np.nan\n",
    "                    pmid = np.nan\n",
    "                else: # DOI is not np.nan\n",
    "                    doi = str(df[\"DOI\"][ind]).strip()\n",
    "                    pmid = plib.doi2pmid(doi)\n",
    "            else: # EXTERNAL_ID is not np.nan\n",
    "                pmid = str(df[\"EXTERNAL_ID\"][ind]).strip()\n",
    "                if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # DOI is np.nan\n",
    "                    doi, a = plib.pmid2doi_pmcid(pmid)\n",
    "                else: # DOI is not np.nan\n",
    "                    doi = str(df[\"DOI\"][ind]).strip()\n",
    "                \n",
    "        # get pmcid, full_text_url, full_text_source\n",
    "        if pmid != pmid: # pmid is np.nan\n",
    "            pmcid = df[\"PMCID\"][ind]\n",
    "            # if pmcid == pmcid: # pmcid is np.nan\n",
    "            #     full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "            #     full_text_source = \"PMC\"\n",
    "            # elif doi == doi: # doi is not np.nan\n",
    "            #     full_text_url = \"https://doi.org/\" + str(doi).strip()\n",
    "            #     full_text_source = \"DOI\"\n",
    "            # else:\n",
    "            #     full_text_url = np.nan\n",
    "            #     full_text_source = np.nan\n",
    "        else: # pmid is not np.nan\n",
    "            # request the webpage\n",
    "            url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "            # proxies = plib.get_proxies()\n",
    "            soup = plib.request_webpage(url)\n",
    "            # print(soup)\n",
    "\n",
    "            # get pmcid\n",
    "            try:\n",
    "                pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                pmcid = np.nan\n",
    "            # print(pmcid)\n",
    "            \n",
    "            # get full_text_url\n",
    "            # if pmcid == pmcid: # pmcid is not np.nan\n",
    "            #     full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "            #     full_text_source = \"PMC\"\n",
    "            # else: # pmcid is not np.nan\n",
    "            #     try:\n",
    "            #         full_text_url = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"href\"].strip()\n",
    "            #         full_text_source = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"data-ga-action\"].strip()\n",
    "            #     except:\n",
    "            #         full_text_url = np.nan\n",
    "            #         full_text_source = np.nan\n",
    "        \n",
    "        full_text_url = np.nan\n",
    "        pdf_url = np.nan\n",
    "        title = (df.at[ind, \"TITLE\"]).strip()\n",
    "        abstract = np.nan\n",
    "        keywords = np.nan\n",
    "        \n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"Title\": [title],\n",
    "            \"Abstract\": [abstract],\n",
    "            \"Keywords\": [keywords]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_eupmc\n",
    "# output_path = fpath.poten_litera_eupmc_processed\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, sep=',')\n",
    "# df = df[[\"SOURCE\", \"DOI\", \"EXTERNAL_ID\", \"PMCID\", \"TITLE\"]]\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "\n",
    "# col_one_list = set(df['SOURCE'].tolist())\n",
    "# print(col_one_list)\n",
    "# # ['PMC', 'MED', 'ETH', 'PPR']\n",
    "\n",
    "# print(df[\"SOURCE\"].isnull().values.any())\n",
    "# print(df[\"DOI\"].isnull().values.any())\n",
    "# print(df[\"EXTERNAL_ID\"].isnull().values.any())\n",
    "# print(df[\"PMCID\"].isnull().values.any())\n",
    "# print(df[\"TITLE\"].isnull().values.any())\n",
    "# # False, True, False, True, False\n",
    "# # SOURCE, EXTERNAL_ID, Title don't contain np.nan\n",
    "# # DOI, PMCID contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_eupmc(source_path, output_path, 0, 10)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_google_shcolar_step1(source_path, output_path, start, end):\n",
    "    print(\"Starting merging search results from Google Scholar...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, header=None, sep=',')\n",
    "    df.columns = [\"title\", \"url\", \"url_tag\", \"full_text_url\", \"full_text_tag\"]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # df[\"url_tag\"][ind]: {'[CITATION][C]', '[HTML][HTML]', '[PDF][PDF]', '[BOOK][B]', nan}\n",
    "        # we don't need books, as they are not likely to include connecivity information\n",
    "        if df.at[ind, \"url_tag\"] == \"[BOOK][B]\":\n",
    "            continue\n",
    "        \n",
    "        if (df.at[ind, \"url\"] != df.at[ind, \"url\"] or df.at[ind, \"title\"] != df.at[ind, \"title\"]) and (df.at[ind, \"full_text_tag\"] == \"[PDF]\" or df.at[ind, \"full_text_tag\"] == \"[HTML]\"):\n",
    "            raise Exception(ind, \": url or title are both nan, but full_text_tag is [PDF] or [HTML]!\")\n",
    "\n",
    "        # if url or title doesn't exsit AND full_text_url doesn't exist\n",
    "        if (df.at[ind, \"url\"] != df.at[ind, \"url\"] or df.at[ind, \"title\"] != df.at[ind, \"title\"]):\n",
    "            continue \n",
    "        \n",
    "        title = str(df[\"title\"][ind]).strip()\n",
    "\n",
    "        # now every row has at least title and url\n",
    "        if df[\"url_tag\"][ind] == \"[PDF][PDF]\": # {'[CITATION][C]', '[HTML][HTML]', '[PDF][PDF]', nan}\n",
    "            if df[\"full_text_tag\"][ind] == \"[HTML]\": # {'[PDF]', '[HTML]', nan}\n",
    "                link = str(df[\"full_text_url\"][ind]).strip()\n",
    "                full_text_url, status_code  = plib.get_final_redirected_url(link)\n",
    "                if full_text_url == full_text_url:\n",
    "                    full_text_source = full_text_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                else:\n",
    "                    full_text_source = np.nan\n",
    "            else:\n",
    "                full_text_url = np.nan\n",
    "                full_text_source = np.nan\n",
    "            # get pdf_url, pdf_source\n",
    "            link = str(df[\"url\"][ind]).strip()\n",
    "            pdf_url, status_code = plib.get_final_redirected_url(link)\n",
    "            # if pdf_url == pdf_url:\n",
    "            #     pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "            # else:\n",
    "            #     pdf_source = np.nan\n",
    "        else: # {'[CITATION][C]', '[HTML][HTML]', '[PDF][PDF]', nan}\n",
    "            link = str(df[\"url\"][ind]).strip()\n",
    "            full_text_url, status_code = plib.get_final_redirected_url(link)\n",
    "            if full_text_url == full_text_url:\n",
    "                full_text_source = full_text_url.split(\"://\")[1].split(\"/\")[0]\n",
    "            else:\n",
    "                full_text_source = np.nan\n",
    "            # get pdf_url, pdf_source\n",
    "            if df[\"full_text_tag\"][ind] == \"[PDF]\": # full_text_type = {'[HTML]', nan, '[PDF]'}\n",
    "                link = str(df[\"full_text_url\"][ind]).strip()\n",
    "                pdf_url, status_code  = plib.get_final_redirected_url(link)\n",
    "                # if pdf_url == pdf_url:\n",
    "                #     pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                # else:\n",
    "                #     pdf_source = np.nan\n",
    "            else:\n",
    "                pdf_url = np.nan\n",
    "        \n",
    "        columns = [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\"]\n",
    "        row = {\n",
    "            \"Title\": [title],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"full_text_source\": [full_text_source],\n",
    "            \"pdf_url\": [pdf_url]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_gs\n",
    "# output_path = fpath.poten_litera_gs_processed_step1\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"title\", \"url\", \"url_tag\", \"full_text_url\", \"full_text_tag\"]\n",
    "# # print(df.head(3))\n",
    "# print(df.shape)\n",
    "# # (980, 5)\n",
    "\n",
    "# url_type = set(df['url_tag'].tolist())\n",
    "# print(url_type)\n",
    "# # {'[CITATION][C]', '[HTML][HTML]', '[PDF][PDF]', '[BOOK][B]', nan}\n",
    "# full_text_tag = set(df['full_text_tag'].tolist())\n",
    "# print(full_text_tag)\n",
    "# # {'[PDF]', '[HTML]', nan}\n",
    "# # ---------------------end of test code---------------------\n",
    "\n",
    "# # --------------------start of test code--------------------\n",
    "# # [\"title\", \"url\", \"url_tag\", \"full_text_url\", \"full_text_tag\"]\n",
    "# print(df[\"title\"].isnull().any().any())\n",
    "# print(df[\"url\"].isnull().any().any())\n",
    "# print(df[\"url_tag\"].isnull().any().any())\n",
    "# print(df[\"full_text_url\"].isnull().any().any())\n",
    "# print(df[\"full_text_tag\"].isnull().any().any())\n",
    "# # True, True, True, True, True\n",
    "# # title, url, url_tag, full_text_url, full_text_tag, all contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_google_shcolar_step1(source_path, output_path, 0, 1000)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_google_shcolar_step2(source_path, output_path, start, end):\n",
    "    print(\"Starting merging search results from Google Scholar...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, header=None, sep=',')\n",
    "    df.columns = [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\"]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # get doi from url\n",
    "        if df[\"full_text_url\"][ind] == df[\"full_text_url\"][ind]: # there's a full_text_url\n",
    "            url = str(df[\"full_text_url\"][ind]).strip()\n",
    "            source = url.split(\"://\")[1].split(\"/\")[0]\n",
    "            # check if the full_text_url is one of our websites\n",
    "            flag = False\n",
    "            for website in params.websites_gs:\n",
    "                if website in source:\n",
    "                    flag = True\n",
    "                    break\n",
    "            if not flag:\n",
    "                continue\n",
    "            info = extra_info.extract_info_from_webpage(url, params.websites_gs)\n",
    "            doi = info[\"doi\"]\n",
    "            pmid = info[\"pmid\"]\n",
    "            pmcid = info[\"pmcid\"]\n",
    "        else:\n",
    "            url = np.nan\n",
    "            doi = np.nan\n",
    "            pmid = np.nan\n",
    "            pmcid = np.nan\n",
    "        \n",
    "        # # get pmid from DOI\n",
    "        # if doi == doi: # there's doi\n",
    "        #     pmid = plib.doi2pmid(doi)\n",
    "        # else: # doi not found\n",
    "        #     pmid = np.nan\n",
    "        # # get pmcid, full_text_url, full_text_source\n",
    "        # if pmid != pmid: # pmid is np.nan\n",
    "        #     pmcid = np.nan\n",
    "        #     if doi == doi: # doi is not np.nan\n",
    "        #         full_text_url = \"https://doi.org/\" + str(doi).strip()\n",
    "        #         full_text_source = \"DOI\"\n",
    "        #     else:\n",
    "        #         full_text_url = np.nan\n",
    "        #         full_text_source = np.nan\n",
    "        # else: # pmid is not np.nan\n",
    "        #     # request the webpage\n",
    "        #     url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "        #     # proxies = plib.get_proxies()\n",
    "        #     soup = plib.request_webpage(url)\n",
    "        #     # print(soup)\n",
    "\n",
    "        #     # get pmcid\n",
    "        #     try:\n",
    "        #         pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "        #     except:\n",
    "        #         pmcid = np.nan\n",
    "        #     # print(pmcid)\n",
    "            \n",
    "        #     # get full_text_url, full_text_source\n",
    "        #     if pmcid == pmcid: # pmcid is not np.nan\n",
    "        #         full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "        #         full_text_source = \"PMC\"\n",
    "        #     else: # pmcid is not np.nan\n",
    "        #         try:\n",
    "        #             full_text_url = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"href\"].strip()\n",
    "        #             full_text_source = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"data-ga-action\"].strip()\n",
    "        #         except:\n",
    "        #             full_text_url = np.nan\n",
    "        #             full_text_source = np.nan\n",
    "        \n",
    "        full_text_url = url\n",
    "        pdf_url = df.at[ind, \"pdf_url\"]\n",
    "        title = df.at[ind, \"Title\"]\n",
    "        abstract = np.nan\n",
    "        keywords = np.nan\n",
    "\n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"Title\": [title],\n",
    "            \"Abstract\": [abstract],\n",
    "            \"Keywords\": [keywords]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(doi)\n",
    "        if doi != doi:\n",
    "            print([df[\"full_text_url\"][ind]])\n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_gs_processed_step1\n",
    "# output_path = fpath.poten_litera_gs_processed_step2\n",
    "# plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\"]\n",
    "# # print(df.head(3))\n",
    "# print(df.shape)\n",
    "# # (926, 4)\n",
    "# full_text_source = set(df['full_text_source'].tolist())\n",
    "# print(full_text_source)\n",
    "# # {'www.elibrary.ru', 'n.neurology.org', 'jnnp.bmj.com', 'anatomypubs.onlinelibrary.wiley.com', 'academic.oup.com', \n",
    "# #  'nyaspubs.onlinelibrary.wiley.com', 'cir.nii.ac.jp', 'link.springer.com', 'www.mdpi.com', 'pure.mpg.de', \n",
    "# #  'bmcneurosci.biomedcentral.com', 'elibrary.ru', 'journals.sagepub.com', 'tbiomed.biomedcentral.com', \n",
    "# #  'onlinelibrary.wiley.com', 'www.cambridge.org', 'wakespace.lib.wfu.edu', nan, 'www.cell.com', 'europepmc.org', \n",
    "# #  'var.scholarpedia.org', 'jpet.aspetjournals.org', 'journal.psych.ac.cn', 'www.biorxiv.org', 'ieeexplore.ieee.org', \n",
    "# #  'www.jstor.org', 'www.cabdirect.org', 'royalsocietypublishing.org', 'analyticalsciencejournals.onlinelibrary.wiley.com', \n",
    "# #  'open.bu.edu', 'journals.lww.com', 'www.eneuro.org', 'www.jstage.jst.go.jp', 'journals.plos.org', 'www.ncbi.nlm.nih.gov', \n",
    "# #  'www.liebertpub.com', 'neuro.psychiatryonline.org', 'www.sciencedirect.com', 'psycnet.apa.org', 'www.taylorfrancis.com', \n",
    "# #  'www.degruyter.com', 'www.nature.com', 'jamanetwork.com', 'karger.com', 'www.tandfonline.com', 'journals.physiology.org', \n",
    "# #  'movementdisorders.onlinelibrary.wiley.com', 'www.pnas.org', 'www.jneurosci.org', 'thejns.org', 'pascal-francis.inist.fr', \n",
    "# #  'physoc.onlinelibrary.wiley.com', 'agro.icm.edu.pl', 'elifesciences.org', 'www.frontiersin.org', 'escholarship.mcgill.ca', \n",
    "# #  'ajp.psychiatryonline.org', 'www.science.org', 'books.google.de'}\n",
    "\n",
    "# # {'elibrary.ru', 'neurology.org', 'bmj.com', 'wiley.com', 'oup.com', 'cir.nii.ac.jp', 'springer.com', 'mdpi.com', 'mpg.de', \n",
    "# #  'biomedcentral.com', 'sagepub.com', 'cambridge.org', 'wfu.edu', nan, 'cell.com', 'europepmc.org', 'scholarpedia.org', \n",
    "# #  'aspetjournals.org', 'psych.ac.cn', 'biorxiv.org', 'ieee.org', 'jstor.org', 'cabdirect.org', 'royalsocietypublishing.org', \n",
    "# #  'bu.edu', 'lww.com', 'eneuro.org', 'jst.go.jp', 'plos.org', 'ncbi.nlm.nih.gov', 'liebertpub.com', 'psychiatryonline.org', \n",
    "# #  'sciencedirect.com', 'psycnet.apa.org', 'taylorfrancis.com', 'degruyter.com', 'nature.com', 'jamanetwork.com', \n",
    "# #  'karger.com', 'www.tandfonline.com', 'physiology.org', 'www.pnas.org', 'jneurosci.org', 'thejns.org', \n",
    "# #  'pascal-francis.inist.fr', 'agro.icm.edu.pl', 'elifesciences.org', 'frontiersin.org', 'mcgill.ca', \n",
    "# #  'science.org', 'books.google.de'}\n",
    "\n",
    "# # websites_gs = {\n",
    "# #     'neurology.org', 'bmj.com', 'wiley.com', 'oup.com', 'springer.com', 'mdpi.com', \n",
    "# #     'biomedcentral.com', 'sagepub.com', 'cambridge.org', 'wfu.edu', 'cell.com', 'europepmc.org', \n",
    "# #     'aspetjournals.org', 'psych.ac.cn', 'biorxiv.org', 'ieee.org', 'jstor.org', 'royalsocietypublishing.org', \n",
    "# #     'bu.edu', 'lww.com', 'eneuro.org', 'jst.go.jp', 'plos.org', 'ncbi.nlm.nih.gov', 'liebertpub.com', \n",
    "# #     'psychiatryonline.org', 'sciencedirect.com', 'psycnet.apa.org', 'degruyter.com', 'nature.com', 'jamanetwork.com', \n",
    "# #     'karger.com', 'tandfonline.com', 'physiology.org', 'pnas.org', 'jneurosci.org', 'thejns.org', \n",
    "# #     'agro.icm.edu.pl', 'elifesciences.org', 'frontiersin.org', 'science.org'}\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# # [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\"]\n",
    "# print(df[\"Title\"].isnull().any().any())\n",
    "# print(df[\"full_text_url\"].isnull().any().any())\n",
    "# print(df[\"full_text_source\"].isnull().any().any())\n",
    "# print(df[\"pdf_url\"].isnull().any().any())\n",
    "# # False, True, True, True\n",
    "# # full_text_url, full_text_source, pdf_url contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_google_shcolar_step2(source_path, output_path, 0, 905)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_seed_paper_spanning(source_path, output_path):\n",
    "    print(\"Starting preprocessing search results from spanning citations of seed paper...\")\n",
    "    return True\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cocomac_paper(source_path, output_path):\n",
    "    print(\"Starting preprocessing search results from CoCoMac papers...\")\n",
    "    return True\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(input, output_path):\n",
    "    # combine all results\n",
    "    df = pd.DataFrame()\n",
    "    for search_result in input:\n",
    "        df_single = pd.read_csv(search_result, header=None, sep = \",\")\n",
    "        # df = df.append(df_single, ignore_index=True, sort=False)\n",
    "        df = pd.concat([df, df_single], ignore_index=True, sort=False)\n",
    "    df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.to_csv(output_path, header=False, index=False)\n",
    "# --------------------start of test code--------------------\n",
    "# gos = fpath.poten_litera_gs_processed_step2\n",
    "# wos = fpath.poten_litera_wos_processed\n",
    "# pubmed = fpath.poten_litera_pubmed_processed\n",
    "# eupmc = fpath.poten_litera_eupmc_processed\n",
    "# input = [gos, wos, pubmed, eupmc]\n",
    "# output_path = fpath.poten_litera_combined\n",
    "# # plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# combine(input, output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "# # (14627, 8)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_in_identifiers(input_path, output_path, start, end):\n",
    "    df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "    df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "    \n",
    "    # fill in elements that are missing\n",
    "    for ind in range(start, end):\n",
    "        # if all 3 identifiers are missing, and full_text_url and pdf_url are missing, skip\n",
    "        if df.at[ind, \"DOI\"] != df.at[ind, \"DOI\"] and df.at[ind, \"PMID\"] != df.at[ind, \"PMID\"] and df.at[ind, \"PMCID\"] != df.at[ind, \"PMCID\"] and df.at[ind, \"full_text_url\"] != df.at[ind, \"full_text_url\"] and df.at[ind, \"pdf_url\"] != df.at[ind, \"pdf_url\"]:\n",
    "            continue\n",
    "        \n",
    "        # initialzie\n",
    "        doi = np.nan\n",
    "        pmid = np.nan\n",
    "        pmcid = np.nan\n",
    "        full_text_url = df.at[ind, \"full_text_url\"]\n",
    "        pdf_url = df.at[ind, \"pdf_url\"]\n",
    "        title = df.at[ind, \"Title\"]\n",
    "        abstract = df.at[ind, \"Abstract\"]\n",
    "        keywords = df.at[ind, \"Keywords\"]\n",
    "\n",
    "        # if all 3 identifiers are missing, and full_text_url is missing\n",
    "        if df.at[ind, \"DOI\"] != df.at[ind, \"DOI\"] and df.at[ind, \"PMID\"] != df.at[ind, \"PMID\"] and df.at[ind, \"PMCID\"] != df.at[ind, \"PMCID\"]:\n",
    "            columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "            row = {\n",
    "                \"DOI\": [doi],\n",
    "                \"PMID\": [pmid],\n",
    "                \"PMCID\": [pmcid],\n",
    "                \"full_text_url\": [full_text_url],\n",
    "                \"pdf_url\": [pdf_url],\n",
    "                \"Title\": [title],\n",
    "                \"Abstract\": [abstract],\n",
    "                \"Keywords\": [keywords]\n",
    "            }\n",
    "\n",
    "            if not plib.add_row_to_csv(output_path, row, columns):\n",
    "                print(\"Error detected when adding a row to csv!\")\n",
    "\n",
    "            print(ind)\n",
    "            continue\n",
    "        \n",
    "        # we have at least one of the 3 identifiers\n",
    "        # doi, pmid\n",
    "        if df[\"DOI\"][ind] == df[\"DOI\"][ind]: # DOI -> PMID\n",
    "            doi = str(df[\"DOI\"][ind]).strip().lower()\n",
    "            # print(doi)\n",
    "            if df[\"PMID\"][ind] == df[\"PMID\"][ind]:\n",
    "                pmid = str(df[\"PMID\"][ind]).strip()\n",
    "                # print(pmid)\n",
    "            else:\n",
    "                pmid = plib.doi2pmid(doi)\n",
    "                # print(pmid)\n",
    "                if pmid != pmid:\n",
    "                    pmid_cadidate = plib.title2pmid(title)\n",
    "                    # print(pmid_cadidate)\n",
    "                    if pmid_cadidate == pmid_cadidate:   \n",
    "                        doi_validate, a = plib.pmid2doi_pmcid(pmid_cadidate)\n",
    "                        if doi_validate == doi_validate:\n",
    "                            doi_validate = doi_validate.lower()\n",
    "                            if doi_validate == doi:\n",
    "                                pmid = pmid_cadidate\n",
    "                                # print(pmid)\n",
    "        elif df[\"PMID\"][ind] == df[\"PMID\"][ind]: # PMID -> DOI\n",
    "            pmid = str(int(df[\"PMID\"][ind])).strip()\n",
    "            # print(pmid)\n",
    "            doi, pmcid = plib.pmid2doi_pmcid(pmid)\n",
    "            # print(doi)\n",
    "        elif df[\"PMCID\"][ind] == df[\"PMCID\"][ind]: # PMCID -> DOI, PMID\n",
    "            pmcid = str(df[\"PMCID\"][ind]).strip()\n",
    "            try:\n",
    "                doi, pmid = plib.pmcid2doi_pmid(pmcid)\n",
    "            except:\n",
    "                doi = np.nan\n",
    "                pmid = np.nan\n",
    "            # print(doi)\n",
    "            # print(pmid)\n",
    "        else:\n",
    "            doi = np.nan\n",
    "            pmid = np.nan\n",
    "        # print(doi)\n",
    "        # print(pmid)\n",
    "        \n",
    "        # pmcid\n",
    "        if df[\"PMCID\"][ind] == df[\"PMCID\"][ind]:\n",
    "            pmcid = str(df[\"PMCID\"][ind]).strip()\n",
    "        elif pmid == pmid:\n",
    "            a, pmcid = plib.pmid2doi_pmcid(pmid)\n",
    "        else:\n",
    "            pmcid = np.nan\n",
    "        # print(pmcid)\n",
    "\n",
    "        # full_text_url\n",
    "        # if pmcid == pmcid:\n",
    "        #     full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "        # elif doi == doi:\n",
    "        #     full_text_url = plib.get_final_redirected_url(str(\"https://doi.org/\" + doi).strip())\n",
    "        # elif df[\"full_text_url\"][ind] == df[\"full_text_url\"][ind]:\n",
    "        #     full_text_url = plib.get_final_redirected_url(df[\"full_text_url\"][ind])\n",
    "        # else:\n",
    "        #     full_text_url = np.nan\n",
    "        # print(full_text_url)\n",
    "\n",
    "        # pdf_url\n",
    "        # if df[\"pdf_url\"][ind] == df[\"pdf_url\"][ind]:\n",
    "        #     pdf_url = plib.get_final_redirected_url(str(df[\"pdf_url\"][ind]).strip())\n",
    "        # else:\n",
    "        #     pdf_url = np.nan\n",
    "        # print(pdf_url)\n",
    "    \n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"Title\": [title],\n",
    "            \"Abstract\": [abstract],\n",
    "            \"Keywords\": [keywords]\n",
    "        }\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "\n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# input_path = fpath.poten_litera_combined\n",
    "# # output_path = fpath.poten_litera_filled\n",
    "# # plib.clear_file(output_path)\n",
    "# df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "# print(df.shape)\n",
    "# df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "# print(df[\"DOI\"].isnull().any().any())\n",
    "# print(df[\"PMID\"].isnull().any().any())\n",
    "# print(df[\"PMCID\"].isnull().any().any())\n",
    "# print(df[\"full_text_url\"].isnull().any().any())\n",
    "# print(df[\"pdf_url\"].isnull().any().any())\n",
    "# print(df[\"Title\"].isnull().any().any())\n",
    "# print(df[\"Abstract\"].isnull().any().any())\n",
    "# print(df[\"Keywords\"].isnull().any().any())\n",
    "# True, True, True, True, True, False, True, True\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# fill_in_identifiers(input_path, output_path, 0, 14690)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_remove_dupli(input_path, output_path, identifiers): \n",
    "    df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "    df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "\n",
    "    # remove all duplicates\n",
    "    for identifier in identifiers:\n",
    "        remove_dup_by = identifier\n",
    "        df = df[df[remove_dup_by].isnull() | ~df[df[remove_dup_by].notnull()].duplicated(subset=remove_dup_by, keep='first')]\n",
    "        # df = df.drop_duplicates(subset=['DOI'])\n",
    "        # df = df.drop_duplicates(subset=['PMID'])\n",
    "        # df = df.drop_duplicates(subset=['PMCID'])\n",
    "\n",
    "    # reset index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df.to_csv(output_path, header=False, index=False)\n",
    "    print(\"Duplication in the potential related literature removed.\")\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_ids_filled\n",
    "# output_path = fpath.poten_litra_filtered\n",
    "# plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# merge all search results\n",
    "# identifiers = [\"DOI\", \"PMID\", \"PMCID\"]\n",
    "# merge_remove_dupli(source_path, output_path, identifiers)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_text_url_filling(input_path, output_path, start, end):\n",
    "    df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "    df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "    \n",
    "    for ind in range(start, end):\n",
    "        doi = df.at[ind, \"DOI\"]\n",
    "        pmid = df.at[ind, \"PMID\"]\n",
    "        pmcid = df.at[ind, \"PMCID\"]\n",
    "        full_text_url = np.nan\n",
    "\n",
    "        # get full text link\n",
    "        if full_text_url!= full_text_url and pmcid == pmcid:\n",
    "            url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + str(pmcid).strip() + \"/\"\n",
    "            try:\n",
    "                full_text_url, status_code = plib.get_final_redirected_url(url)\n",
    "                if status_code == 403:\n",
    "                    if pmid == pmid:\n",
    "                        link = \"https://pubmed.ncbi.nlm.nih.gov/\" + str(int(df.at[ind, \"PMID\"])).strip() + \"/\"\n",
    "                        soup = plib.request_webpage(link)\n",
    "                        link = soup.find(\"div\", {\"class\": \"full-text-links-list\"}).find(\"a\", {\"class\": \"link-item dialog-focus\"})[\"href\"]\n",
    "                        full_text_url, status_code = plib.get_final_redirected_url(link)\n",
    "                    else:\n",
    "                        full_text_url = np.nan\n",
    "            except:\n",
    "                raise Exception(status_code, \"Error when trying to get final redirected url from\", url)\n",
    "        \n",
    "        if full_text_url != full_text_url and doi == doi:\n",
    "            url = \"https://doi.org/\" + str(doi).strip().lower()\n",
    "            try:\n",
    "                full_text_url, status_code = plib.get_final_redirected_url(url)\n",
    "            except:\n",
    "                raise Exception(status_code, \"Error when trying to get final redirected url from\", url)\n",
    "        \n",
    "        if full_text_url != full_text_url and df.at[ind, \"full_text_url\"] == df.at[ind, \"full_text_url\"]:\n",
    "            try:\n",
    "                full_text_url, status_code = plib.get_final_redirected_url(df.at[ind, \"full_text_url\"])\n",
    "            except:\n",
    "                raise Exception(status_code, \"Error when trying to get final redirected url from\", df.at[ind, \"full_text_url\"])\n",
    "        \n",
    "        if  full_text_url != full_text_url:\n",
    "            print(doi)\n",
    "            print(pmid)\n",
    "            print(pmcid)\n",
    "            print(df.at[ind, \"full_text_url\"])\n",
    "            if df.at[ind, \"pdf_url\"] == df.at[ind, \"pdf_url\"]:\n",
    "                full_text_url = np.nan\n",
    "            else:\n",
    "                continue\n",
    "        # print(full_text_url)\n",
    "\n",
    "        # get full text source\n",
    "        if full_text_url == full_text_url:\n",
    "            full_text_source = full_text_url.split(\"://\")[1].split(\"/\")[0]\n",
    "        else:\n",
    "            full_text_source = np.nan\n",
    "        \n",
    "        if doi == doi:\n",
    "            doi = doi.lower()\n",
    "\n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"full_text_source\": [full_text_source],\n",
    "            \"pdf_url\": [df.at[ind, \"pdf_url\"]],\n",
    "            \"Title\": [df.at[ind, \"Title\"]],\n",
    "            \"Abstract\": [df.at[ind, \"Abstract\"]],\n",
    "            \"Keywords\": [df.at[ind, \"Keywords\"]]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Main program: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from PubMed\n",
    "\n",
    "# source_path = fpath.poten_litera_pubmed\n",
    "# output_path = fpath.poten_litera_pubmed_processed\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from PubMed\n",
    "# # 2612 results\n",
    "# preprocess_pubmed(source_path, output_path, 0, 2612)\n",
    "# print(\"preprocessing results from PubMed succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from PubMed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine 2 files of search results from web of science\n",
    "# # clear the file\n",
    "# plib.clear_file(fpath.poten_litera_wos)\n",
    "\n",
    "# # combine the 2 files of search results from web of science\n",
    "# source_path_1 = fpath.poten_litera_wos_1\n",
    "# source_path_2 = fpath.poten_litera_wos_2\n",
    "# df_1 = pd.read_csv(source_path_1, sep=',')\n",
    "# df_2 = pd.read_csv(source_path_2, sep=',')\n",
    "# df_1.to_csv(fpath.poten_litera_wos, header=True, index=False, sep=\",\")\n",
    "# df_2.to_csv(fpath.poten_litera_wos, mode=\"a\", header=False, index=False, sep=\",\")\n",
    "# # --------------------start of test code--------------------\n",
    "# df = pd.read_csv(fpath.poten_litera_wos, sep=',')\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "# # (1993, 72)\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from Web of Science\n",
    "\n",
    "# source_path = fpath.poten_litera_wos\n",
    "# output_path = fpath.poten_litera_wos_processed\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from Web of Science\n",
    "# # 1993 results\n",
    "# preprocess_webofscience(source_path, output_path, 0, 1993)\n",
    "# print(\"preprocessing results from Web of Science succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from Web of Science!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from Europe PMC\n",
    "\n",
    "# source_path = fpath.poten_litera_eupmc\n",
    "# output_path = fpath.poten_litera_eupmc_processed\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from Europe PMC\n",
    "# preprocess_eupmc(source_path, output_path, 2980, 9178)\n",
    "# # 9178 results\n",
    "# print(\"preprocessing results from Europe PMC succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from Europe PMC!\")\n",
    "\n",
    "# # 2980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from Google Scholar step 1\n",
    "\n",
    "# source_path = fpath.poten_litera_gs\n",
    "# # 980 results\n",
    "# output_path = fpath.poten_litera_gs_processed_step1\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from Google Scholar\n",
    "# preprocess_google_shcolar_step1(source_path, output_path, 0, 980)\n",
    "# # 926 results\n",
    "# print(\"step 1 of preprocessing results from Google Scholar succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from Google Scholar step 1!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reset index for poten_litera_gs_processed_step1\n",
    "# input_path = fpath.poten_litera_gs_processed_step1\n",
    "# output_path = fpath.poten_litera_gs_processed_step1\n",
    "# df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df.to_csv(output_path, header=False, index=False)\n",
    "\n",
    "# input_path = fpath.poten_litera_gs_processed_step1\n",
    "# df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "# print(df.shape)\n",
    "# # (926, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from Google Scholar step 2\n",
    "\n",
    "# source_path = fpath.poten_litera_gs_processed_step1\n",
    "# # (926, 4)\n",
    "# output_path = fpath.poten_litera_gs_processed_step2\n",
    "\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from Google Scholar\n",
    "# preprocess_google_shcolar_step2(source_path, output_path, 0, 926)\n",
    "# print(\"step 2 of preprocessing results from Google Scholar succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from Google Scholar step 2!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reset index for poten_litera_gs_processed_step2\n",
    "# input_path = fpath.poten_litera_gs_processed_step2\n",
    "# output_path = fpath.poten_litera_gs_processed_step2\n",
    "# df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df.to_csv(output_path, header=False, index=False)\n",
    "\n",
    "# input_path = fpath.poten_litera_gs_processed_step2\n",
    "# df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "# print(df.shape)\n",
    "# # (926, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from spanning citations of seed paper\n",
    "\n",
    "# preprocess_seed_paper_spanning(source_path, output_path, columns):\n",
    "# print(\"preprocessing results from spanning citations of seed papers succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from spanning citations of seed papers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from CoCoMac papers\n",
    "\n",
    "# preprocess_cocomac_paper(source_path, output_path, columns)\n",
    "# print(\"preprocessing results from CoCoMac papers succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from CoCoMac papers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # take a look at all the preprossed search results\n",
    "# gos = fpath.poten_litera_gs_processed_step2\n",
    "# wos = fpath.poten_litera_wos_processed\n",
    "# pubmed = fpath.poten_litera_pubmed_processed\n",
    "# eupmc = fpath.poten_litera_eupmc_processed\n",
    "\n",
    "# df_gs = pd.read_csv(gos, header=None, sep=',')\n",
    "# print(df_gs.shape)\n",
    "# # (907, 6)\n",
    "# df_wos = pd.read_csv(wos, header=None, sep=',')\n",
    "# print(df_wos.shape)\n",
    "# # (1993, 8)\n",
    "# df_pubmed = pd.read_csv(pubmed, header=None, sep=',')\n",
    "# print(df_pubmed.shape)\n",
    "# # (2612, 8)\n",
    "# df_eupmc = pd.read_csv(eupmc, header=None, sep=',')\n",
    "# print(df_eupmc.shape)\n",
    "# # (9178, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine all search results\n",
    "\n",
    "# gos = fpath.poten_litera_gs_processed_step2\n",
    "# wos = fpath.poten_litera_wos_processed\n",
    "# pubmed = fpath.poten_litera_pubmed_processed\n",
    "# eupmc = fpath.poten_litera_eupmc_processed\n",
    "# input = [gos, wos, pubmed, eupmc]\n",
    "# output_path = fpath.poten_litera_combined\n",
    "\n",
    "# # clear the file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# combine(input, output_path)\n",
    "# # (14627, 8)\n",
    "# print(\"Combining all search results succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when combining all search results!\")\n",
    "\n",
    "# df_combined = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df_combined.shape)\n",
    "# # (14690, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 502 when searching page: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6453219/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m output_path \u001b[39m=\u001b[39m fpath\u001b[39m.\u001b[39mpoten_litera_ids_filled\n\u001b[1;32m      5\u001b[0m \u001b[39m# clear file\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# plib.clear_file(output_path)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m fill_in_identifiers(input_path, output_path, \u001b[39m13308\u001b[39;49m, \u001b[39m14690\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFilling in missing elements succeeded!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 69\u001b[0m, in \u001b[0;36mfill_in_identifiers\u001b[0;34m(input_path, output_path, start, end)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39melif\u001b[39;00m df[\u001b[39m\"\u001b[39m\u001b[39mPMCID\u001b[39m\u001b[39m\"\u001b[39m][ind] \u001b[39m==\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mPMCID\u001b[39m\u001b[39m\"\u001b[39m][ind]: \u001b[39m# PMCID -> DOI, PMID\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     pmcid \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(df[\u001b[39m\"\u001b[39m\u001b[39mPMCID\u001b[39m\u001b[39m\"\u001b[39m][ind])\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m---> 69\u001b[0m     doi, pmid \u001b[39m=\u001b[39m plib\u001b[39m.\u001b[39;49mpmcid2doi_pmid(pmcid)\n\u001b[1;32m     70\u001b[0m     \u001b[39m# print(doi)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[39m# print(pmid)\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     doi \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnan\n",
      "File \u001b[0;32m~/myProjects/thalamocortical_connectivity_in_macaque/public_library.py:302\u001b[0m, in \u001b[0;36mpmcid2doi_pmid\u001b[0;34m(pmcid)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpmcid2doi_pmid\u001b[39m(pmcid):\n\u001b[1;32m    301\u001b[0m     url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://www.ncbi.nlm.nih.gov/pmc/articles/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m pmcid \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 302\u001b[0m     soup \u001b[39m=\u001b[39m plib\u001b[39m.\u001b[39;49mrequest_webpage(url)\n\u001b[1;32m    303\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    304\u001b[0m         doi \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind_all(\u001b[39m\"\u001b[39m\u001b[39mspan\u001b[39m\u001b[39m\"\u001b[39m, {\u001b[39m\"\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mdoi\u001b[39m\u001b[39m\"\u001b[39m})[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mfind_all(\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_text()\u001b[39m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/myProjects/thalamocortical_connectivity_in_macaque/public_library.py:68\u001b[0m, in \u001b[0;36mrequest_webpage\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mwhile\u001b[39;00m(response\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m \u001b[39m200\u001b[39m):\n\u001b[1;32m     67\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, response\u001b[39m.\u001b[39mstatus_code, \u001b[39m\"\u001b[39m\u001b[39mwhen searching page:\u001b[39m\u001b[39m\"\u001b[39m, url)\n\u001b[0;32m---> 68\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m5\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m60\u001b[39;49m)\n\u001b[1;32m     69\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url, headers \u001b[39m=\u001b[39m plib\u001b[39m.\u001b[39mheaders)\n\u001b[1;32m     70\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mcontent, \u001b[39m\"\u001b[39m\u001b[39mlxml\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fill in missing identifiers\n",
    "input_path = fpath.poten_litera_combined\n",
    "output_path = fpath.poten_litera_ids_filled\n",
    "\n",
    "# clear file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "fill_in_identifiers(input_path, output_path, 13308, 14690)\n",
    "print(\"Filling in missing elements succeeded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge all search results and remove duplication by identifiers\n",
    "\n",
    "# source_path = fpath.poten_litera_ids_filled\n",
    "# output_path = fpath.poten_litra_filtered\n",
    "\n",
    "# # clear the file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# # merge all search results\n",
    "# identifiers = [\"DOI\", \"PMID\", \"PMCID\"]\n",
    "# merge_remove_dupli(source_path, output_path, identifiers)\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litra_filtered\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# print(df.shape)\n",
    "# # (11021, 6)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fill in full_text_url\n",
    "\n",
    "# source_path = fpath.poten_litra_filtered\n",
    "# output_path = fpath.poten_litera_ids_ftl_filled\n",
    "\n",
    "# # clear the file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# # merge all search results\n",
    "# full_text_url_filling(source_path, output_path, 0, 11021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reset the index of poten_litera_ids_ftl_filled\n",
    "# source_path = fpath.poten_litera_ids_ftl_filled\n",
    "# output_path = fpath.poten_litera_ids_ftl_filled\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df.to_csv(output_path, header=False, index=False)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_ids_ftl_filled\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# # print(df.head(5))\n",
    "# print(df.shape)\n",
    "# # (10769, 7)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check all possible full_text_source\n",
    "# input_path = fpath.poten_litera_ids_ftl_filled\n",
    "# df = pd.read_csv(input_path, header=None, sep=\",\")\n",
    "# columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\"]\n",
    "# df.columns = columns\n",
    "\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "# # (10769, 7)\n",
    "\n",
    "# full_text_source = set(df['full_text_source'].tolist())\n",
    "# print(full_text_source)\n",
    "# # {'pure.mpg.de', 'direct.mit.edu', 'pubs.acs.org', 'thejns.org', 'www.microbiologyresearch.org', 'www.ncbi.nlm.nih.gov', \n",
    "# #  'journals.sagepub.com', 'journals.physiology.org', 'www.thieme-connect.de', 'www.jstage.jst.go.jp', 'www.rbojournal.org', \n",
    "# #  'www.annualreviews.org', 'var.scholarpedia.org', 'ujms.net', 'papers.ssrn.com', 'www.degruyter.com', 'jamanetwork.com', \n",
    "# #  'escholarship.mcgill.ca', 'www.tandfonline.com', 'wakespace.lib.wfu.edu', 'www.taylorfrancis.com', 'content.iospress.com:443', \n",
    "# #  'www.cambridge.org', 'n.neurology.org', 'journals.biologists.com', 'www.nature.com', 'pubs.aip.org', 'books.google.de', \n",
    "# #  'linkinghub.elsevier.com', 'academic.oup.com', 'link.springer.com', 'karger.com', 'neurologia.com', 'onlinelibrary.wiley.com', \n",
    "# #  'www.ajtmh.org', 'iovs.arvojournals.org', 'elibrary.ru', 'psycnet.apa.org:443', 'journals.aps.org', 'royalsocietypublishing.org', \n",
    "# #  'jpet.aspetjournals.org', 'www.biorxiv.org', 'ieeexplore.ieee.org', 'journals.lww.com', 'ekja.org', 'open.bu.edu', \n",
    "# #  'www.cabdirect.org', 'www.elibrary.ru', 'jnm.snmjournals.org', 'www.architalbiol.org', 'www.imrpress.com', \n",
    "# #  'neuro.psychiatryonline.org', 'submissions.mirasmart.com', 'pubs.asahq.org', 'europepmc.org', 'www.ahajournals.org', \n",
    "# #  'www.science.org', 'nrc-prod.literatumonline.com', 'pharmrev.aspetjournals.org', 'www.liebertpub.com', 'opg.optica.org', \n",
    "# #  'www.ingentaconnect.com', 'symposium.cshlp.org', 'ajp.psychiatryonline.org', 'webview.isho.jp', 'www.theses.fr', \n",
    "# #  'www.worldscientific.com'}\n",
    "\n",
    "# # [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\"]\n",
    "# print(df[\"DOI\"].isnull().any().any()) # True\n",
    "# print(df[\"PMID\"].isnull().any().any()) # True\n",
    "# print(df[\"PMCID\"].isnull().any().any()) # True\n",
    "# print(df[\"Title\"].isnull().any().any()) # False\n",
    "# print(df[\"full_text_url\"].isnull().any().any()) # False\n",
    "# print(df[\"full_text_source\"].isnull().any().any()) # False\n",
    "# print(df[\"pdf_url\"].isnull().any().any()) # True\n",
    "\n",
    "# print(df[\"DOI\"].dtypes) # object\n",
    "# print(df[\"PMID\"].dtypes) # float64\n",
    "# print(df[\"PMCID\"].dtypes) # object\n",
    "# print(df[\"Title\"].dtypes) # object\n",
    "# print(df[\"full_text_url\"].dtypes) # object\n",
    "# print(df[\"full_text_source\"].dtypes) # object\n",
    "# print(df[\"pdf_url\"].dtypes) # object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # websites_hosts\n",
    "# # {'pure.mpg.de', 'direct.mit.edu', 'pubs.acs.org', 'thejns.org', 'www.microbiologyresearch.org', 'www.ncbi.nlm.nih.gov', \n",
    "# #  'journals.sagepub.com', 'journals.physiology.org', 'www.thieme-connect.de', 'www.jstage.jst.go.jp', 'www.rbojournal.org', \n",
    "# #  'www.annualreviews.org', 'var.scholarpedia.org', 'ujms.net', 'papers.ssrn.com', 'www.degruyter.com', 'jamanetwork.com', \n",
    "# #  'escholarship.mcgill.ca', 'www.tandfonline.com', 'wakespace.lib.wfu.edu', 'www.taylorfrancis.com', 'content.iospress.com:443', \n",
    "# #  'www.cambridge.org', 'n.neurology.org', 'journals.biologists.com', 'www.nature.com', 'pubs.aip.org', 'books.google.de', \n",
    "# #  'linkinghub.elsevier.com', 'academic.oup.com', 'link.springer.com', 'karger.com', 'neurologia.com', 'onlinelibrary.wiley.com', \n",
    "# #  'www.ajtmh.org', 'iovs.arvojournals.org', 'elibrary.ru', 'psycnet.apa.org:443', 'journals.aps.org', 'royalsocietypublishing.org', \n",
    "# #  'jpet.aspetjournals.org', 'www.biorxiv.org', 'ieeexplore.ieee.org', 'journals.lww.com', 'ekja.org', 'open.bu.edu', \n",
    "# #  'www.cabdirect.org', 'www.elibrary.ru', 'jnm.snmjournals.org', 'www.architalbiol.org', 'www.imrpress.com', \n",
    "# #  'neuro.psychiatryonline.org', 'submissions.mirasmart.com', 'pubs.asahq.org', 'europepmc.org', 'www.ahajournals.org', \n",
    "# #  'www.science.org', 'nrc-prod.literatumonline.com', 'pharmrev.aspetjournals.org', 'www.liebertpub.com', 'opg.optica.org', \n",
    "# #  'www.ingentaconnect.com', 'symposium.cshlp.org', 'ajp.psychiatryonline.org', 'webview.isho.jp', 'www.theses.fr', \n",
    "# #  'www.worldscientific.com'}\n",
    "# websites_hosts = [\n",
    "#     'karger.com', 'rbojournal.org', 'sagepub.com', 'neurology.org', 'asahq.org', 'aspetjournals.org', 'thieme-connect.de', \n",
    "#     'taylorfrancis.com', 'lww.com', 'neurologia.com', 'ekja.org', 'www.imrpress.com', 'europepmc.org', 'springer.com', \n",
    "#     'theses.fr', 'ieee.org', 'ssrn.com', 'nature.com', 'liebertpub.com', 'oup.com', 'open.bu.edu', 'journals.biologists.com', \n",
    "#     'aip.org', 'mpg.de', 'lib.wfu.edu', 'cambridge.org', 'literatumonline.com', 'acs.org', 'scholarpedia.org', 'isho.jp', \n",
    "#     'mirasmart.com', 'jstage.jst.go.jp', 'psychiatryonline.org', 'psycnet.apa.org', 'thejns.org', 'microbiologyresearch.org', \n",
    "#     'wiley.com', 'snmjournals.org', 'degruyter.com', 'worldscientific.com', 'opg.optica.org', 'science.org', 'aps.org', \n",
    "#     'ujms.net', 'mit.edu', 'biorxiv.org','annualreviews.org', 'elibrary.ru', 'www.ingentaconnect.com', 'mcgill.ca', \n",
    "#     'symposium.cshlp.org', 'architalbiol.org', 'arvojournals.org', 'jamanetwork.com', 'elsevier.com', 'ncbi.nlm.nih.gov', \n",
    "#     'cabdirect.org', 'books.google.de', 'iospress.com', 'tandfonline.com', 'ajtmh.org', 'royalsocietypublishing.org', \n",
    "#     'ahajournals.org', 'physiology.org']\n",
    "# # --------------------start of test code--------------------\n",
    "# if len(websites_hosts) == len(set(websites_hosts)):\n",
    "#     print(\"There are no duplicates in the list.\")\n",
    "# else:\n",
    "#     print(\"There are duplicates in the list.\")\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sort the websites by the number of articles they have\n",
    "# input_path = fpath.poten_litera_ids_ftl_filled\n",
    "# df = pd.read_csv(input_path, header=None, sep=\",\")\n",
    "# columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\"]\n",
    "# df.columns = columns\n",
    "# func_dict = {website: 0 for website in websites_hosts}\n",
    "# # print(func_dict)\n",
    "\n",
    "# for ind in df.index:\n",
    "#     for website in websites_hosts:\n",
    "#         if website in df.loc[ind, \"full_text_source\"]:\n",
    "#             func_dict[website] += 1\n",
    "#             break\n",
    "\n",
    "# # Sort dictionary by values\n",
    "# sorted_dict = dict(sorted(func_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "# print(sorted_dict)\n",
    "# # {'ncbi.nlm.nih.gov': 7886, 'elsevier.com': 1019, 'wiley.com': 696, 'springer.com': 285, 'physiology.org': 205, \n",
    "# #  'oup.com': 152, 'cambridge.org': 74, 'karger.com': 53, 'lww.com': 49, 'nature.com': 44, 'science.org': 30, \n",
    "# #  'tandfonline.com': 29, 'sagepub.com': 21, 'jamanetwork.com': 20, 'neurology.org': 16, 'biorxiv.org': 15, \n",
    "# #  'royalsocietypublishing.org': 13, 'psycnet.apa.org': 12, 'arvojournals.org': 12, 'jstage.jst.go.jp': 11, \n",
    "# #  'psychiatryonline.org': 11, 'europepmc.org': 10, 'mit.edu': 10, 'thejns.org': 8, 'annualreviews.org': 8, \n",
    "# #  'snmjournals.org': 7, 'aspetjournals.org': 6, 'elibrary.ru': 5, 'books.google.de': 5, 'architalbiol.org': 4, \n",
    "# #  'ahajournals.org': 4, 'liebertpub.com': 3, 'acs.org': 3, 'degruyter.com': 3, 'worldscientific.com': 3, \n",
    "# #  'iospress.com': 3, 'asahq.org': 2, 'thieme-connect.de': 2, 'neurologia.com': 2, 'mpg.de': 2, 'opg.optica.org': 2, \n",
    "# #  'mcgill.ca': 2, 'rbojournal.org': 1, 'taylorfrancis.com': 1, 'ekja.org': 1, 'www.imrpress.com': 1, 'theses.fr': 1, \n",
    "# #  'ieee.org': 1, 'ssrn.com': 1, 'open.bu.edu': 1, 'journals.biologists.com': 1, 'aip.org': 1, 'lib.wfu.edu': 1, \n",
    "# #  'literatumonline.com': 1, 'scholarpedia.org': 1, 'isho.jp': 1, 'mirasmart.com': 1, 'microbiologyresearch.org': 1, \n",
    "# #  'aps.org': 1, 'ujms.net': 1, 'www.ingentaconnect.com': 1, 'symposium.cshlp.org': 1, 'cabdirect.org': 1, 'ajtmh.org': 1}\n",
    "\n",
    "# non_zero_keys = [key for key, value in sorted_dict.items() if value != 0]\n",
    "# print(non_zero_keys)\n",
    "# # ['ncbi.nlm.nih.gov', 'elsevier.com', 'wiley.com', 'springer.com', 'physiology.org', 'oup.com', 'cambridge.org', \n",
    "# #  'karger.com', 'lww.com', 'nature.com', 'science.org', 'tandfonline.com', 'sagepub.com', 'jamanetwork.com', \n",
    "# #  'neurology.org', 'biorxiv.org', 'royalsocietypublishing.org', 'psycnet.apa.org', 'arvojournals.org', 'jstage.jst.go.jp', \n",
    "# #  'psychiatryonline.org', 'europepmc.org', 'mit.edu', 'thejns.org', 'annualreviews.org', 'snmjournals.org', \n",
    "# #  'aspetjournals.org', 'elibrary.ru', 'books.google.de', 'architalbiol.org', 'ahajournals.org', 'liebertpub.com', \n",
    "# #  'acs.org', 'degruyter.com', 'worldscientific.com', 'iospress.com', 'asahq.org', 'thieme-connect.de', 'neurologia.com', \n",
    "# #  'mpg.de', 'opg.optica.org', 'mcgill.ca', 'rbojournal.org', 'taylorfrancis.com', 'ekja.org', 'www.imrpress.com', \n",
    "# #  'theses.fr', 'ieee.org', 'ssrn.com', 'open.bu.edu', 'journals.biologists.com', 'aip.org', 'lib.wfu.edu', \n",
    "# #  'literatumonline.com', 'scholarpedia.org', 'isho.jp', 'mirasmart.com', 'microbiologyresearch.org', 'aps.org', \n",
    "# #  'ujms.net', 'www.ingentaconnect.com', 'symposium.cshlp.org', 'cabdirect.org', 'ajtmh.org']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # websites\n",
    "# websites = [\n",
    "#     'ncbi.nlm.nih.gov', 'elsevier.com', 'wiley.com', 'springer.com', 'physiology.org', 'oup.com', \n",
    "#     'cambridge.org', 'karger.com', 'lww.com', 'nature.com', 'science.org', 'tandfonline.com', \n",
    "#     'sagepub.com', 'jamanetwork.com', 'neurology.org', 'biorxiv.org', 'royalsocietypublishing.org', \n",
    "#     'psycnet.apa.org', 'arvojournals.org', 'jstage.jst.go.jp', 'psychiatryonline.org', 'europepmc.org', \n",
    "#     'mit.edu', 'thejns.org', 'annualreviews.org', 'snmjournals.org', 'aspetjournals.org', 'elibrary.ru', \n",
    "#     'books.google.de', 'architalbiol.org', 'ahajournals.org', 'liebertpub.com', 'acs.org', 'degruyter.com', \n",
    "#     'worldscientific.com', 'iospress.com', 'asahq.org', 'thieme-connect.de', 'neurologia.com', 'mpg.de', \n",
    "#     'opg.optica.org', 'mcgill.ca', 'rbojournal.org', 'taylorfrancis.com', 'ekja.org', 'www.imrpress.com', \n",
    "#     'theses.fr', 'ieee.org', 'ssrn.com', 'open.bu.edu', 'journals.biologists.com', 'aip.org', 'lib.wfu.edu', \n",
    "#     'literatumonline.com', 'scholarpedia.org', 'isho.jp', 'mirasmart.com', 'microbiologyresearch.org', \n",
    "#     'aps.org', 'ujms.net', 'www.ingentaconnect.com', 'symposium.cshlp.org', 'cabdirect.org', 'ajtmh.org'\n",
    "# ]\n",
    "# # --------------------start of test code--------------------\n",
    "# if len(websites) == len(websites_hosts):\n",
    "#     print('The number of websites is correct')\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Next step: automatic filtering </h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
