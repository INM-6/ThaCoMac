{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Searched literature data preprocessing </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal .py modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Parameters: </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns of file: potential_related_literature.csv\n",
    "columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"First_Author\", \"full_text_url\", \"full_text_source\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Predefined fucntions: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_wegpage(url, proxies):\n",
    "    response = requests.get(url, headers = plib.headers, proxies = proxies)\n",
    "    if response.status_code != 200:\n",
    "        # print(\"Error when requesting:\", url)\n",
    "        # print(response.status_code)\n",
    "        raise Exception(\"Your request was declined, again!\")\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pmc(columns):\n",
    "    print(\"Starting merging search results from PubMed Central PMC...\")\n",
    "    # process pmc search results\n",
    "    df = pd.read_csv(fpath.poten_litera_pmc, sep=',')\n",
    "    df = df[[\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"First Author\"]]\n",
    "    for ind in df.index:\n",
    "        pmid = str(df[\"PMID\"][ind])\n",
    "        url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "        if(ind%5 == 0):\n",
    "            time.sleep(random.randint(1, 10))\n",
    "            proxies, auth = plib.get_proxies()\n",
    "        soup = request_wegpage(url, proxies)\n",
    "        # print(soup)\n",
    "        # get full name of first author\n",
    "        try:\n",
    "            first_author = soup.find_all(\"span\", {\"class\": \"authors-list-item\"})[0].find_all(\"a\", {\"class\": \"full-name\"})[0].get_text().strip()\n",
    "        except:\n",
    "            first_author = \"not found\"\n",
    "        # get PMCID\n",
    "        # print(df[\"PMCID\"][ind])\n",
    "        if df[\"PMCID\"][ind] is np.nan:\n",
    "            try:\n",
    "                pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                pmcid  =\"not found\"\n",
    "        else:\n",
    "            pmcid = str(df[\"PMCID\"][ind])\n",
    "        # print(pmcid)\n",
    "        # get DOI\n",
    "        if df[\"DOI\"][ind] is np.nan:\n",
    "            try:\n",
    "                doi = soup.find_all(\"span\", {\"class\": \"identifier doi\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                doi  =\"not found\"\n",
    "        else:\n",
    "            doi = str(df[\"DOI\"][ind])\n",
    "        # get full_text_url\n",
    "        if pmcid != \"not found\":\n",
    "            full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "            full_text_source = \"PMC\"\n",
    "        else:\n",
    "            try:\n",
    "                full_text_url = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"href\"].strip()\n",
    "                full_text_source = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"data-ga-action\"].strip()\n",
    "            except:\n",
    "                full_text_url = \"not found\"\n",
    "                full_text_source = \"not found\"\n",
    "        # columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"First_Author\", \"full_text_url\", \"full_text_source\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"Title\": [str(df[\"Title\"][ind])],\n",
    "            \"First_Author\": [first_author],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"full_text_source\": [full_text_source]\n",
    "        }\n",
    "        # print(row)\n",
    "        if not plib.add_row_to_csv(fpath.poten_litera, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "# --------------------start of test code--------------------\n",
    "# plib.clear_file(fpath.poten_litera)\n",
    "# df = pd.read_csv(fpath.poten_litera_pmc, sep=',')\n",
    "# df = df[[\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"First Author\"]]\n",
    "# print(df.head(5))\n",
    "# print(df[\"DOI\"].isnull().values.any())\n",
    "# print(df[\"PMID\"].isnull().values.any())\n",
    "# print(df[\"PMCID\"].isnull().values.any())\n",
    "# print(df[\"Title\"].isnull().values.any())\n",
    "# print(df[\"First Author\"].isnull().values.any())\n",
    "# # the columns PMID, Title, First Author don't contain np.nan\n",
    "# # the columns DOI, PMCID contain np.nan, we need to fill in what are missing\n",
    "# # we also need to reenter the full name of the first author\n",
    "# merge_pmc(columns)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_webofscience(columns):\n",
    "    print(\"Starting merging search results from Web of Science...\")\n",
    "    df = pd.read_csv(fpath.poten_litera_wos_1, sep = \";\")\n",
    "    df = df[[\"DOI\", \"Pubmed Id\", \"Article Title\"]]\n",
    "    df.rename(columns={\"DOI\": \"DOI\", \"Pubmed Id\": \"PMID\", \"Article Title\": \"Title\"}, inplace = True)\n",
    "    df[\"PMID\"] = df[\"PMID\"].fillna(0)\n",
    "    df[\"PMID\"] = df[\"PMID\"].astype(int)\n",
    "    df[\"PMID\"] = df[\"PMID\"].astype(str)\n",
    "    pmcid = []\n",
    "    for ind in df_wos.index:\n",
    "        # print(df_wos[\"PMID\"][ind])\n",
    "        if df_wos[\"PMID\"][ind] != \"0\":\n",
    "            pmid = df_wos[\"PMID\"][ind]\n",
    "            # print(pmid)\n",
    "            df_wos[\"PMID\"][ind] = pmid\n",
    "            url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "            # print(url)\n",
    "            time.sleep(random.randint(5, 20))\n",
    "            response = requests.get(url, headers = plib.headers)\n",
    "            if response.status_code != 200:\n",
    "                raise Exception(\"Error when request webpages!\")\n",
    "            soup = BeautifulSoup(response.content, \"lxml\")\n",
    "            l = soup.find_all(\"a\", {\"data-ga-action\": \"PMCID\"})\n",
    "            if(len(l) != 0):\n",
    "                # print(l[0].get_text().strip())\n",
    "                pmcid.append(l[0].get_text().strip())\n",
    "            else:\n",
    "                pmcid.append(np.nan)\n",
    "        else:\n",
    "            pmcid.append(np.nan)\n",
    "        # print(df_wos[ind])\n",
    "    df_wos[\"PMCID\"] = pmcid\n",
    "    df_wos[\"PMCID\"].replace(\"0\", np.nan)\n",
    "    # print(df_wos.head(5))\n",
    "    df_wos = df_wos[columns]\n",
    "    df_wos.to_csv(fpath.poten_litera, header = True, index = None)\n",
    "# --------------------start of test code--------------------\n",
    "# plib.clear_file(fpath.poten_litera)\n",
    "# df = pd.read_csv(fpath.poten_litera_wos_1, sep=';')\n",
    "# df = df[[\"DOI\", \"Pubmed Id\", \"Article Title\"]]\n",
    "# print(df.head(5))\n",
    "# print(df[\"DOI\"].isnull().values.any())\n",
    "# print(df[\"Pubmed Id\"].isnull().values.any())\n",
    "# print(df[\"Article Title\"].isnull().values.any())\n",
    "# # the columns Article Title don't contain np.nan\n",
    "# # the columns DOI, Pubmed Id contain np.nan, we need to fill in what are missing\n",
    "# # we also need to reenter the full name of the first author\n",
    "# merge_webofscience(columns)\n",
    "# ---------------------end of test code---------------------  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          DOI EXTERNAL_ID        PMCID  \\\n",
      "0  10.3389/fnins.2023.1163600    37123374  PMC10133512   \n",
      "1        10.3390/ijms24119643    37298594  PMC10254002   \n",
      "2  10.3389/fninf.2023.1170337    37377946  PMC10291062   \n",
      "3  10.1038/s41467-023-38582-7    37291094  PMC10250345   \n",
      "4  10.3389/fnins.2023.1136500    37360183  PMC10288156   \n",
      "\n",
      "                                               TITLE  \n",
      "0       The thalamus in psychosis spectrum disorder.  \n",
      "1  Thyroid Hormone Transporters MCT8 and OATP1C1 ...  \n",
      "2  Connectomes: from a sparsity of networks to la...  \n",
      "3  Functional gene delivery to and across brain v...  \n",
      "4  Current analysis of hypoxic-ischemic encephalo...  \n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "Starting merging search results from Europe PMC...\n",
      "37123374\n",
      "pmid\n"
     ]
    },
    {
     "ename": "ProxyError",
     "evalue": "HTTPSConnectionPool(host='pubmed.ncbi.nlm.nih.gov', port=443): Max retries exceeded with url: /37123374/ (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Forbidden')))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/urllib3/connectionpool.py:711\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[39mif\u001b[39;00m is_new_proxy_conn \u001b[39mand\u001b[39;00m http_tunnel_required:\n\u001b[0;32m--> 711\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/urllib3/connectionpool.py:1007\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._prepare_proxy\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     conn\u001b[39m.\u001b[39mtls_in_tls_required \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m-> 1007\u001b[0m conn\u001b[39m.\u001b[39;49mconnect()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/urllib3/connection.py:374\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39m# Calls self._set_hostport(), so self.host is\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[39m# self._tunnel_host below.\u001b[39;00m\n\u001b[0;32m--> 374\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tunnel()\n\u001b[1;32m    375\u001b[0m \u001b[39m# Mark this connection as not reusable\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/http/client.py:925\u001b[0m, in \u001b[0;36mHTTPConnection._tunnel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n\u001b[0;32m--> 925\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTunnel connection failed: \u001b[39m\u001b[39m{\u001b[39;00mcode\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mmessage\u001b[39m.\u001b[39mstrip()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    926\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: Tunnel connection failed: 403 Forbidden",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/urllib3/connectionpool.py:798\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    796\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 798\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    799\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    800\u001b[0m )\n\u001b[1;32m    801\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='pubmed.ncbi.nlm.nih.gov', port=443): Max retries exceeded with url: /37123374/ (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Forbidden')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProxyError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 90\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mprint\u001b[39m(df[\u001b[39m\"\u001b[39m\u001b[39mTITLE\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39misnull()\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39many())\n\u001b[1;32m     87\u001b[0m \u001b[39m# the columns PMID, Title don't contain np.nan\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[39m# the columns DOI, PMCID contain np.nan, we need to fill in what are missing\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39m# we also need to reenter the full name of the first author\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m merge_eupmc(columns)\n",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m, in \u001b[0;36mmerge_eupmc\u001b[0;34m(columns)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(re\u001b[39m.\u001b[39mfindall(regex, pmid)) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     17\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mpmid\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     soup \u001b[39m=\u001b[39m request_wegpage(url, proxies)\n\u001b[1;32m     19\u001b[0m     \u001b[39m# print(soup)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[39m# get full name of first author\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m, in \u001b[0;36mrequest_wegpage\u001b[0;34m(url, proxies)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest_wegpage\u001b[39m(url, proxies):\n\u001b[0;32m----> 2\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(url, headers \u001b[39m=\u001b[39;49m plib\u001b[39m.\u001b[39;49mheaders, proxies \u001b[39m=\u001b[39;49m proxies)\n\u001b[1;32m      3\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[1;32m      4\u001b[0m         \u001b[39m# print(\"Error when requesting:\", url)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         \u001b[39m# print(response.status_code)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYour request was declined, again!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/requests/adapters.py:513\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[39mraise\u001b[39;00m RetryError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    512\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _ProxyError):\n\u001b[0;32m--> 513\u001b[0m     \u001b[39mraise\u001b[39;00m ProxyError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    515\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    516\u001b[0m     \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n",
      "\u001b[0;31mProxyError\u001b[0m: HTTPSConnectionPool(host='pubmed.ncbi.nlm.nih.gov', port=443): Max retries exceeded with url: /37123374/ (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Forbidden')))"
     ]
    }
   ],
   "source": [
    "def merge_eupmc(columns):\n",
    "    print(\"Starting merging search results from Europe PMC...\")\n",
    "    # process eupmc search results\n",
    "    df = pd.read_csv(fpath.poten_litera_eupmc, sep = \",\")\n",
    "    df = df[[\"DOI\", \"EXTERNAL_ID\", \"PMCID\", \"TITLE\"]]\n",
    "    df = df.rename(columns={\"EXTERNAL_ID\": \"PMID\", \"TITLE\": \"Title\"}, errors = \"raise\")\n",
    "    for ind in df.index:\n",
    "        pmid = str(df[\"PMID\"][ind])\n",
    "        print(pmid)\n",
    "        url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "        if(ind%5 == 0):\n",
    "            time.sleep(random.randint(1, 10))\n",
    "            proxies = plib.get_proxies()\n",
    "        regex = \"[a-zA-Z]\"\n",
    "        \n",
    "        if len(re.findall(regex, pmid)) == 0:\n",
    "            print(\"pmid\")\n",
    "            soup = request_wegpage(url, proxies)\n",
    "            # print(soup)\n",
    "            # get full name of first author\n",
    "            try:\n",
    "                first_author = soup.find_all(\"span\", {\"class\": \"authors-list-item\"})[0].find_all(\"a\", {\"class\": \"full-name\"})[0].get_text().strip()\n",
    "            except:\n",
    "                first_author = \"not found\"\n",
    "            # get PMCID\n",
    "            # print(df[\"PMCID\"][ind])\n",
    "            if df[\"PMCID\"][ind] is np.nan:\n",
    "                try:\n",
    "                    pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "                except:\n",
    "                    pmcid  =\"not found\"\n",
    "            else:\n",
    "                pmcid = str(df[\"PMCID\"][ind])\n",
    "            # print(pmcid)\n",
    "            # get DOI\n",
    "            if df[\"DOI\"][ind] is np.nan:\n",
    "                try:\n",
    "                    doi = soup.find_all(\"span\", {\"class\": \"identifier doi\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "                except:\n",
    "                    doi  =\"not found\"\n",
    "            else:\n",
    "                doi = str(df[\"DOI\"][ind])\n",
    "            # get full_text_url\n",
    "            if pmcid != \"not found\":\n",
    "                full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "                full_text_source = \"PMC\"\n",
    "            else:\n",
    "                try:\n",
    "                    full_text_url = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"href\"].strip()\n",
    "                    full_text_source = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"data-ga-action\"].strip()\n",
    "                except:\n",
    "                    full_text_url = \"not found\"\n",
    "                    full_text_source = \"not found\"\n",
    "            # columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"First_Author\", \"full_text_url\", \"full_text_source\"]\n",
    "        else:\n",
    "            print(\"not pmid\")\n",
    "            if df[\"DOI\"][ind] is np.nan:\n",
    "                doi  =\"not found\"\n",
    "            else:\n",
    "                doi = str(df[\"DOI\"][ind])\n",
    "            full_text_url = \"not found\"\n",
    "            full_text_source = \"not found\"\n",
    "            pmid = \"not found\"\n",
    "            pmcid = \"not found\"\n",
    "            first_author = \"not found\"\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"Title\": [str(df[\"Title\"][ind])],\n",
    "            \"First_Author\": [first_author],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"full_text_source\": [full_text_source]\n",
    "        }\n",
    "        # print(row)\n",
    "        if not plib.add_row_to_csv(fpath.poten_litera, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "# --------------------start of test code--------------------\n",
    "plib.clear_file(fpath.poten_litera)\n",
    "df = pd.read_csv(fpath.poten_litera_eupmc, sep=',')\n",
    "df = df[[\"DOI\", \"EXTERNAL_ID\", \"PMCID\", \"TITLE\"]]\n",
    "print(df.head(5))\n",
    "print(df[\"DOI\"].isnull().values.any())\n",
    "print(df[\"EXTERNAL_ID\"].isnull().values.any())\n",
    "print(df[\"PMCID\"].isnull().values.any())\n",
    "print(df[\"TITLE\"].isnull().values.any())\n",
    "# the columns PMID, Title don't contain np.nan\n",
    "# the columns DOI, PMCID contain np.nan, we need to fill in what are missing\n",
    "# we also need to reenter the full name of the first author\n",
    "merge_eupmc(columns)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_google_shcolar(columns):\n",
    "    print(\"Starting merging search results from Google Scholar...\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_seed_paper_spanning(columns):\n",
    "    print(\"Starting merging search results from spanning citations of seed paper...\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_cocomac_paper(columns):\n",
    "    print(\"Starting merging search results from CoCoMac papers...\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure at least PMID and PMCID is present as two of the four identifiers, otherwise manually fill in\n",
    "def fill_in_elements(file_path):\n",
    "    # PMID -> PMCID\n",
    "    # done already\n",
    "    # PMCID -> PMID\n",
    "    # done already\n",
    "    # PMID -> DOI\n",
    "    df = pd.read_csv(file_path, sep = \",\")\n",
    "    for ind in df.index:\n",
    "        if (df[\"PMID\"][ind] is not np.nan) and (df[\"DOI\"][ind] is np.nan):\n",
    "            pmid = df[\"PMID\"][ind]\n",
    "            url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "            print(url)\n",
    "            response = requests.get(url, headers = plib.headers)\n",
    "            if response.status_code != 200:\n",
    "                raise Exception(\"Error when request webpages!\")\n",
    "            soup = BeautifulSoup(response.content, \"lxml\")\n",
    "            l = soup.find_all(\"a\", {\"class: id-link\"}, {\"data-ga-action\": \"DOI\"})\n",
    "            if(len(l) != 0):\n",
    "                # print(l[0].get_text().strip())\n",
    "                df.at[ind, \"DOI\"] = l[0].get_text().strip()\n",
    "            else:\n",
    "                df.at[ind, \"DOI\"] = np.nan\n",
    "    df.to_csv(fpath.poten_litera_csv, header = True, index = False)\n",
    "    print(\"All 3 identifiers: DOI, PMID, and PMCID filled in when possible.\")\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplciations based on identifiers in the potential related literature\n",
    "def remove_dupli(file_path):\n",
    "    df = pd.read_csv(file_path, sep = \",\")\n",
    "    print(len(df))\n",
    "    df = df.drop_duplicates(subset=['DOI'])\n",
    "    df = df.drop_duplicates(subset=['PMID'])\n",
    "    df = df.drop_duplicates(subset=['PMCID'])\n",
    "    print(len(df))\n",
    "    # plib.clear_file(fpath.poten_litera_csv)\n",
    "    # df.csv(fpath.poten_litera_csv, idnex = None)\n",
    "    print(\"Duplication in the potential related literature removed.\")\n",
    "    print(\"Found \" + len(df) + \" potential related literature in total.\")\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Main program: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the file\n",
    "plib.clear_file(fpath.poten_litera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge search results from PubMed Central PMC\n",
    "# if merge_pmc(columns):\n",
    "#     print(\"Merging results from PubMed Central PMC succeeded!\")\n",
    "# else:\n",
    "#     print(\"Attention! Something went wrong when merging results from PubMed Central PMC!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge search results from Web of Science\n",
    "# if merge_webofscience():\n",
    "#     print(\"Merging results from Web of Science succeeded!\")\n",
    "# else:\n",
    "#     print(\"Attention! Something went wrong when merging results from Web of Science!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge search results from Europe PMC\n",
    "# if merge_eupmc():\n",
    "#     print(\"Merging results from Europe PMC succeeded!\")\n",
    "# else:\n",
    "#     print(\"Attention! Something went wrong when merging results from Europe PMC!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge search results from Google Scholar\n",
    "# if merge_google_shcolar():\n",
    "#     print(\"Merging results from Google Scholar succeeded!\")\n",
    "# else:\n",
    "#     print(\"Attention! Something went wrong when merging results from Google Scholar!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge search results from spanning citations of seed paper\n",
    "# if merge_seed_paper_spanning():\n",
    "#     print(\"Merging results from spanning citations of seed papers succeeded!\")\n",
    "# else:\n",
    "#     print(\"Attention! Something went wrong when merging results from spanning citations of seed papers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge search results from CoCoMac papers\n",
    "# if merge_cocomac_paper():\n",
    "#     print(\"Merging results from CoCoMac papers succeeded!\")\n",
    "# else:\n",
    "#     print(\"Attention! Something went wrong when merging results from CoCoMac papers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fill in all elements in the columns when possible, if not, fill in \"not found\"\n",
    "# fill_in_elements(fpath.poten_litera_csv, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifier = [\"DOI\", \"PMID\", \"PMCID\"]\n",
    "# remove_dupli(fpath.poten_litera_csv, identifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Next step: automatic filtering </h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
