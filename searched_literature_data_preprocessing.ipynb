{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Searched literature data preprocessing </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal .py modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Parameters: </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns of file: potential_related_literature.csv\n",
    "columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "# e.g., [\"10.1113/JP282626\", \"35851953\", \"PMC10087288\", \n",
    "#        \"Cortico-thalamocortical interactions for learning, memory and decision-making\",\n",
    "#        \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10087288/\", \"PMC\",\n",
    "#        \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10087288/pdf/TJP-601-25.pdf\", \"PMC\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Predefined fucntions: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pubmed(source_path, output_path, start, end):\n",
    "    print(\"Starting preprocessing search results from PubMed...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, sep=',')\n",
    "    df = df[[\"DOI\", \"PMID\", \"PMCID\", \"Title\"]]\n",
    "    \n",
    "    for ind in range(start, end):\n",
    "        # sleep to avoid to be blocked\n",
    "        time.sleep(random.randint(3, 5))\n",
    "        # if(ind%50 == 0):\n",
    "        #     time.sleep(random.randint(10,15)*10)\n",
    "        \n",
    "        # request the webpage\n",
    "        # the columns PMID, Title don't contain np.nan\n",
    "        pmid = str(df[\"PMID\"][ind]).strip()\n",
    "        url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "        # proxies = plib.get_proxies()\n",
    "        soup = plib.request_webpage(url)\n",
    "        # print(soup)\n",
    "        \n",
    "        # get pmcid\n",
    "        if df[\"PMCID\"][ind] != df[\"PMCID\"][ind]: # PMCID is np.nan\n",
    "            try:\n",
    "                pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                pmcid = np.nan\n",
    "        else: # PMCID is not np.nan\n",
    "            pmcid = str(df[\"PMCID\"][ind]).strip()\n",
    "        # print(pmcid)\n",
    "\n",
    "        # get doi\n",
    "        if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # DOI is np.nan\n",
    "            try:\n",
    "                doi = soup.find_all(\"span\", {\"class\": \"identifier doi\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                doi  = np.nan\n",
    "        else: # DOI is not np.nan\n",
    "            doi = str(df[\"DOI\"][ind]).strip()\n",
    "        # print(doi)\n",
    "\n",
    "        # get full_text_url, full_text_source\n",
    "        if pmcid == pmcid: # pmcid is not np.nan\n",
    "            full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "            full_text_source = \"PMC\"\n",
    "        else: # pmcid is np.nan\n",
    "            # PMC does not include this paper\n",
    "            try:\n",
    "                full_text_url = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"href\"].strip()\n",
    "                full_text_source = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"data-ga-action\"].strip()\n",
    "            except:\n",
    "                full_text_url = np.nan\n",
    "                full_text_source = np.nan\n",
    "        # print(full_text_url)\n",
    "        # print(full_text_source)\n",
    "        \n",
    "        # get pdf_url, pdf_source\n",
    "        pdf_url = np.nan\n",
    "        pdf_source = np.nan\n",
    "                \n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"Title\": [str(df[\"Title\"][ind]).strip()],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"full_text_source\": [full_text_source],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"pdf_source\": [pdf_source]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_pubmed\n",
    "# output_path = fpath.poten_litera_pubmed_processed\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, sep=',')\n",
    "# print(df.shape)\n",
    "# df = df[[\"DOI\", \"PMID\", \"PMCID\", \"Title\"]]\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "\n",
    "# print(df[\"DOI\"].isnull().values.any())\n",
    "# print(df[\"PMID\"].isnull().values.any())\n",
    "# print(df[\"PMCID\"].isnull().values.any())\n",
    "# print(df[\"Title\"].isnull().values.any())\n",
    "# # True, False, True, Flase\n",
    "# # PMID, Title don't contain np.nan\n",
    "# # DOI, PMCID contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_pubmed(source_path, output_path, start, end)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_webofscience(source_path, output_path, start, end):\n",
    "    print(\"Starting preprocessing search results from Web of Science...\")\n",
    "    \n",
    "    df = pd.read_csv(source_path, sep=\";\")\n",
    "    df = df[[\"DOI\", \"Pubmed Id\", \"Article Title\"]]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # sleep to avoid to be blocked\n",
    "        time.sleep(random.randint(3, 5))\n",
    "        # if(ind%50 == 0):\n",
    "        #     time.sleep(random.randint(10,15)*10)\n",
    "        \n",
    "        # the columns Article Title don't contain np.nan\n",
    "        # the columns DOI and PMID might contain np.nan\n",
    "        # get pmid, doi\n",
    "        if df[\"Pubmed Id\"][ind] != df[\"Pubmed Id\"][ind]: # Pubmed Id is np.nan\n",
    "            if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # DOI is np.nan\n",
    "                doi = np.nan\n",
    "                pmid = np.nan\n",
    "            else: # DOI is not np.nan\n",
    "                doi = str(df[\"DOI\"][ind]).strip()\n",
    "                pmid = plib.doi2pmid(doi)\n",
    "        else: # Pubmed Id is not np.nan\n",
    "            pmid = str(int(df[\"Pubmed Id\"][ind])).strip()\n",
    "            if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # DOI is not np.nan\n",
    "                doi = plib.pmid2doi(pmid)\n",
    "            else: # DOI is not np.nan\n",
    "                doi = str(df[\"DOI\"][ind]).strip()\n",
    "        \n",
    "        # get pmcid, full_text_url, full_text_source\n",
    "        if pmid != pmid: # pmid is np.nan\n",
    "            pmcid = np.nan\n",
    "            if doi != doi: # doi is np.nan\n",
    "                full_text_url = np.nan\n",
    "                full_text_source = np.nan\n",
    "            else:\n",
    "                full_text_url = \"https://doi.org/\" + str(doi).strip()\n",
    "                full_text_source = \"DOI\"\n",
    "        else: # pmid is not np.nan\n",
    "            # request the webpage\n",
    "            url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "            # proxies = plib.get_proxies()\n",
    "            soup = plib.request_webpage(url)\n",
    "            # print(soup)\n",
    "\n",
    "            # get pmcid\n",
    "            try:\n",
    "                pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                pmcid = np.nan\n",
    "            # print(pmcid)\n",
    "            \n",
    "            # get full_text_url, full_text_source\n",
    "            if pmcid == pmcid:\n",
    "                full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "                full_text_source = \"PMC\"\n",
    "            else:\n",
    "                try:\n",
    "                    full_text_url = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"href\"].strip()\n",
    "                    full_text_source = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"data-ga-action\"].strip()\n",
    "                except:\n",
    "                    full_text_url = np.nan\n",
    "                    full_text_source = np.nan\n",
    "        \n",
    "        # get pdf_url, pdf_source\n",
    "        pdf_url = np.nan\n",
    "        pdf_source = np.nan\n",
    "\n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"Title\": [str(df[\"Article Title\"][ind]).strip()],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"full_text_source\": [full_text_source],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"pdf_source\": [pdf_source]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# # source_path = fpath.poten_litera_wos\n",
    "# # output_path = fpath.poten_litera_wos_processed\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, sep=';')\n",
    "# df = df[[\"DOI\", \"Pubmed Id\", \"Article Title\"]]\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "\n",
    "# print(df[\"DOI\"].isnull().values.any())\n",
    "# print(df[\"Pubmed Id\"].isnull().values.any())\n",
    "# print(df[\"Article Title\"].isnull().values.any())\n",
    "# # True, True, False\n",
    "# # Article Title don't contain np.nan\n",
    "# # DOI, Pubmed Id contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code--------------------- \n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_webofscience(source_path, output_path, 0, 10)\n",
    "# ---------------------end of test code--------------------- \n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=';')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_eupmc(source_path, output_path, start, end):\n",
    "    print(\"Starting preprocessing search results from Europe PMC...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, sep=\",\")\n",
    "    df = df[[\"SOURCE\", \"DOI\", \"EXTERNAL_ID\", \"PMCID\", \"TITLE\"]]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # sleep to avoid to be blocked\n",
    "        time.sleep(random.randint(1, 3))\n",
    "        # if(ind%50 == 0):\n",
    "        #     time.sleep(random.randint(10,15)*10)\n",
    "\n",
    "        # get pmid, doi\n",
    "        # SOURCE = {'PMC', 'MED', 'ETH', 'PPR'}\n",
    "        if df[\"SOURCE\"][ind] != \"MED\": # SOURCE is not \"MED\" \n",
    "            if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # doi is np.nan\n",
    "                doi = np.nan\n",
    "                pmid = np.nan\n",
    "            else:\n",
    "                doi = str(df[\"DOI\"][ind]).strip()\n",
    "                pmid = plib.doi2pmid(doi)\n",
    "        else: # SOURCE is \"MED\"\n",
    "            # get doi, pmid\n",
    "            if df[\"EXTERNAL_ID\"][ind] != df[\"EXTERNAL_ID\"][ind]: # EXTERNAL_ID is np.nan\n",
    "                if df[\"DOI\"][ind] != df[\"DOI\"][ind](): # DOI is np.nan\n",
    "                    doi = np.nan\n",
    "                    pmid = np.nan\n",
    "                else: # DOI is not np.nan\n",
    "                    doi = str(df[\"DOI\"][ind]).strip()\n",
    "                    pmid = plib.doi2pmid(doi)\n",
    "            else: # EXTERNAL_ID is not np.nan\n",
    "                pmid = str(df[\"EXTERNAL_ID\"][ind]).strip()\n",
    "                if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # DOI is np.nan\n",
    "                    doi = plib.pmid2doi(pmid)\n",
    "                else: # DOI is not np.nan\n",
    "                    doi = str(df[\"DOI\"][ind]).strip()\n",
    "                \n",
    "        # get pmcid, full_text_url, full_text_source\n",
    "        if pmid != pmid: # pmid is np.nan\n",
    "            pmcid = df[\"PMCID\"][ind]\n",
    "            if pmcid == pmcid: # pmcid is np.nan\n",
    "                full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "                full_text_source = \"PMC\"\n",
    "            elif doi == doi: # doi is not np.nan\n",
    "                full_text_url = \"https://doi.org/\" + str(doi).strip()\n",
    "                full_text_source = \"DOI\"\n",
    "            else:\n",
    "                full_text_url = np.nan\n",
    "                full_text_source = np.nan\n",
    "        else: # pmid is not np.nan\n",
    "            # request the webpage\n",
    "            url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "            # proxies = plib.get_proxies()\n",
    "            soup = plib.request_webpage(url)\n",
    "            # print(soup)\n",
    "\n",
    "            # get pmcid\n",
    "            try:\n",
    "                pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                pmcid = np.nan\n",
    "            # print(pmcid)\n",
    "            \n",
    "            # get full_text_url, full_text_source\n",
    "            if pmcid == pmcid: # pmcid is not np.nan\n",
    "                full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "                full_text_source = \"PMC\"\n",
    "            else: # pmcid is not np.nan\n",
    "                try:\n",
    "                    full_text_url = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"href\"].strip()\n",
    "                    full_text_source = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"data-ga-action\"].strip()\n",
    "                except:\n",
    "                    full_text_url = np.nan\n",
    "                    full_text_source = np.nan\n",
    "        \n",
    "        # get pdf_url, pdf_source\n",
    "        pdf_url = np.nan\n",
    "        pdf_source = np.nan\n",
    "\n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"Title\": [str(df[\"TITLE\"][ind]).strip()],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"full_text_source\": [full_text_source],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"pdf_source\": [pdf_source]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_eupmc\n",
    "# output_path = fpath.poten_litera_eupmc_processed\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, sep=',')\n",
    "# df = df[[\"SOURCE\", \"DOI\", \"EXTERNAL_ID\", \"PMCID\", \"TITLE\"]]\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "\n",
    "# col_one_list = set(df['SOURCE'].tolist())\n",
    "# print(col_one_list)\n",
    "# # ['PMC', 'MED', 'ETH', 'PPR']\n",
    "\n",
    "# print(df[\"SOURCE\"].isnull().values.any())\n",
    "# print(df[\"DOI\"].isnull().values.any())\n",
    "# print(df[\"EXTERNAL_ID\"].isnull().values.any())\n",
    "# print(df[\"PMCID\"].isnull().values.any())\n",
    "# print(df[\"TITLE\"].isnull().values.any())\n",
    "# # False, True, False, True, False\n",
    "# # SOURCE, EXTERNAL_ID, Title don't contain np.nan\n",
    "# # DOI, PMCID contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_eupmc(source_path, output_path, 0, 10)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_google_shcolar_step1(source_path, output_path, start, end):\n",
    "    print(\"Starting merging search results from Google Scholar...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, header=None, sep=',')\n",
    "    df.columns = [\"title\", \"url\", \"url_type\", \"full_text_url\", \"full_text_type\", \"full_text_source\"]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # df[\"url_type\"][ind]: {'[CITATION][C]', '[PDF][PDF]', '[BOOK][B]', nan, '[HTML][HTML]'}\n",
    "        # we don't need citations and books, as they are not likely to include connecivity information\n",
    "        if (df[\"url_type\"][ind] == \"[CITATION][C]\") or (df[\"url_type\"][ind] == \"[BOOK][B]\"):\n",
    "            continue\n",
    "        \n",
    "        # if url or title doesn't exsit AND full_text_url doesn't exist\n",
    "        if (df[\"url\"][ind] != df[\"url\"][ind]) or (df[\"title\"][ind] != df[\"title\"][ind]):\n",
    "            continue\n",
    "        \n",
    "        # now every row has at least title and url, and the url_text = {\"[PDF][PDF]\", nan, \"[HTML][HTML]\"}\n",
    "        if df[\"url_type\"][ind] == \"[PDF][PDF]\":\n",
    "            # full_text_type = {'[HTML]', nan, '[PDF]', 'UB'}\n",
    "            if df[\"full_text_type\"][ind] == \"[HTML]\":\n",
    "                link = str(df[\"full_text_url\"][ind]).strip()\n",
    "                full_text_url, status_code  = plib.get_final_redirected_url(link)\n",
    "                if full_text_url == full_text_url:\n",
    "                    full_text_source = full_text_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                else:\n",
    "                    full_text_source = np.nan\n",
    "            else:\n",
    "                full_text_url = np.nan\n",
    "                full_text_source = np.nan\n",
    "            # get pdf_url, pdf_source\n",
    "            link = str(df[\"url\"][ind]).strip()\n",
    "            pdf_url, status_code  = plib.get_final_redirected_url(link)\n",
    "            if pdf_url == pdf_url:\n",
    "                pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "            else:\n",
    "                pdf_source = np.nan\n",
    "        else: # df[\"url_type\"][ind] == nan or '[HTML][HTML]'\n",
    "            link = str(df[\"url\"][ind]).strip()\n",
    "            full_text_url, status_code = plib.get_final_redirected_url(link)\n",
    "            if full_text_url == full_text_url:\n",
    "                full_text_source = full_text_url.split(\"://\")[1].split(\"/\")[0]\n",
    "            else:\n",
    "                full_text_source = np.nan\n",
    "            # get pdf_url, pdf_source\n",
    "            # full_text_type = {'[HTML]', nan, '[PDF]', 'UB'}\n",
    "            if df[\"full_text_type\"][ind] == \"[PDF]\":\n",
    "                link = str(df[\"full_text_url\"][ind]).strip()\n",
    "                pdf_url, status_code  = plib.get_final_redirected_url(link)\n",
    "                if pdf_url == pdf_url:\n",
    "                    pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                else:\n",
    "                    pdf_source = np.nan\n",
    "            else:\n",
    "                pdf_url = np.nan\n",
    "                pdf_source = np.nan\n",
    "        \n",
    "        columns = [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "        row = {\n",
    "            \"Title\": [str(df[\"title\"][ind]).strip()],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"full_text_source\": [full_text_source],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"pdf_source\": [pdf_source]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_gs\n",
    "# output_path = fpath.poten_litera_gs_processed_step1\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"title\", \"url\", \"url_type\", \"full_text_url\", \"full_text_type\", \"full_text_source\"]\n",
    "# print(df.head(3))\n",
    "# print(df.head)\n",
    "\n",
    "# url_type = set(df['url_type'].tolist())\n",
    "# print(url_type)\n",
    "# # {'[CITATION][C]', '[PDF][PDF]', '[BOOK][B]', nan, '[HTML][HTML]'}\n",
    "# full_text_type = set(df['full_text_type'].tolist())\n",
    "# print(full_text_type)\n",
    "# # {nan, 'UB', '[HTML]', '[PDF]'}\n",
    "# full_text_source = set(df['full_text_source'].tolist())\n",
    "# print(full_text_source)\n",
    "# # {'ahajournals.org', 'lww.com', 'springer.com', 'academia.edu', 'plos.org', 'ieee.org', 'nature.com', \n",
    "# # 'mdpi.com', 'jpn.ca', 'uottawa.ca', nan, 'northwestern.edu', 'bmj.com', 'ekja.org', 'RWTH-Link', 'wiley.com', \n",
    "# # 'escholarship.org', 'nyu.edu', 'frontiersin.org', 'sciencedirect.com', 'eneuro.org', 'jneurosci.org', \n",
    "# # 'royalsocietypublishing.org', 'karger.com', 'harvard.edu', 'annualreviews.org', 'mcgill.ca', \n",
    "# # 'elifesciences.org', 'mirasmart.com', 'duke.edu', 'ucdavis.edu', 'physiology.org', 'cell.com', \n",
    "# # 'wustl.edu', 'epfl.ch', 'udc.es', 'psychiatryonline.org', 'jst.go.jp', 'core.ac.uk', 'rero.ch', \n",
    "# # 'zsp.com.pk', 'sagepub.com', 'europepmc.org', 'tandfonline.com', 'asahq.org', 'sonar.ch', 'koreamed.org', \n",
    "# # 'oup.com', 'science.org', 'scholarpedia.org', 'psu.edu', 'jordanbpeterson.com', 'pnas.org', 'uzh.ch', 'biorxiv.org', \n",
    "# # 'biomedcentral.com', 'umich.edu', 'ahuman.org', 'researchgate.net', 'ijpp.com', 'unav.edu', 'nih.gov', 'bu.edu'}\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# # [\"title\", \"url\", \"url_type\", \"full_text_url\", \"full_text_type\", \"full_text_source\"]\n",
    "# print(df[\"title\"].isnull().any().any())\n",
    "# print(df[\"url\"].isnull().any().any())\n",
    "# print(df[\"url_type\"].isnull().any().any())\n",
    "# print(df[\"full_text_url\"].isnull().any().any())\n",
    "# print(df[\"full_text_type\"].isnull().any().any())\n",
    "# print(df[\"full_text_source\"].isnull().any().any())\n",
    "# # True, True, True, True, True, True\n",
    "# # title, url, url_type, full_text_url, full_text_type, full_text_source contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_google_shcolar_step1(source_path, output_path, 0, 1000)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_google_shcolar_step2(source_path, output_path, start, end):\n",
    "    print(\"Starting merging search results from Google Scholar...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, header=None, sep=',')\n",
    "    df.columns = [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # get doi from url\n",
    "        if df[\"full_text_url\"][ind] == df[\"full_text_url\"][ind]: # there's a full_text_url\n",
    "            url = str(df[\"full_text_url\"][ind]).strip()\n",
    "            source = str(df[\"full_text_source\"][ind]).strip()\n",
    "            info = plib.extract_info_from_webpage(url)\n",
    "            doi = info[\"doi\"]\n",
    "            pmid = info[\"pmid\"]\n",
    "            pmcid = info[\"pmcid\"]\n",
    "        else:\n",
    "            doi = np.nan\n",
    "            pmid = np.nan\n",
    "            pmcid = np.nan\n",
    "        # # get pmid from DOI\n",
    "        # if doi == doi: # there's doi\n",
    "        #     pmid = plib.doi2pmid(doi)\n",
    "        # else: # doi not found\n",
    "        #     pmid = np.nan\n",
    "        # # get pmcid, full_text_url, full_text_source\n",
    "        # if pmid != pmid: # pmid is np.nan\n",
    "        #     pmcid = np.nan\n",
    "        #     if doi == doi: # doi is not np.nan\n",
    "        #         full_text_url = \"https://doi.org/\" + str(doi).strip()\n",
    "        #         full_text_source = \"DOI\"\n",
    "        #     else:\n",
    "        #         full_text_url = np.nan\n",
    "        #         full_text_source = np.nan\n",
    "        # else: # pmid is not np.nan\n",
    "        #     # request the webpage\n",
    "        #     url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "        #     # proxies = plib.get_proxies()\n",
    "        #     soup = plib.request_webpage(url)\n",
    "        #     # print(soup)\n",
    "\n",
    "        #     # get pmcid\n",
    "        #     try:\n",
    "        #         pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "        #     except:\n",
    "        #         pmcid = np.nan\n",
    "        #     # print(pmcid)\n",
    "            \n",
    "        #     # get full_text_url, full_text_source\n",
    "        #     if pmcid == pmcid: # pmcid is not np.nan\n",
    "        #         full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "        #         full_text_source = \"PMC\"\n",
    "        #     else: # pmcid is not np.nan\n",
    "        #         try:\n",
    "        #             full_text_url = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"href\"].strip()\n",
    "        #             full_text_source = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"data-ga-action\"].strip()\n",
    "        #         except:\n",
    "        #             full_text_url = np.nan\n",
    "        #             full_text_source = np.nan\n",
    "\n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"Title\": [df[\"Title\"][ind]],\n",
    "            \"full_text_url\": [df[\"full_text_url\"][ind]],\n",
    "            \"full_text_source\": [df[\"full_text_source\"][ind]],\n",
    "            \"pdf_url\": [df[\"pdf_url\"][ind]],\n",
    "            \"pdf_source\": [df[\"pdf_source\"][ind]]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(doi)\n",
    "        if doi != doi:\n",
    "            print([df[\"full_text_url\"][ind]])\n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_gs_processed_step1\n",
    "# output_path = fpath.poten_litera_gs_processed_step2\n",
    "# plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "# # # (905, 5)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# full_text_source = set(df['full_text_source'].tolist())\n",
    "# print(full_text_source)\n",
    "# # {'www.frontiersin.org', 'www.elibrary.ru', 'orca.cardiff.ac.uk', 'www.jneurosci.org', \n",
    "# # 'europepmc.org', 'www.theses.fr', 'www.biorxiv.org', 'submissions.mirasmart.com', \n",
    "# # 'royalsocietypublishing.org', 'www.science.org', 'thejns.org', \n",
    "# # 'escholarship.mcgill.ca', 'www.cambridge.org', 'movementdisorders.onlinelibrary.wiley.com', \n",
    "# # 'www.ahajournals.org', 'books.google.de', 'www.mdpi.com', 'www.sciencedirect.com', \n",
    "# # 'ieeexplore.ieee.org', 'academic.oup.com', 'www.pnas.org', 'physoc.onlinelibrary.wiley.com', \n",
    "# # 'www.jstage.jst.go.jp', 'wakespace.lib.wfu.edu', 'elibrary.ru', 'www.cabdirect.org', \n",
    "# # 'www.tandfonline.com', 'www.jpn.ca', 'jpet.aspetjournals.org', 'onlinelibrary.wiley.com', \n",
    "# # 'open.bu.edu', 'tbiomed.biomedcentral.com', 'www.liebertpub.com', 'journals.lww.com', \n",
    "# # 'agro.icm.edu.pl', 'ekja.org', 'analyticalsciencejournals.onlinelibrary.wiley.com', \n",
    "# # 'n.neurology.org', 'pubs.asahq.org', 'journals.sagepub.com', 'neuro.psychiatryonline.org', \n",
    "# # 'karger.com', 'nyaspubs.onlinelibrary.wiley.com', 'pure.mpg.de', 'elifesciences.org', \n",
    "# # 'link.springer.com', 'psycnet.apa.org', 'jnnp.bmj.com', 'www.degruyter.com', 'ajp.psychiatryonline.org', \n",
    "# # 'journals.physiology.org', 'www.nature.com', 'www.jstor.org', 'var.scholarpedia.org', 'www.eneuro.org', \n",
    "# # 'journals.plos.org', 'www.cell.com', 'www.ncbi.nlm.nih.gov', 'www.taylorfrancis.com', \n",
    "# # 'bmcneurosci.biomedcentral.com', nan, 'jamanetwork.com'}\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# # [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "# print(df[\"Title\"].isnull().any().any())\n",
    "# print(df[\"full_text_url\"].isnull().any().any())\n",
    "# print(df[\"full_text_source\"].isnull().any().any())\n",
    "# print(df[\"pdf_url\"].isnull().any().any())\n",
    "# print(df[\"pdf_source\"].isnull().any().any())\n",
    "# # False, True, True, True, True\n",
    "# # full_text_url, full_text_source, pdf_url, pdf_source contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_google_shcolar_step2(source_path, output_path, 0, 905)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_seed_paper_spanning(source_path, output_path):\n",
    "    print(\"Starting preprocessing search results from spanning citations of seed paper...\")\n",
    "    return True\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cocomac_paper(source_path, output_path):\n",
    "    print(\"Starting preprocessing search results from CoCoMac papers...\")\n",
    "    return True\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(input, output_path):\n",
    "    # combine all results\n",
    "    df = pd.DataFrame()\n",
    "    for search_result in input:\n",
    "        df_single = pd.read_csv(search_result, header=None, sep = \",\")\n",
    "        # df = df.append(df_single, ignore_index=True, sort=False)\n",
    "        df = pd.concat([df, df_single], ignore_index=True, sort=False)\n",
    "    df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "    df.to_csv(output_path, header=False, index=False)\n",
    "# --------------------start of test code--------------------\n",
    "# gos = fpath.poten_litera_gs_processed_step2\n",
    "# wos = fpath.poten_litera_wos_processed\n",
    "# pubmed = fpath.poten_litera_pubmed_processed\n",
    "# eupmc = fpath.poten_litera_eupmc_processed\n",
    "# input = [gos, wos, pubmed, eupmc]\n",
    "# output_path = fpath.poten_litera_combined\n",
    "# # plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# combine(input, output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "# # (14627, 8)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_in_missing(input_path, output_path, start, end):\n",
    "    df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "    df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "    \n",
    "    # fill in elements that are missing\n",
    "    for ind in range(start, end):\n",
    "        # initialzie\n",
    "        doi = np.nan\n",
    "        pmid = np.nan\n",
    "        pmcid = np.nan\n",
    "        title = str(df[\"Title\"][ind]).strip()\n",
    "        full_text_link = np.nan\n",
    "        pdf_link = np.nan\n",
    "\n",
    "        # doi, pmid\n",
    "        if df[\"DOI\"][ind] == df[\"DOI\"][ind]: # DOI -> PMID\n",
    "            doi = str(df[\"DOI\"][ind]).strip().lower()\n",
    "            # print(doi)\n",
    "            if df[\"PMID\"][ind] == df[\"PMID\"][ind]:\n",
    "                pmid = str(df[\"PMID\"][ind]).strip()\n",
    "                # print(pmid)\n",
    "            else:\n",
    "                pmid = plib.doi2pmid(doi)\n",
    "                # print(pmid)\n",
    "                if pmid != pmid:\n",
    "                    pmid_cadidate = plib.title2pmid(title)\n",
    "                    # print(pmid_cadidate)\n",
    "                    if pmid_cadidate == pmid_cadidate:   \n",
    "                        doi_validate, a = plib.pmid2doi_pmcid(pmid_cadidate)\n",
    "                        if doi_validate == doi_validate:\n",
    "                            doi_validate = doi_validate.lower()\n",
    "                            if doi_validate == doi:\n",
    "                                pmid = pmid_cadidate\n",
    "                                # print(pmid)\n",
    "        elif df[\"PMID\"][ind] == df[\"PMID\"][ind]: # PMID -> DOI\n",
    "            pmid = str(df[\"PMID\"][ind]).strip()\n",
    "            # print(pmid)\n",
    "            doi, pmcid = plib.pmid2doi_pmcid(str(pmid).strip())\n",
    "            # print(doi)\n",
    "        elif df[\"PMCID\"][ind] == df[\"PMCID\"][ind]: # PMCID -> DOI, PMID\n",
    "            pmcid = str(df[\"PMCID\"][ind]).strip()\n",
    "            doi, pmid = plib.pmcid2doi_pmid(pmcid)\n",
    "            # print(doi)\n",
    "            # print(pmid)\n",
    "        else:\n",
    "            doi = np.nan\n",
    "            pmid = np.nan\n",
    "        # print(doi)\n",
    "        # print(pmid)\n",
    "        \n",
    "        # pmcid\n",
    "        if df[\"PMCID\"][ind] == df[\"PMCID\"][ind]:\n",
    "            pmcid = str(df[\"PMCID\"][ind]).strip()\n",
    "        elif pmid == pmid:\n",
    "            doi_1, pmcid = plib.pmid2doi_pmcid(str(pmid).strip())\n",
    "        else:\n",
    "            pmcid = np.nan\n",
    "        # print(pmcid)\n",
    "\n",
    "        # full_text_link\n",
    "        if pmcid == pmcid:\n",
    "            full_text_link = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "        elif doi == doi:\n",
    "            full_text_link = plib.get_final_redirected_url(str(\"https://doi.org/\" + doi).strip())\n",
    "        elif df[\"full_text_url\"][ind] == df[\"full_text_url\"][ind]:\n",
    "            full_text_link = plib.get_final_redirected_url(df[\"full_text_url\"][ind])\n",
    "        else:\n",
    "            full_text_link = np.nan\n",
    "        # print(full_text_link)\n",
    "\n",
    "        # pdf_link\n",
    "        if df[\"pdf_url\"][ind] == df[\"pdf_url\"][ind]:\n",
    "            pdf_link = plib.get_final_redirected_url(str(df[\"pdf_url\"][ind]).strip())\n",
    "        else:\n",
    "            pdf_link = np.nan\n",
    "        # print(pdf_link)\n",
    "    \n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_link\", \"pdf_link\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi.lower()],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"Title\": [title],\n",
    "            \"full_text_link\": [full_text_link],\n",
    "            \"pdf_link\": [pdf_link],\n",
    "        }\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "\n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# input_path = fpath.poten_litera_combined\n",
    "# output_path = fpath.poten_litera_filled\n",
    "# # plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# fill_in_missing(input_path, output_path, 0, 14627)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_remove_dupli(input_path, output_path, identifiers): \n",
    "    df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "    df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_link\", \"pdf_link\"]\n",
    "\n",
    "    # remove all duplicates\n",
    "    for identifier in identifiers:\n",
    "        remove_dup_by = identifier\n",
    "        df = df[df[remove_dup_by].isnull() | ~df[df[remove_dup_by].notnull()].duplicated(subset=remove_dup_by, keep='first')]\n",
    "        # df = df.drop_duplicates(subset=['DOI'])\n",
    "        # df = df.drop_duplicates(subset=['PMID'])\n",
    "        # df = df.drop_duplicates(subset=['PMCID'])\n",
    "\n",
    "    # reset index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df.to_csv(output_path, header=False, index=False)\n",
    "    print(\"Duplication in the potential related literature removed.\")\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_ids_filled\n",
    "# output_path = fpath.poten_litra_filtered\n",
    "# plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# merge all search results\n",
    "# identifiers = [\"DOI\", \"PMID\", \"PMCID\"]\n",
    "# merge_remove_dupli(source_path, output_path, identifiers)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_text_link_filling(input_path, output_path, start, end):\n",
    "    df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "    df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_link\", \"pdf_link\"]\n",
    "    \n",
    "    for ind in range(start, end):\n",
    "        pmcid = df.at[ind, \"PMCID\"]\n",
    "        doi = df.at[ind, \"DOI\"]\n",
    "        full_text_link = np.nan\n",
    "\n",
    "        # get full text link\n",
    "        if pmcid == pmcid:\n",
    "            url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + str(pmcid).strip() + \"/\"\n",
    "            try:\n",
    "                full_text_link, status_code = plib.get_final_redirected_url(url)\n",
    "                if status_code == 403:\n",
    "                    link = \"https://pubmed.ncbi.nlm.nih.gov/\" + str(int(df.at[ind, \"PMID\"])).strip() + \"/\"\n",
    "                    soup = plib.request_webpage(link)\n",
    "                    link = soup.find(\"div\", {\"class\": \"full-text-links-list\"}).find(\"a\", {\"class\": \"link-item dialog-focus\"})[\"href\"]\n",
    "                    full_text_link, status_code = plib.get_final_redirected_url(link)\n",
    "            except:\n",
    "                raise Exception(\"Error when trying to get final redirected url from\", url)\n",
    "        \n",
    "        if full_text_link != full_text_link and df.at[ind, \"full_text_link\"] == df.at[ind, \"full_text_link\"]:\n",
    "            try:\n",
    "                full_text_link, status_code = plib.get_final_redirected_url(df.at[ind, \"full_text_link\"])\n",
    "            except:\n",
    "                raise Exception(\"Error when trying to get final redirected url from\", df.at[ind, \"full_text_link\"])\n",
    "        \n",
    "        if full_text_link != full_text_link and doi == doi:\n",
    "            url = \"https://doi.org/\" + str(doi).strip().lower()\n",
    "            try:\n",
    "                full_text_link, status_code = plib.get_final_redirected_url(url)\n",
    "            except:\n",
    "                raise Exception(\"Error when trying to get final redirected url from\", url)\n",
    "        \n",
    "        if  full_text_link != full_text_link and df[\"pdf_link\"][ind] == df[\"pdf_link\"][ind]:\n",
    "            full_text_link = np.nan\n",
    "        \n",
    "        if full_text_link != full_text_link:\n",
    "            continue\n",
    "        # print(full_text_link)\n",
    "\n",
    "        # get full text source\n",
    "        if full_text_link == full_text_link:\n",
    "            full_text_source = full_text_link.split(\"://\")[1].split(\"/\")[0]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if doi == doi:\n",
    "            doi = doi.lower()\n",
    "        \n",
    "        if df.at[ind, \"PMID\"] == df.at[ind, \"PMID\"]:\n",
    "            pmid = str(int(df.at[ind, \"PMID\"])).strip()\n",
    "        else:\n",
    "            pmid = np.nan\n",
    "        \n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_link\", \"full_text_source\", \"pdf_link\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"Title\": [df.at[ind, \"Title\"]],\n",
    "            \"full_text_link\": [full_text_link],\n",
    "            \"full_text_source\": [full_text_source],\n",
    "            \"pdf_link\": [df.at[ind, \"pdf_link\"]]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Main program: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from PubMed\n",
    "\n",
    "# source_path = fpath.poten_litera_pubmed\n",
    "# output_path = fpath.poten_litera_pubmed_processed\n",
    "\n",
    "# # clear the file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from PubMed\n",
    "# # 2606 results\n",
    "# preprocess_pubmed(source_path, output_path, columns, 2565, 2606)\n",
    "# print(\"preprocessing results from PubMed succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from PubMed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clear the file\n",
    "# plib.clear_file(fpath.poten_litera_wos)\n",
    "\n",
    "# # combine the 2 files of search results from web of science\n",
    "# source_path_1 = fpath.poten_litera_wos_1\n",
    "# source_path_2 = fpath.poten_litera_wos_2\n",
    "# df_1 = pd.read_csv(source_path_1, sep=';')\n",
    "# df_2 = pd.read_csv(source_path_2, sep=';')\n",
    "# df_1.to_csv(fpath.poten_litera_wos, header=True, index=False, sep=\";\")\n",
    "# df_2.to_csv(fpath.poten_litera_wos, mode=\"a\", header=False, index=False, sep=\";\")\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(fpath.poten_litera_wos, sep=';')\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "# (1976, 72)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from Web of Science\n",
    "\n",
    "# source_path = fpath.poten_litera_wos\n",
    "# output_path = fpath.poten_litera_wos_processed\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from Web of Science\n",
    "# # 1976 results\n",
    "# preprocess_webofscience(source_path, output_path, columns, 0, 1976)\n",
    "# print(\"preprocessing results from Web of Science succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from Web of Science!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from Europe PMC\n",
    "\n",
    "# source_path = fpath.poten_litera_eupmc\n",
    "# output_path = fpath.poten_litera_eupmc_processed\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from Europe PMC\n",
    "# preprocess_eupmc(source_path, output_path, columns, 0, 9140)\n",
    "# # 9140 results\n",
    "# print(\"preprocessing results from Europe PMC succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from Europe PMC!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from Google Scholar step 1\n",
    "\n",
    "# source_path = fpath.poten_litera_gs\n",
    "# output_path = fpath.poten_litera_gs_processed_step1\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from Google Scholar\n",
    "# preprocess_google_shcolar_step1(source_path, output_path, 0, 1000)\n",
    "# # 905 results\n",
    "# print(\"step 1 of preprocessing results from Google Scholar succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from Google Scholar step 1!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from Google Scholar step 2\n",
    "\n",
    "# source_path = fpath.poten_litera_gs_processed_step1\n",
    "# output_path = fpath.poten_litera_gs_processed_step2\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from Google Scholar\n",
    "# preprocess_google_shcolar_step2(source_path, output_path, 0, 905)\n",
    "# # 905 results\n",
    "# print(\"step 2 of preprocessing results from Google Scholar succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from Google Scholar step 2!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from spanning citations of seed paper\n",
    "\n",
    "# preprocess_seed_paper_spanning(source_path, output_path, columns):\n",
    "# print(\"preprocessing results from spanning citations of seed papers succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from spanning citations of seed papers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from CoCoMac papers\n",
    "\n",
    "# preprocess_cocomac_paper(source_path, output_path, columns)\n",
    "# print(\"preprocessing results from CoCoMac papers succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from CoCoMac papers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine all search results\n",
    "\n",
    "# gos = fpath.poten_litera_gs_processed_step2\n",
    "# wos = fpath.poten_litera_wos_processed\n",
    "# pubmed = fpath.poten_litera_pubmed_processed\n",
    "# eupmc = fpath.poten_litera_eupmc_processed\n",
    "# input = [gos, wos, pubmed, eupmc]\n",
    "# output_path = fpath.poten_litera_combined\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# combine(input, output_path)\n",
    "# # (14627, 8)\n",
    "# print(\"Combining all search results succeeded!\")\n",
    "# # # print(\"Attention! Something went wrong when combining all search results!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fill in elements missed, remove duplciations based on identifiers in the potential related literature\n",
    "\n",
    "# input_path = fpath.poten_litera_combined\n",
    "# output_path = fpath.poten_litera_ids_filled\n",
    "\n",
    "\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# fill_in_missing(input_path, output_path, 0, 14627)\n",
    "# print(\"Filling in missing elements succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when filling in missing elements!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge all search results and remove duplication by identifiers\n",
    "\n",
    "# source_path = fpath.poten_litera_ids_filled\n",
    "# output_path = fpath.poten_litra_filtered\n",
    "\n",
    "# # clear the file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# # merge all search results\n",
    "# identifiers = [\"DOI\", \"PMID\", \"PMCID\"]\n",
    "# merge_remove_dupli(source_path, output_path, identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_path = fpath.poten_litra_filtered\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# print(df.shape)\n",
    "# # (11021, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in full_text_link\n",
    "\n",
    "source_path = fpath.poten_litra_filtered\n",
    "output_path = fpath.poten_litera_ids_ftl_filled\n",
    "\n",
    "# clear the file\n",
    "plib.clear_file(output_path)\n",
    "\n",
    "# merge all search results\n",
    "full_text_link_filling(source_path, output_path, 0, 11021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reset the index of the dataframe and save it to a csv file\n",
    "# source_path = fpath.poten_litera_ids_ftl_filled\n",
    "# output_path = fpath.poten_litera_ids_ftl_filled\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df.to_csv(output_path, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_path = fpath.poten_litera_ids_ftl_filled\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# # print(df.head(5))\n",
    "# print(df.shape)\n",
    "# # (10769, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all possible full_text_source\n",
    "input_path = fpath.poten_litera_ids_ftl_filled\n",
    "df = pd.read_csv(input_path, header=None, sep=\",\")\n",
    "columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_link\", \"full_text_source\", \"pdf_link\"]\n",
    "df.columns = columns\n",
    "\n",
    "print(df.head(3))\n",
    "print(df.shape)\n",
    "# (10769, 7)\n",
    "\n",
    "full_text_source = set(df['full_text_source'].tolist())\n",
    "print(full_text_source)\n",
    "# {'pure.mpg.de', 'direct.mit.edu', 'pubs.acs.org', 'thejns.org', 'www.microbiologyresearch.org', 'www.ncbi.nlm.nih.gov', \n",
    "#  'journals.sagepub.com', 'journals.physiology.org', 'www.thieme-connect.de', 'www.jstage.jst.go.jp', 'www.rbojournal.org', \n",
    "#  'www.annualreviews.org', 'var.scholarpedia.org', 'ujms.net', 'papers.ssrn.com', 'www.degruyter.com', 'jamanetwork.com', \n",
    "#  'escholarship.mcgill.ca', 'www.tandfonline.com', 'wakespace.lib.wfu.edu', 'www.taylorfrancis.com', 'content.iospress.com:443', \n",
    "#  'www.cambridge.org', 'n.neurology.org', 'journals.biologists.com', 'www.nature.com', 'pubs.aip.org', 'books.google.de', \n",
    "#  'linkinghub.elsevier.com', 'academic.oup.com', 'link.springer.com', 'karger.com', 'neurologia.com', 'onlinelibrary.wiley.com', \n",
    "#  'www.ajtmh.org', 'iovs.arvojournals.org', 'elibrary.ru', 'psycnet.apa.org:443', 'journals.aps.org', 'royalsocietypublishing.org', \n",
    "#  'jpet.aspetjournals.org', 'www.biorxiv.org', 'ieeexplore.ieee.org', 'journals.lww.com', 'ekja.org', 'open.bu.edu', \n",
    "#  'www.cabdirect.org', 'www.elibrary.ru', 'jnm.snmjournals.org', 'www.architalbiol.org', 'www.imrpress.com', \n",
    "#  'neuro.psychiatryonline.org', 'submissions.mirasmart.com', 'pubs.asahq.org', 'europepmc.org', 'www.ahajournals.org', \n",
    "#  'www.science.org', 'nrc-prod.literatumonline.com', 'pharmrev.aspetjournals.org', 'www.liebertpub.com', 'opg.optica.org', \n",
    "#  'www.ingentaconnect.com', 'symposium.cshlp.org', 'ajp.psychiatryonline.org', 'webview.isho.jp', 'www.theses.fr', \n",
    "#  'www.worldscientific.com'}\n",
    "\n",
    "# [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_link\", \"full_text_source\", \"pdf_link\"]\n",
    "print(df[\"DOI\"].isnull().any().any()) # True\n",
    "print(df[\"PMID\"].isnull().any().any()) # True\n",
    "print(df[\"PMCID\"].isnull().any().any()) # True\n",
    "print(df[\"Title\"].isnull().any().any()) # False\n",
    "print(df[\"full_text_link\"].isnull().any().any()) # False\n",
    "print(df[\"full_text_source\"].isnull().any().any()) # False\n",
    "print(df[\"pdf_link\"].isnull().any().any()) # True\n",
    "\n",
    "print(df[\"DOI\"].dtypes) # object\n",
    "print(df[\"PMID\"].dtypes) # float64\n",
    "print(df[\"PMCID\"].dtypes) # object\n",
    "print(df[\"Title\"].dtypes) # object\n",
    "print(df[\"full_text_link\"].dtypes) # object\n",
    "print(df[\"full_text_source\"].dtypes) # object\n",
    "print(df[\"pdf_link\"].dtypes) # object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# websites_hosts\n",
    "# {'pure.mpg.de', 'direct.mit.edu', 'pubs.acs.org', 'thejns.org', 'www.microbiologyresearch.org', 'www.ncbi.nlm.nih.gov', \n",
    "#  'journals.sagepub.com', 'journals.physiology.org', 'www.thieme-connect.de', 'www.jstage.jst.go.jp', 'www.rbojournal.org', \n",
    "#  'www.annualreviews.org', 'var.scholarpedia.org', 'ujms.net', 'papers.ssrn.com', 'www.degruyter.com', 'jamanetwork.com', \n",
    "#  'escholarship.mcgill.ca', 'www.tandfonline.com', 'wakespace.lib.wfu.edu', 'www.taylorfrancis.com', 'content.iospress.com:443', \n",
    "#  'www.cambridge.org', 'n.neurology.org', 'journals.biologists.com', 'www.nature.com', 'pubs.aip.org', 'books.google.de', \n",
    "#  'linkinghub.elsevier.com', 'academic.oup.com', 'link.springer.com', 'karger.com', 'neurologia.com', 'onlinelibrary.wiley.com', \n",
    "#  'www.ajtmh.org', 'iovs.arvojournals.org', 'elibrary.ru', 'psycnet.apa.org:443', 'journals.aps.org', 'royalsocietypublishing.org', \n",
    "#  'jpet.aspetjournals.org', 'www.biorxiv.org', 'ieeexplore.ieee.org', 'journals.lww.com', 'ekja.org', 'open.bu.edu', \n",
    "#  'www.cabdirect.org', 'www.elibrary.ru', 'jnm.snmjournals.org', 'www.architalbiol.org', 'www.imrpress.com', \n",
    "#  'neuro.psychiatryonline.org', 'submissions.mirasmart.com', 'pubs.asahq.org', 'europepmc.org', 'www.ahajournals.org', \n",
    "#  'www.science.org', 'nrc-prod.literatumonline.com', 'pharmrev.aspetjournals.org', 'www.liebertpub.com', 'opg.optica.org', \n",
    "#  'www.ingentaconnect.com', 'symposium.cshlp.org', 'ajp.psychiatryonline.org', 'webview.isho.jp', 'www.theses.fr', \n",
    "#  'www.worldscientific.com'}\n",
    "websites_hosts = [\n",
    "    'karger.com', 'rbojournal.org', 'sagepub.com', 'neurology.org', 'asahq.org', 'aspetjournals.org', 'thieme-connect.de', \n",
    "    'taylorfrancis.com', 'lww.com', 'neurologia.com', 'ekja.org', 'www.imrpress.com', 'europepmc.org', 'springer.com', \n",
    "    'theses.fr', 'ieee.org', 'ssrn.com', 'nature.com', 'liebertpub.com', 'oup.com', 'open.bu.edu', 'journals.biologists.com', \n",
    "    'aip.org', 'mpg.de', 'lib.wfu.edu', 'cambridge.org', 'literatumonline.com', 'acs.org', 'scholarpedia.org', 'isho.jp', \n",
    "    'mirasmart.com', 'jstage.jst.go.jp', 'psychiatryonline.org', 'psycnet.apa.org', 'thejns.org', 'microbiologyresearch.org', \n",
    "    'wiley.com', 'snmjournals.org', 'degruyter.com', 'worldscientific.com', 'opg.optica.org', 'science.org', 'aps.org', \n",
    "    'ujms.net', 'mit.edu', 'biorxiv.org','annualreviews.org', 'elibrary.ru', 'www.ingentaconnect.com', 'mcgill.ca', \n",
    "    'symposium.cshlp.org', 'architalbiol.org', 'arvojournals.org', 'jamanetwork.com', 'elsevier.com', 'ncbi.nlm.nih.gov', \n",
    "    'cabdirect.org', 'books.google.de', 'iospress.com', 'tandfonline.com', 'ajtmh.org', 'royalsocietypublishing.org', \n",
    "    'ahajournals.org', 'physiology.org']\n",
    "# --------------------start of test code--------------------\n",
    "if len(websites_hosts) == len(set(websites_hosts)):\n",
    "    print(\"There are no duplicates in the list.\")\n",
    "else:\n",
    "    print(\"There are duplicates in the list.\")\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the websites by the number of articles they have\n",
    "input_path = fpath.poten_litera_ids_ftl_filled\n",
    "df = pd.read_csv(input_path, header=None, sep=\",\")\n",
    "columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_link\", \"full_text_source\", \"pdf_link\"]\n",
    "df.columns = columns\n",
    "func_dict = {website: 0 for website in websites_hosts}\n",
    "# print(func_dict)\n",
    "\n",
    "for ind in df.index:\n",
    "    for website in websites_hosts:\n",
    "        if website in df.loc[ind, \"full_text_source\"]:\n",
    "            func_dict[website] += 1\n",
    "            break\n",
    "\n",
    "# Sort dictionary by values\n",
    "sorted_dict = dict(sorted(func_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "print(sorted_dict)\n",
    "# {'ncbi.nlm.nih.gov': 7886, 'elsevier.com': 1019, 'wiley.com': 696, 'springer.com': 285, 'physiology.org': 205, \n",
    "#  'oup.com': 152, 'cambridge.org': 74, 'karger.com': 53, 'lww.com': 49, 'nature.com': 44, 'science.org': 30, \n",
    "#  'tandfonline.com': 29, 'sagepub.com': 21, 'jamanetwork.com': 20, 'neurology.org': 16, 'biorxiv.org': 15, \n",
    "#  'royalsocietypublishing.org': 13, 'psycnet.apa.org': 12, 'arvojournals.org': 12, 'jstage.jst.go.jp': 11, \n",
    "#  'psychiatryonline.org': 11, 'europepmc.org': 10, 'mit.edu': 10, 'thejns.org': 8, 'annualreviews.org': 8, \n",
    "#  'snmjournals.org': 7, 'aspetjournals.org': 6, 'elibrary.ru': 5, 'books.google.de': 5, 'architalbiol.org': 4, \n",
    "#  'ahajournals.org': 4, 'liebertpub.com': 3, 'acs.org': 3, 'degruyter.com': 3, 'worldscientific.com': 3, \n",
    "#  'iospress.com': 3, 'asahq.org': 2, 'thieme-connect.de': 2, 'neurologia.com': 2, 'mpg.de': 2, 'opg.optica.org': 2, \n",
    "#  'mcgill.ca': 2, 'rbojournal.org': 1, 'taylorfrancis.com': 1, 'ekja.org': 1, 'www.imrpress.com': 1, 'theses.fr': 1, \n",
    "#  'ieee.org': 1, 'ssrn.com': 1, 'open.bu.edu': 1, 'journals.biologists.com': 1, 'aip.org': 1, 'lib.wfu.edu': 1, \n",
    "#  'literatumonline.com': 1, 'scholarpedia.org': 1, 'isho.jp': 1, 'mirasmart.com': 1, 'microbiologyresearch.org': 1, \n",
    "#  'aps.org': 1, 'ujms.net': 1, 'www.ingentaconnect.com': 1, 'symposium.cshlp.org': 1, 'cabdirect.org': 1, 'ajtmh.org': 1}\n",
    "\n",
    "non_zero_keys = [key for key, value in sorted_dict.items() if value != 0]\n",
    "print(non_zero_keys)\n",
    "# ['ncbi.nlm.nih.gov', 'elsevier.com', 'wiley.com', 'springer.com', 'physiology.org', 'oup.com', 'cambridge.org', \n",
    "#  'karger.com', 'lww.com', 'nature.com', 'science.org', 'tandfonline.com', 'sagepub.com', 'jamanetwork.com', \n",
    "#  'neurology.org', 'biorxiv.org', 'royalsocietypublishing.org', 'psycnet.apa.org', 'arvojournals.org', 'jstage.jst.go.jp', \n",
    "#  'psychiatryonline.org', 'europepmc.org', 'mit.edu', 'thejns.org', 'annualreviews.org', 'snmjournals.org', \n",
    "#  'aspetjournals.org', 'elibrary.ru', 'books.google.de', 'architalbiol.org', 'ahajournals.org', 'liebertpub.com', \n",
    "#  'acs.org', 'degruyter.com', 'worldscientific.com', 'iospress.com', 'asahq.org', 'thieme-connect.de', 'neurologia.com', \n",
    "#  'mpg.de', 'opg.optica.org', 'mcgill.ca', 'rbojournal.org', 'taylorfrancis.com', 'ekja.org', 'www.imrpress.com', \n",
    "#  'theses.fr', 'ieee.org', 'ssrn.com', 'open.bu.edu', 'journals.biologists.com', 'aip.org', 'lib.wfu.edu', \n",
    "#  'literatumonline.com', 'scholarpedia.org', 'isho.jp', 'mirasmart.com', 'microbiologyresearch.org', 'aps.org', \n",
    "#  'ujms.net', 'www.ingentaconnect.com', 'symposium.cshlp.org', 'cabdirect.org', 'ajtmh.org']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# websites\n",
    "websites = [\n",
    "    'ncbi.nlm.nih.gov', 'elsevier.com', 'wiley.com', 'springer.com', 'physiology.org', 'oup.com', \n",
    "    'cambridge.org', 'karger.com', 'lww.com', 'nature.com', 'science.org', 'tandfonline.com', \n",
    "    'sagepub.com', 'jamanetwork.com', 'neurology.org', 'biorxiv.org', 'royalsocietypublishing.org', \n",
    "    'psycnet.apa.org', 'arvojournals.org', 'jstage.jst.go.jp', 'psychiatryonline.org', 'europepmc.org', \n",
    "    'mit.edu', 'thejns.org', 'annualreviews.org', 'snmjournals.org', 'aspetjournals.org', 'elibrary.ru', \n",
    "    'books.google.de', 'architalbiol.org', 'ahajournals.org', 'liebertpub.com', 'acs.org', 'degruyter.com', \n",
    "    'worldscientific.com', 'iospress.com', 'asahq.org', 'thieme-connect.de', 'neurologia.com', 'mpg.de', \n",
    "    'opg.optica.org', 'mcgill.ca', 'rbojournal.org', 'taylorfrancis.com', 'ekja.org', 'www.imrpress.com', \n",
    "    'theses.fr', 'ieee.org', 'ssrn.com', 'open.bu.edu', 'journals.biologists.com', 'aip.org', 'lib.wfu.edu', \n",
    "    'literatumonline.com', 'scholarpedia.org', 'isho.jp', 'mirasmart.com', 'microbiologyresearch.org', \n",
    "    'aps.org', 'ujms.net', 'www.ingentaconnect.com', 'symposium.cshlp.org', 'cabdirect.org', 'ajtmh.org'\n",
    "]\n",
    "# --------------------start of test code--------------------\n",
    "if len(websites) == len(websites_hosts):\n",
    "    print('The number of websites is correct')\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Next step: automatic filtering </h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
