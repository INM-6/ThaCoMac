{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Searched literature data preprocessing </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-13 01:00:53 GM01X680 metapub.config[95304] WARNING NCBI_API_KEY was not set.\n"
     ]
    }
   ],
   "source": [
    "# import internal .py modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Parameters: </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns of file: potential_related_literature.csv\n",
    "columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "# e.g., [\"10.1113/JP282626\", \"35851953\", \"PMC10087288\", \n",
    "#        \"Cortico-thalamocortical interactions for learning, memory and decision-making\",\n",
    "#        \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10087288/\", \"PMC\",\n",
    "#        \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10087288/pdf/TJP-601-25.pdf\", \"PMC\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Predefined fucntions: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pubmed(source_path, output_path, start, end):\n",
    "    print(\"Starting preprocessing search results from PubMed...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, sep=',')\n",
    "    df = df[[\"DOI\", \"PMID\", \"PMCID\", \"Title\"]]\n",
    "    \n",
    "    for ind in range(start, end):\n",
    "        # sleep to avoid to be blocked\n",
    "        time.sleep(random.randint(3, 5))\n",
    "        # if(ind%50 == 0):\n",
    "        #     time.sleep(random.randint(10,15)*10)\n",
    "        \n",
    "        # request the webpage\n",
    "        # the columns PMID, Title don't contain np.nan\n",
    "        pmid = str(df[\"PMID\"][ind]).strip()\n",
    "        url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "        # proxies = plib.get_proxies()\n",
    "        soup = plib.request_webpage(url)\n",
    "        # print(soup)\n",
    "        \n",
    "        # get pmcid\n",
    "        if df[\"PMCID\"][ind] != df[\"PMCID\"][ind]: # PMCID is np.nan\n",
    "            try:\n",
    "                pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                pmcid = np.nan\n",
    "        else: # PMCID is not np.nan\n",
    "            pmcid = str(df[\"PMCID\"][ind]).strip()\n",
    "        # print(pmcid)\n",
    "\n",
    "        # get doi\n",
    "        if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # DOI is np.nan\n",
    "            try:\n",
    "                doi = soup.find_all(\"span\", {\"class\": \"identifier doi\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                doi  = np.nan\n",
    "        else: # DOI is not np.nan\n",
    "            doi = str(df[\"DOI\"][ind]).strip()\n",
    "        # print(doi)\n",
    "\n",
    "        # get full_text_url, full_text_source\n",
    "        if pmcid == pmcid: # pmcid is not np.nan\n",
    "            full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "            full_text_source = \"PMC\"\n",
    "        else: # pmcid is np.nan\n",
    "            # PMC does not include this paper\n",
    "            try:\n",
    "                full_text_url = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"href\"].strip()\n",
    "                full_text_source = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"data-ga-action\"].strip()\n",
    "            except:\n",
    "                full_text_url = np.nan\n",
    "                full_text_source = np.nan\n",
    "        # print(full_text_url)\n",
    "        # print(full_text_source)\n",
    "        \n",
    "        # get pdf_url, pdf_source\n",
    "        pdf_url = np.nan\n",
    "        pdf_source = np.nan\n",
    "                \n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"Title\": [str(df[\"Title\"][ind]).strip()],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"full_text_source\": [full_text_source],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"pdf_source\": [pdf_source]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_pubmed\n",
    "# output_path = fpath.poten_litera_pubmed_processed\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, sep=',')\n",
    "# print(df.shape)\n",
    "# df = df[[\"DOI\", \"PMID\", \"PMCID\", \"Title\"]]\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "\n",
    "# print(df[\"DOI\"].isnull().values.any())\n",
    "# print(df[\"PMID\"].isnull().values.any())\n",
    "# print(df[\"PMCID\"].isnull().values.any())\n",
    "# print(df[\"Title\"].isnull().values.any())\n",
    "# # True, False, True, Flase\n",
    "# # PMID, Title don't contain np.nan\n",
    "# # DOI, PMCID contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_pubmed(source_path, output_path, start, end)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_webofscience(source_path, output_path, start, end):\n",
    "    print(\"Starting preprocessing search results from Web of Science...\")\n",
    "    \n",
    "    df = pd.read_csv(source_path, sep=\";\")\n",
    "    df = df[[\"DOI\", \"Pubmed Id\", \"Article Title\"]]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # sleep to avoid to be blocked\n",
    "        time.sleep(random.randint(3, 5))\n",
    "        # if(ind%50 == 0):\n",
    "        #     time.sleep(random.randint(10,15)*10)\n",
    "        \n",
    "        # the columns Article Title don't contain np.nan\n",
    "        # the columns DOI and PMID might contain np.nan\n",
    "        # get pmid, doi\n",
    "        if df[\"Pubmed Id\"][ind] != df[\"Pubmed Id\"][ind]: # Pubmed Id is np.nan\n",
    "            if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # DOI is np.nan\n",
    "                doi = np.nan\n",
    "                pmid = np.nan\n",
    "            else: # DOI is not np.nan\n",
    "                doi = str(df[\"DOI\"][ind]).strip()\n",
    "                pmid = plib.doi2pmid(doi)\n",
    "        else: # Pubmed Id is not np.nan\n",
    "            pmid = str(int(df[\"Pubmed Id\"][ind])).strip()\n",
    "            if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # DOI is not np.nan\n",
    "                doi = plib.pmid2doi(pmid)\n",
    "            else: # DOI is not np.nan\n",
    "                doi = str(df[\"DOI\"][ind]).strip()\n",
    "        \n",
    "        # get pmcid, full_text_url, full_text_source\n",
    "        if pmid != pmid: # pmid is np.nan\n",
    "            pmcid = np.nan\n",
    "            if doi != doi: # doi is np.nan\n",
    "                full_text_url = np.nan\n",
    "                full_text_source = np.nan\n",
    "            else:\n",
    "                full_text_url = \"https://doi.org/\" + str(doi).strip()\n",
    "                full_text_source = \"DOI\"\n",
    "        else: # pmid is not np.nan\n",
    "            # request the webpage\n",
    "            url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "            # proxies = plib.get_proxies()\n",
    "            soup = plib.request_webpage(url)\n",
    "            # print(soup)\n",
    "\n",
    "            # get pmcid\n",
    "            try:\n",
    "                pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                pmcid = np.nan\n",
    "            # print(pmcid)\n",
    "            \n",
    "            # get full_text_url, full_text_source\n",
    "            if pmcid == pmcid:\n",
    "                full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "                full_text_source = \"PMC\"\n",
    "            else:\n",
    "                try:\n",
    "                    full_text_url = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"href\"].strip()\n",
    "                    full_text_source = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"data-ga-action\"].strip()\n",
    "                except:\n",
    "                    full_text_url = np.nan\n",
    "                    full_text_source = np.nan\n",
    "        \n",
    "        # get pdf_url, pdf_source\n",
    "        pdf_url = np.nan\n",
    "        pdf_source = np.nan\n",
    "\n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"Title\": [str(df[\"Article Title\"][ind]).strip()],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"full_text_source\": [full_text_source],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"pdf_source\": [pdf_source]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# # source_path = fpath.poten_litera_wos\n",
    "# # output_path = fpath.poten_litera_wos_processed\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, sep=';')\n",
    "# df = df[[\"DOI\", \"Pubmed Id\", \"Article Title\"]]\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "\n",
    "# print(df[\"DOI\"].isnull().values.any())\n",
    "# print(df[\"Pubmed Id\"].isnull().values.any())\n",
    "# print(df[\"Article Title\"].isnull().values.any())\n",
    "# # True, True, False\n",
    "# # Article Title don't contain np.nan\n",
    "# # DOI, Pubmed Id contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code--------------------- \n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_webofscience(source_path, output_path, 0, 10)\n",
    "# ---------------------end of test code--------------------- \n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=';')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_eupmc(source_path, output_path, start, end):\n",
    "    print(\"Starting preprocessing search results from Europe PMC...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, sep=\",\")\n",
    "    df = df[[\"SOURCE\", \"DOI\", \"EXTERNAL_ID\", \"PMCID\", \"TITLE\"]]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # sleep to avoid to be blocked\n",
    "        time.sleep(random.randint(1, 3))\n",
    "        # if(ind%50 == 0):\n",
    "        #     time.sleep(random.randint(10,15)*10)\n",
    "\n",
    "        # get pmid, doi\n",
    "        # SOURCE = {'PMC', 'MED', 'ETH', 'PPR'}\n",
    "        if df[\"SOURCE\"][ind] != \"MED\": # SOURCE is not \"MED\" \n",
    "            if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # doi is np.nan\n",
    "                doi = np.nan\n",
    "                pmid = np.nan\n",
    "            else:\n",
    "                doi = str(df[\"DOI\"][ind]).strip()\n",
    "                pmid = plib.doi2pmid(doi)\n",
    "        else: # SOURCE is \"MED\"\n",
    "            # get doi, pmid\n",
    "            if df[\"EXTERNAL_ID\"][ind] != df[\"EXTERNAL_ID\"][ind]: # EXTERNAL_ID is np.nan\n",
    "                if df[\"DOI\"][ind] != df[\"DOI\"][ind](): # DOI is np.nan\n",
    "                    doi = np.nan\n",
    "                    pmid = np.nan\n",
    "                else: # DOI is not np.nan\n",
    "                    doi = str(df[\"DOI\"][ind]).strip()\n",
    "                    pmid = plib.doi2pmid(doi)\n",
    "            else: # EXTERNAL_ID is not np.nan\n",
    "                pmid = str(df[\"EXTERNAL_ID\"][ind]).strip()\n",
    "                if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # DOI is np.nan\n",
    "                    doi = plib.pmid2doi(pmid)\n",
    "                else: # DOI is not np.nan\n",
    "                    doi = str(df[\"DOI\"][ind]).strip()\n",
    "                \n",
    "        # get pmcid, full_text_url, full_text_source\n",
    "        if pmid != pmid: # pmid is np.nan\n",
    "            pmcid = df[\"PMCID\"][ind]\n",
    "            if pmcid == pmcid: # pmcid is np.nan\n",
    "                full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "                full_text_source = \"PMC\"\n",
    "            elif doi == doi: # doi is not np.nan\n",
    "                full_text_url = \"https://doi.org/\" + str(doi).strip()\n",
    "                full_text_source = \"DOI\"\n",
    "            else:\n",
    "                full_text_url = np.nan\n",
    "                full_text_source = np.nan\n",
    "        else: # pmid is not np.nan\n",
    "            # request the webpage\n",
    "            url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "            # proxies = plib.get_proxies()\n",
    "            soup = plib.request_webpage(url)\n",
    "            # print(soup)\n",
    "\n",
    "            # get pmcid\n",
    "            try:\n",
    "                pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                pmcid = np.nan\n",
    "            # print(pmcid)\n",
    "            \n",
    "            # get full_text_url, full_text_source\n",
    "            if pmcid == pmcid: # pmcid is not np.nan\n",
    "                full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "                full_text_source = \"PMC\"\n",
    "            else: # pmcid is not np.nan\n",
    "                try:\n",
    "                    full_text_url = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"href\"].strip()\n",
    "                    full_text_source = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"data-ga-action\"].strip()\n",
    "                except:\n",
    "                    full_text_url = np.nan\n",
    "                    full_text_source = np.nan\n",
    "        \n",
    "        # get pdf_url, pdf_source\n",
    "        pdf_url = np.nan\n",
    "        pdf_source = np.nan\n",
    "\n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"Title\": [str(df[\"TITLE\"][ind]).strip()],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"full_text_source\": [full_text_source],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"pdf_source\": [pdf_source]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_eupmc\n",
    "# output_path = fpath.poten_litera_eupmc_processed\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, sep=',')\n",
    "# df = df[[\"SOURCE\", \"DOI\", \"EXTERNAL_ID\", \"PMCID\", \"TITLE\"]]\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "\n",
    "# col_one_list = set(df['SOURCE'].tolist())\n",
    "# print(col_one_list)\n",
    "# # ['PMC', 'MED', 'ETH', 'PPR']\n",
    "\n",
    "# print(df[\"SOURCE\"].isnull().values.any())\n",
    "# print(df[\"DOI\"].isnull().values.any())\n",
    "# print(df[\"EXTERNAL_ID\"].isnull().values.any())\n",
    "# print(df[\"PMCID\"].isnull().values.any())\n",
    "# print(df[\"TITLE\"].isnull().values.any())\n",
    "# # False, True, False, True, False\n",
    "# # SOURCE, EXTERNAL_ID, Title don't contain np.nan\n",
    "# # DOI, PMCID contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_eupmc(source_path, output_path, 0, 10)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_google_shcolar_step1(source_path, output_path, start, end):\n",
    "    print(\"Starting merging search results from Google Scholar...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, header=None, sep=',')\n",
    "    df.columns = [\"title\", \"url\", \"url_type\", \"full_text_url\", \"full_text_type\", \"full_text_source\"]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # df[\"url_type\"][ind]: {'[CITATION][C]', '[PDF][PDF]', '[BOOK][B]', nan, '[HTML][HTML]'}\n",
    "        # we don't need citations and books, as they are not likely to include connecivity information\n",
    "        if (df[\"url_type\"][ind] == \"[CITATION][C]\") or (df[\"url_type\"][ind] == \"[BOOK][B]\"):\n",
    "            continue\n",
    "        \n",
    "        # if url or title doesn't exsit AND full_text_url doesn't exist\n",
    "        if (df[\"url\"][ind] != df[\"url\"][ind]) or (df[\"title\"][ind] != df[\"title\"][ind]):\n",
    "            continue\n",
    "        \n",
    "        # now every row has at least title and url, and the url_text = {\"[PDF][PDF]\", nan, \"[HTML][HTML]\"}\n",
    "        if df[\"url_type\"][ind] == \"[PDF][PDF]\":\n",
    "            # full_text_type = {'[HTML]', nan, '[PDF]', 'UB'}\n",
    "            if df[\"full_text_type\"][ind] == \"[HTML]\":\n",
    "                link = str(df[\"full_text_url\"][ind]).strip()\n",
    "                full_text_url = plib.get_final_redirected_url(link)\n",
    "                if full_text_url == full_text_url:\n",
    "                    full_text_source = full_text_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                else:\n",
    "                    full_text_source = np.nan\n",
    "            else:\n",
    "                full_text_url = np.nan\n",
    "                full_text_source = np.nan\n",
    "            # get pdf_url, pdf_source\n",
    "            link = str(df[\"url\"][ind]).strip()\n",
    "            pdf_url = plib.get_final_redirected_url(link)\n",
    "            if pdf_url == pdf_url:\n",
    "                pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "            else:\n",
    "                pdf_source = np.nan\n",
    "        else: # df[\"url_type\"][ind] == nan or '[HTML][HTML]'\n",
    "            link = str(df[\"url\"][ind]).strip()\n",
    "            full_text_url = plib.get_final_redirected_url(link)\n",
    "            if full_text_url == full_text_url:\n",
    "                full_text_source = full_text_url.split(\"://\")[1].split(\"/\")[0]\n",
    "            else:\n",
    "                full_text_source = np.nan\n",
    "            # get pdf_url, pdf_source\n",
    "            # full_text_type = {'[HTML]', nan, '[PDF]', 'UB'}\n",
    "            if df[\"full_text_type\"][ind] == \"[PDF]\":\n",
    "                link = str(df[\"full_text_url\"][ind]).strip()\n",
    "                pdf_url = plib.get_final_redirected_url(link)\n",
    "                if pdf_url == pdf_url:\n",
    "                    pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                else:\n",
    "                    pdf_source = np.nan\n",
    "            else:\n",
    "                pdf_url = np.nan\n",
    "                pdf_source = np.nan\n",
    "        \n",
    "        columns = [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "        row = {\n",
    "            \"Title\": [str(df[\"title\"][ind]).strip()],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"full_text_source\": [full_text_source],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"pdf_source\": [pdf_source]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_gs\n",
    "# output_path = fpath.poten_litera_gs_processed_step1\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"title\", \"url\", \"url_type\", \"full_text_url\", \"full_text_type\", \"full_text_source\"]\n",
    "# print(df.head(3))\n",
    "# print(df.head)\n",
    "\n",
    "# url_type = set(df['url_type'].tolist())\n",
    "# print(url_type)\n",
    "# # {'[CITATION][C]', '[PDF][PDF]', '[BOOK][B]', nan, '[HTML][HTML]'}\n",
    "# full_text_type = set(df['full_text_type'].tolist())\n",
    "# print(full_text_type)\n",
    "# # {nan, 'UB', '[HTML]', '[PDF]'}\n",
    "# full_text_source = set(df['full_text_source'].tolist())\n",
    "# print(full_text_source)\n",
    "# # {'ahajournals.org', 'lww.com', 'springer.com', 'academia.edu', 'plos.org', 'ieee.org', 'nature.com', \n",
    "# # 'mdpi.com', 'jpn.ca', 'uottawa.ca', nan, 'northwestern.edu', 'bmj.com', 'ekja.org', 'RWTH-Link', 'wiley.com', \n",
    "# # 'escholarship.org', 'nyu.edu', 'frontiersin.org', 'sciencedirect.com', 'eneuro.org', 'jneurosci.org', \n",
    "# # 'royalsocietypublishing.org', 'karger.com', 'harvard.edu', 'annualreviews.org', 'mcgill.ca', \n",
    "# # 'elifesciences.org', 'mirasmart.com', 'duke.edu', 'ucdavis.edu', 'physiology.org', 'cell.com', \n",
    "# # 'wustl.edu', 'epfl.ch', 'udc.es', 'psychiatryonline.org', 'jst.go.jp', 'core.ac.uk', 'rero.ch', \n",
    "# # 'zsp.com.pk', 'sagepub.com', 'europepmc.org', 'tandfonline.com', 'asahq.org', 'sonar.ch', 'koreamed.org', \n",
    "# # 'oup.com', 'science.org', 'scholarpedia.org', 'psu.edu', 'jordanbpeterson.com', 'pnas.org', 'uzh.ch', 'biorxiv.org', \n",
    "# # 'biomedcentral.com', 'umich.edu', 'ahuman.org', 'researchgate.net', 'ijpp.com', 'unav.edu', 'nih.gov', 'bu.edu'}\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# # [\"title\", \"url\", \"url_type\", \"full_text_url\", \"full_text_type\", \"full_text_source\"]\n",
    "# print(df[\"title\"].isnull().any().any())\n",
    "# print(df[\"url\"].isnull().any().any())\n",
    "# print(df[\"url_type\"].isnull().any().any())\n",
    "# print(df[\"full_text_url\"].isnull().any().any())\n",
    "# print(df[\"full_text_type\"].isnull().any().any())\n",
    "# print(df[\"full_text_source\"].isnull().any().any())\n",
    "# # True, True, True, True, True, True\n",
    "# # title, url, url_type, full_text_url, full_text_type, full_text_source contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_google_shcolar_step1(source_path, output_path, 0, 1000)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_google_shcolar_step2(source_path, output_path, start, end):\n",
    "    print(\"Starting merging search results from Google Scholar...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, header=None, sep=',')\n",
    "    df.columns = [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # get doi from url\n",
    "        if df[\"full_text_url\"][ind] == df[\"full_text_url\"][ind]: # there's a full_text_url\n",
    "            url = str(df[\"full_text_url\"][ind]).strip()\n",
    "            source = str(df[\"full_text_source\"][ind]).strip()\n",
    "            info = plib.extract_info_from_webpage(url)\n",
    "            doi = info[\"doi\"]\n",
    "            pmid = info[\"pmid\"]\n",
    "            pmcid = info[\"pmcid\"]\n",
    "        else:\n",
    "            doi = np.nan\n",
    "            pmid = np.nan\n",
    "            pmcid = np.nan\n",
    "        # # get pmid from DOI\n",
    "        # if doi == doi: # there's doi\n",
    "        #     pmid = plib.doi2pmid(doi)\n",
    "        # else: # doi not found\n",
    "        #     pmid = np.nan\n",
    "        # # get pmcid, full_text_url, full_text_source\n",
    "        # if pmid != pmid: # pmid is np.nan\n",
    "        #     pmcid = np.nan\n",
    "        #     if doi == doi: # doi is not np.nan\n",
    "        #         full_text_url = \"https://doi.org/\" + str(doi).strip()\n",
    "        #         full_text_source = \"DOI\"\n",
    "        #     else:\n",
    "        #         full_text_url = np.nan\n",
    "        #         full_text_source = np.nan\n",
    "        # else: # pmid is not np.nan\n",
    "        #     # request the webpage\n",
    "        #     url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "        #     # proxies = plib.get_proxies()\n",
    "        #     soup = plib.request_webpage(url)\n",
    "        #     # print(soup)\n",
    "\n",
    "        #     # get pmcid\n",
    "        #     try:\n",
    "        #         pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "        #     except:\n",
    "        #         pmcid = np.nan\n",
    "        #     # print(pmcid)\n",
    "            \n",
    "        #     # get full_text_url, full_text_source\n",
    "        #     if pmcid == pmcid: # pmcid is not np.nan\n",
    "        #         full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "        #         full_text_source = \"PMC\"\n",
    "        #     else: # pmcid is not np.nan\n",
    "        #         try:\n",
    "        #             full_text_url = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"href\"].strip()\n",
    "        #             full_text_source = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"data-ga-action\"].strip()\n",
    "        #         except:\n",
    "        #             full_text_url = np.nan\n",
    "        #             full_text_source = np.nan\n",
    "\n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"Title\": [df[\"Title\"][ind]],\n",
    "            \"full_text_url\": [df[\"full_text_url\"][ind]],\n",
    "            \"full_text_source\": [df[\"full_text_source\"][ind]],\n",
    "            \"pdf_url\": [df[\"pdf_url\"][ind]],\n",
    "            \"pdf_source\": [df[\"pdf_source\"][ind]]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(doi)\n",
    "        if doi != doi:\n",
    "            print([df[\"full_text_url\"][ind]])\n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "source_path = fpath.poten_litera_gs_processed_step1\n",
    "output_path = fpath.poten_litera_gs_processed_step2\n",
    "# plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "# # # (905, 5)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# full_text_source = set(df['full_text_source'].tolist())\n",
    "# print(full_text_source)\n",
    "# # {'www.frontiersin.org', 'www.elibrary.ru', 'orca.cardiff.ac.uk', 'www.jneurosci.org', \n",
    "# # 'europepmc.org', 'www.theses.fr', 'www.biorxiv.org', 'submissions.mirasmart.com', \n",
    "# # 'royalsocietypublishing.org', 'www.science.org', 'thejns.org', \n",
    "# # 'escholarship.mcgill.ca', 'www.cambridge.org', 'movementdisorders.onlinelibrary.wiley.com', \n",
    "# # 'www.ahajournals.org', 'books.google.de', 'www.mdpi.com', 'www.sciencedirect.com', \n",
    "# # 'ieeexplore.ieee.org', 'academic.oup.com', 'www.pnas.org', 'physoc.onlinelibrary.wiley.com', \n",
    "# # 'www.jstage.jst.go.jp', 'wakespace.lib.wfu.edu', 'elibrary.ru', 'www.cabdirect.org', \n",
    "# # 'www.tandfonline.com', 'www.jpn.ca', 'jpet.aspetjournals.org', 'onlinelibrary.wiley.com', \n",
    "# # 'open.bu.edu', 'tbiomed.biomedcentral.com', 'www.liebertpub.com', 'journals.lww.com', \n",
    "# # 'agro.icm.edu.pl', 'ekja.org', 'analyticalsciencejournals.onlinelibrary.wiley.com', \n",
    "# # 'n.neurology.org', 'pubs.asahq.org', 'journals.sagepub.com', 'neuro.psychiatryonline.org', \n",
    "# # 'karger.com', 'nyaspubs.onlinelibrary.wiley.com', 'pure.mpg.de', 'elifesciences.org', \n",
    "# # 'link.springer.com', 'psycnet.apa.org', 'jnnp.bmj.com', 'www.degruyter.com', 'ajp.psychiatryonline.org', \n",
    "# # 'journals.physiology.org', 'www.nature.com', 'www.jstor.org', 'var.scholarpedia.org', 'www.eneuro.org', \n",
    "# # 'journals.plos.org', 'www.cell.com', 'www.ncbi.nlm.nih.gov', 'www.taylorfrancis.com', \n",
    "# # 'bmcneurosci.biomedcentral.com', nan, 'jamanetwork.com'}\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# # [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "# print(df[\"Title\"].isnull().any().any())\n",
    "# print(df[\"full_text_url\"].isnull().any().any())\n",
    "# print(df[\"full_text_source\"].isnull().any().any())\n",
    "# print(df[\"pdf_url\"].isnull().any().any())\n",
    "# print(df[\"pdf_source\"].isnull().any().any())\n",
    "# # False, True, True, True, True\n",
    "# # full_text_url, full_text_source, pdf_url, pdf_source contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_google_shcolar_step2(source_path, output_path, 0, 905)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_seed_paper_spanning(source_path, output_path):\n",
    "    print(\"Starting preprocessing search results from spanning citations of seed paper...\")\n",
    "    return True\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cocomac_paper(source_path, output_path):\n",
    "    print(\"Starting preprocessing search results from CoCoMac papers...\")\n",
    "    return True\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(input, output_path):\n",
    "    # combine all results\n",
    "    df = pd.DataFrame()\n",
    "    for search_result in input:\n",
    "        df_single = pd.read_csv(search_result, header=None, sep = \",\")\n",
    "        # df = df.append(df_single, ignore_index=True, sort=False)\n",
    "        df = pd.concat([df, df_single], ignore_index=True, sort=False)\n",
    "    df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "    df.to_csv(output_path, header=False, index=False)\n",
    "# --------------------start of test code--------------------\n",
    "# gos = fpath.poten_litera_gs_processed_step2\n",
    "# wos = fpath.poten_litera_wos_processed\n",
    "# pubmed = fpath.poten_litera_pubmed_processed\n",
    "# eupmc = fpath.poten_litera_eupmc_processed\n",
    "# input = [gos, wos, pubmed, eupmc]\n",
    "# output_path = fpath.poten_litera_combined\n",
    "# # plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# combine(input, output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "# # (14627, 8)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in elements missed, remove duplciations based on identifiers in the potential related literature\n",
    "def fill_in_missing(input_path, output_path, start, end):\n",
    "    df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "    df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "    \n",
    "    # fill in elements that are missing\n",
    "    for ind in range(start, end):\n",
    "        # print(ind)\n",
    "        # initialzie\n",
    "        doi = np.nan\n",
    "        pmid = np.nan\n",
    "        pmcid = np.nan\n",
    "        title = str(df[\"Title\"][ind]).strip()\n",
    "        # print(title)\n",
    "        full_text_link = np.nan\n",
    "        pdf_link = np.nan\n",
    "\n",
    "        # doi, pmid\n",
    "        if df[\"DOI\"][ind] == df[\"DOI\"][ind]: # DOI -> PMID\n",
    "            doi = str(df[\"DOI\"][ind]).strip().lower()\n",
    "            # print(doi)\n",
    "            if df[\"PMID\"][ind] == df[\"PMID\"][ind]:\n",
    "                pmid = str(df[\"PMID\"][ind]).strip()\n",
    "                # print(pmid)\n",
    "            else:\n",
    "                pmid = plib.doi2pmid(doi)\n",
    "                # print(pmid)\n",
    "                if pmid != pmid:\n",
    "                    pmid_cadidate = plib.title2pmid(title)\n",
    "                    # print(pmid_cadidate)\n",
    "                    if pmid_cadidate == pmid_cadidate:   \n",
    "                        doi_validate, a = plib.pmid2doi_pmcid(pmid_cadidate)\n",
    "                        if doi_validate == doi_validate:\n",
    "                            doi_validate = doi_validate.lower()\n",
    "                            if doi_validate == doi:\n",
    "                                pmid = pmid_cadidate\n",
    "                                # print(pmid)\n",
    "        elif df[\"PMID\"][ind] == df[\"PMID\"][ind]: # PMID -> DOI\n",
    "            pmid = str(df[\"PMID\"][ind]).strip()\n",
    "            # print(pmid)\n",
    "            doi, pmcid = plib.pmid2doi_pmcid(str(pmid).strip())\n",
    "            # print(doi)\n",
    "        elif df[\"PMCID\"][ind] == df[\"PMCID\"][ind]: # PMCID -> DOI, PMID\n",
    "            pmcid = str(df[\"PMCID\"][ind]).strip()\n",
    "            doi, pmid = plib.pmcid2doi_pmid(pmcid)\n",
    "            # print(doi)\n",
    "            # print(pmid)\n",
    "        else:\n",
    "            doi = np.nan\n",
    "            pmid = np.nan\n",
    "        # print(doi)\n",
    "        # print(pmid)\n",
    "        \n",
    "        # pmcid\n",
    "        if df[\"PMCID\"][ind] == df[\"PMCID\"][ind]:\n",
    "            pmcid = str(df[\"PMCID\"][ind]).strip()\n",
    "        elif pmid == pmid:\n",
    "            doi_1, pmcid = plib.pmid2doi_pmcid(str(pmid).strip())\n",
    "        else:\n",
    "            pmcid = np.nan\n",
    "        # print(pmcid)\n",
    "\n",
    "        # full_text_link\n",
    "        if pmcid == pmcid:\n",
    "            full_text_link = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "        elif doi == doi:\n",
    "            full_text_link = plib.get_final_redirected_url(str(\"https://doi.org/\" + doi).strip())\n",
    "        elif df[\"full_text_url\"][ind] == df[\"full_text_url\"][ind]:\n",
    "            full_text_link = plib.get_final_redirected_url(df[\"full_text_url\"][ind])\n",
    "        else:\n",
    "            full_text_link = np.nan\n",
    "        # print(full_text_link)\n",
    "\n",
    "        # pdf_link\n",
    "        if df[\"pdf_url\"][ind] == df[\"pdf_url\"][ind]:\n",
    "            pdf_link = plib.get_final_redirected_url(str(df[\"pdf_url\"][ind]).strip())\n",
    "        else:\n",
    "            pdf_link = np.nan\n",
    "        # print(pdf_link)\n",
    "    \n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_link\", \"pdf_link\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi.lower()],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"Title\": [title],\n",
    "            \"full_text_link\": [full_text_link],\n",
    "            \"pdf_link\": [pdf_link],\n",
    "        }\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "\n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# input_path = fpath.poten_litera_combined\n",
    "# output_path = fpath.poten_litera_filled\n",
    "# # plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# fill_in_missing(input_path, output_path, 0, 14627)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_remove_dupli(input_path, output_path, identifiers): \n",
    "    df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "    df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_link\", \"pdf_link\"]\n",
    "\n",
    "    # remove all duplicates\n",
    "    for identifier in identifiers:\n",
    "        remove_dup_by = identifier\n",
    "        df = df[df[remove_dup_by].isnull() | ~df[df[remove_dup_by].notnull()].duplicated(subset=remove_dup_by, keep='first')]\n",
    "        # df = df.drop_duplicates(subset=['DOI'])\n",
    "        # df = df.drop_duplicates(subset=['PMID'])\n",
    "        # df = df.drop_duplicates(subset=['PMCID'])\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # add full_text_source\n",
    "    for ind in range(len(df)):\n",
    "        # get a valid full_text_link\n",
    "        if df.at[ind, \"full_text_link\"] == df.at[ind, \"full_text_link\"]:\n",
    "            try:\n",
    "                full_text_link = plib.get_final_redirected_url(df.at[ind, \"full_text_link\"])\n",
    "                # full_text_link = url\n",
    "            except:\n",
    "                if df.at[ind, \"DOI\"] == df.at[ind, \"DOI\"]:\n",
    "                    url = \"https://doi.org/\" + df.at[ind, \"DOI\"]\n",
    "                    try:\n",
    "                        full_text_link = plib.get_final_redirected_url(url)\n",
    "                    except:\n",
    "                        raise Exception(\"Error when trying to get final redirected url from a full_text_link and doi-link\")\n",
    "                elif df.at[ind, \"pdf_link\"] == df.at[ind, \"pdf_link\"]:\n",
    "                    full_text_link = np.nan\n",
    "                else:\n",
    "                    continue\n",
    "        elif df.at[ind, \"DOI\"] == df.at[ind, \"DOI\"]:\n",
    "            url = \"https://doi.org/\" + df.at[ind, \"DOI\"]\n",
    "            try:\n",
    "                full_text_link = plib.get_final_redirected_url(url)\n",
    "            except:\n",
    "                raise Exception(\"Error when trying to get final redirected url from a doi link\")\n",
    "        elif df.at[ind, \"pdf_link\"] == df.at[ind, \"pdf_link\"]:\n",
    "            full_text_link = np.nan\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # get full text source\n",
    "        if full_text_link == full_text_link:\n",
    "            full_text_source = full_text_link.split(\"://\")[1].split(\"/\")[0]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_link\", \"full_text_source\", \"pdf_link\"]\n",
    "        row = {\n",
    "            \"DOI\": [df.at[ind, \"DOI\"]],\n",
    "            \"PMID\": [df.at[ind, \"PMID\"]],\n",
    "            \"PMCID\": [df.at[ind, \"PMCID\"]],\n",
    "            \"Title\": [df.at[ind, \"Title\"]],\n",
    "            \"full_text_link\": [df.at[ind, \"full_text_link\"]],\n",
    "            \"full_text_source\": [full_text_source],\n",
    "            \"pdf_link\": [df.at[ind, \"pdf_link\"]],\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "\n",
    "    print(\"Duplication in the potential related literature removed.\")\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_ids_filled\n",
    "# output_path = fpath.poten_litra_filtered\n",
    "# plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# merge all search results\n",
    "# identifiers = [\"DOI\", \"PMID\", \"PMCID\"]\n",
    "# merge_remove_dupli(source_path, output_path, identifiers)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "# # (11021, 6)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Main program: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from PubMed\n",
    "\n",
    "# source_path = fpath.poten_litera_pubmed\n",
    "# output_path = fpath.poten_litera_pubmed_processed\n",
    "\n",
    "# # clear the file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from PubMed\n",
    "# # 2606 results\n",
    "# preprocess_pubmed(source_path, output_path, columns, 2565, 2606)\n",
    "# print(\"preprocessing results from PubMed succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from PubMed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clear the file\n",
    "# plib.clear_file(fpath.poten_litera_wos)\n",
    "\n",
    "# # combine the 2 files of search results from web of science\n",
    "# source_path_1 = fpath.poten_litera_wos_1\n",
    "# source_path_2 = fpath.poten_litera_wos_2\n",
    "# df_1 = pd.read_csv(source_path_1, sep=';')\n",
    "# df_2 = pd.read_csv(source_path_2, sep=';')\n",
    "# df_1.to_csv(fpath.poten_litera_wos, header=True, index=False, sep=\";\")\n",
    "# df_2.to_csv(fpath.poten_litera_wos, mode=\"a\", header=False, index=False, sep=\";\")\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(fpath.poten_litera_wos, sep=';')\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "# (1976, 72)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from Web of Science\n",
    "\n",
    "# source_path = fpath.poten_litera_wos\n",
    "# output_path = fpath.poten_litera_wos_processed\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from Web of Science\n",
    "# # 1976 results\n",
    "# preprocess_webofscience(source_path, output_path, columns, 0, 1976)\n",
    "# print(\"preprocessing results from Web of Science succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from Web of Science!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from Europe PMC\n",
    "\n",
    "# source_path = fpath.poten_litera_eupmc\n",
    "# output_path = fpath.poten_litera_eupmc_processed\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from Europe PMC\n",
    "# preprocess_eupmc(source_path, output_path, columns, 0, 9140)\n",
    "# # 9140 results\n",
    "# print(\"preprocessing results from Europe PMC succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from Europe PMC!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from Google Scholar step 1\n",
    "\n",
    "# source_path = fpath.poten_litera_gs\n",
    "# output_path = fpath.poten_litera_gs_processed_step1\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from Google Scholar\n",
    "# preprocess_google_shcolar_step1(source_path, output_path, 0, 1000)\n",
    "# # 905 results\n",
    "# print(\"step 1 of preprocessing results from Google Scholar succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from Google Scholar step 1!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from Google Scholar step 2\n",
    "\n",
    "# source_path = fpath.poten_litera_gs_processed_step1\n",
    "# output_path = fpath.poten_litera_gs_processed_step2\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from Google Scholar\n",
    "# preprocess_google_shcolar_step2(source_path, output_path, 0, 905)\n",
    "# # 905 results\n",
    "# print(\"step 2 of preprocessing results from Google Scholar succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from Google Scholar step 2!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from spanning citations of seed paper\n",
    "\n",
    "# preprocess_seed_paper_spanning(source_path, output_path, columns):\n",
    "# print(\"preprocessing results from spanning citations of seed papers succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from spanning citations of seed papers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from CoCoMac papers\n",
    "\n",
    "# preprocess_cocomac_paper(source_path, output_path, columns)\n",
    "# print(\"preprocessing results from CoCoMac papers succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from CoCoMac papers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine all search results\n",
    "\n",
    "# gos = fpath.poten_litera_gs_processed_step2\n",
    "# wos = fpath.poten_litera_wos_processed\n",
    "# pubmed = fpath.poten_litera_pubmed_processed\n",
    "# eupmc = fpath.poten_litera_eupmc_processed\n",
    "# input = [gos, wos, pubmed, eupmc]\n",
    "# output_path = fpath.poten_litera_combined\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# combine(input, output_path)\n",
    "# # (14627, 8)\n",
    "# print(\"Combining all search results succeeded!\")\n",
    "# # # print(\"Attention! Something went wrong when combining all search results!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fill in elements missed, remove duplciations based on identifiers in the potential related literature\n",
    "\n",
    "# input_path = fpath.poten_litera_combined\n",
    "# output_path = fpath.poten_litera_ids_filled\n",
    "\n",
    "\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# fill_in_missing(input_path, output_path, 0, 14627)\n",
    "# print(\"Filling in missing elements succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when filling in missing elements!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge all search results and remove duplication by identifiers\n",
    "\n",
    "# source_path = fpath.poten_litera_ids_filled\n",
    "# output_path = fpath.poten_litra_filtered\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # merge all search results\n",
    "# identifiers = [\"DOI\", \"PMID\", \"PMCID\"]\n",
    "# merge_remove_dupli(source_path, output_path, identifiers)\n",
    "# # 11021 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reset the index of the dataframe and save it to a csv file\n",
    "# source_path = fpath.poten_litra_filtered\n",
    "# output_path = fpath.poten_litra_filtered\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df.to_csv(output_path, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    0           1           2  \\\n",
      "0            10.1152/jn.2001.85.1.219  11152722.0         NaN   \n",
      "1                                 NaN         NaN         NaN   \n",
      "2    10.1016/j.neuroimage.2006.07.032  17140815.0         NaN   \n",
      "3  10.1523/jneurosci.14-05-02485.1994   7514207.0  PMC6577493   \n",
      "4               10.1002/cne.903130106   1761756.0         NaN   \n",
      "\n",
      "                                                   3  \\\n",
      "0  Effect of attentive fixation in macaque thalam...   \n",
      "1  The thalamus of the Macaca, mulatta. An atlas ...   \n",
      "2  Distribution of the dopamine innervation in th...   \n",
      "3  Medial lemniscal and spinal projections to the...   \n",
      "4  Diverse thalamic projections to the prefrontal...   \n",
      "\n",
      "                                                   4                        5  \\\n",
      "0  https://journals.physiology.org/doi/10.1152/jn...  journals.physiology.org   \n",
      "1  https://www.cabdirect.org/cabdirect/welcome/?t...        www.cabdirect.org   \n",
      "2  https://linkinghub.elsevier.com/retrieve/pii/S...  linkinghub.elsevier.com   \n",
      "3  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6...     www.ncbi.nlm.nih.gov   \n",
      "4  https://onlinelibrary.wiley.com/doi/10.1002/cn...  onlinelibrary.wiley.com   \n",
      "\n",
      "                                                   6  \n",
      "0  https://journals.physiology.org/doi/pdf/10.115...  \n",
      "1                                                NaN  \n",
      "2                                                NaN  \n",
      "3  https://www.jneurosci.org/content/jneuro/14/5/...  \n",
      "4  https://onlinelibrary.wiley.com/doi/pdf/10.100...  \n",
      "(10768, 7)\n"
     ]
    }
   ],
   "source": [
    "source_path = fpath.poten_litra_filtered\n",
    "df = pd.read_csv(source_path, header=None, sep=',')\n",
    "print(df.head(5))\n",
    "print(df.shape)\n",
    "# (10768, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Next step: automatic filtering </h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
