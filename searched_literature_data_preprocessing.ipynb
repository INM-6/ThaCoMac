{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Searched literature data preprocessing </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib\n",
    "import extract_info\n",
    "import parameters as params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Parameters: </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns of file: potential_related_literature.csv\n",
    "columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Predefined fucntions: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pubmed(source_path, output_path, start, end):\n",
    "    print(\"Starting preprocessing search results from PubMed...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, sep=',')\n",
    "    df = df[[\"DOI\", \"PMID\", \"PMCID\", \"Title\"]]\n",
    "    \n",
    "    for ind in range(start, end):\n",
    "        # sleep to avoid to be blocked\n",
    "        time.sleep(random.randint(1, 3))\n",
    "        \n",
    "        # request the webpage\n",
    "        # the columns PMID, Title don't contain np.nan\n",
    "        pmid = str(df[\"PMID\"][ind]).strip()\n",
    "        url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "        # proxies = plib.get_proxies()\n",
    "        soup = plib.request_webpage(url)\n",
    "        # print(soup)\n",
    "        \n",
    "        # get pmcid\n",
    "        if df[\"PMCID\"][ind] != df[\"PMCID\"][ind]: # PMCID is np.nan\n",
    "            try:\n",
    "                pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                pmcid = np.nan\n",
    "        else: # PMCID is not np.nan\n",
    "            pmcid = str(df[\"PMCID\"][ind]).strip()\n",
    "        # print(pmcid)\n",
    "\n",
    "        # get doi\n",
    "        if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # DOI is np.nan\n",
    "            try:\n",
    "                doi = soup.find_all(\"span\", {\"class\": \"identifier doi\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                doi  = np.nan\n",
    "        else: # DOI is not np.nan\n",
    "            doi = str(df[\"DOI\"][ind]).strip()\n",
    "        # print(doi)\n",
    "\n",
    "        # get full_text_url\n",
    "        # if pmcid == pmcid: # pmcid is not np.nan\n",
    "        #     full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "        #     full_text_source = \"PMC\"\n",
    "        # else: # pmcid is np.nan\n",
    "        #     # PMC does not include this paper\n",
    "        #     try:\n",
    "        #         full_text_url = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"href\"].strip()\n",
    "        #         full_text_source = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"data-ga-action\"].strip()\n",
    "        #     except:\n",
    "        #         full_text_url = np.nan\n",
    "        #         full_text_source = np.nan\n",
    "        full_text_url = np.nan\n",
    "        # print(full_text_url)\n",
    "        \n",
    "        # get pdf_url\n",
    "        pdf_url = np.nan\n",
    "        title = (df.at[ind, \"Title\"]).strip()\n",
    "        abstract = np.nan\n",
    "        keywords = np.nan\n",
    "        \n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"Title\": [title],\n",
    "            \"Abstract\": [abstract],\n",
    "            \"Keywords\": [keywords]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_pubmed\n",
    "# output_path = fpath.poten_litera_pubmed_processed\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, sep=',')\n",
    "# print(df.shape)\n",
    "# # (2612, 11)\n",
    "# df = df[[\"DOI\", \"PMID\", \"PMCID\", \"Title\"]]\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "\n",
    "# print(df[\"DOI\"].isnull().values.any())\n",
    "# print(df[\"PMID\"].isnull().values.any())\n",
    "# print(df[\"PMCID\"].isnull().values.any())\n",
    "# print(df[\"Title\"].isnull().values.any())\n",
    "# # True, False, True, Flase\n",
    "# # PMID, Title don't contain np.nan\n",
    "# # DOI, PMCID contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_pubmed(source_path, output_path, start, end)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_webofscience(source_path, output_path, start, end):\n",
    "    print(\"Starting preprocessing search results from Web of Science...\")\n",
    "    \n",
    "    df = pd.read_csv(source_path, sep=\",\")\n",
    "    df = df[[\"DOI\", \"Pubmed Id\", \"Article Title\", \"Abstract\", \"Author Keywords\", \"Keywords Plus\"]]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # sleep to avoid to be blocked\n",
    "        time.sleep(random.randint(1, 3))\n",
    "        \n",
    "        # the columns Article Title don't contain np.nan\n",
    "        # the columns DOI and PMID might contain np.nan\n",
    "        # get pmid, doi\n",
    "        if df[\"Pubmed Id\"][ind] != df[\"Pubmed Id\"][ind]: # Pubmed Id is np.nan\n",
    "            if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # DOI is np.nan\n",
    "                doi = np.nan\n",
    "                pmid = np.nan\n",
    "            else: # DOI is not np.nan\n",
    "                doi = str(df[\"DOI\"][ind]).strip()\n",
    "                pmid = plib.doi2pmid(doi)\n",
    "        else: # Pubmed Id is not np.nan\n",
    "            pmid = str(int(df[\"Pubmed Id\"][ind])).strip()\n",
    "            if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # DOI is not np.nan\n",
    "                doi, a = plib.pmid2doi_pmcid(pmid)\n",
    "            else: # DOI is not np.nan\n",
    "                doi = str(df[\"DOI\"][ind]).strip()\n",
    "        \n",
    "        # get pmcid\n",
    "        if pmid != pmid: # pmid is np.nan\n",
    "            pmcid = np.nan\n",
    "            # if doi != doi: # doi is np.nan\n",
    "            #     full_text_url = np.nan\n",
    "            #     full_text_source = np.nan\n",
    "            # else:\n",
    "            #     full_text_url = \"https://doi.org/\" + str(doi).strip()\n",
    "            #     full_text_source = \"DOI\"\n",
    "        else: # pmid is not np.nan\n",
    "            # request the webpage\n",
    "            url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "            soup = plib.request_webpage(url)\n",
    "            # print(soup)\n",
    "\n",
    "            # get pmcid\n",
    "            try:\n",
    "                pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                pmcid = np.nan\n",
    "            # print(pmcid)\n",
    "            \n",
    "            # get full_text_url\n",
    "            # if pmcid == pmcid:\n",
    "            #     full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "            #     full_text_source = \"PMC\"\n",
    "            # else:\n",
    "            #     try:\n",
    "            #         full_text_url = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"href\"].strip()\n",
    "            #         full_text_source = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"data-ga-action\"].strip()\n",
    "            #     except:\n",
    "            #         full_text_url = np.nan\n",
    "            #         full_text_source = np.nan\n",
    "        \n",
    "        full_text_url = np.nan\n",
    "        pdf_url = np.nan\n",
    "        title = str(df[\"Article Title\"][ind]).strip()\n",
    "        abstract = str(df[\"Abstract\"][ind]).strip()\n",
    "        keywords = str(df[\"Author Keywords\"][ind]).strip() + \"; \" + str(df[\"Keywords Plus\"][ind]).strip()\n",
    "\n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"Title\": [title],\n",
    "            \"Abstract\": [abstract],\n",
    "            \"Keywords\": [keywords]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# # source_path = fpath.poten_litera_wos\n",
    "# # output_path = fpath.poten_litera_wos_processed\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, sep=';')\n",
    "# df = df[[\"DOI\", \"Pubmed Id\", \"Article Title\", \"Abstract\", \"Author Keywords\", \"Keywords Plus\"]]\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "\n",
    "# print(df[\"DOI\"].isnull().values.any())\n",
    "# print(df[\"Pubmed Id\"].isnull().values.any())\n",
    "# print(df[\"Article Title\"].isnull().values.any())\n",
    "# print(df[\"Abstract\"].isnull().values.any())\n",
    "# print(df[\"Author Keywords\"].isnull().values.any())\n",
    "# print(df[\"Keywords Plus\"].isnull().values.any())\n",
    "# # True, True, False\n",
    "# # Article Title don't contain np.nan\n",
    "# # DOI, Pubmed Id contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code--------------------- \n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_webofscience(source_path, output_path, 0, 10)\n",
    "# ---------------------end of test code--------------------- \n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=';')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_eupmc(source_path, output_path, start, end):\n",
    "    print(\"Starting preprocessing search results from Europe PMC...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, sep=\",\")\n",
    "    df = df[[\"SOURCE\", \"DOI\", \"EXTERNAL_ID\", \"PMCID\", \"TITLE\"]]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # sleep to avoid to be blocked\n",
    "        time.sleep(random.randint(1, 3))\n",
    "\n",
    "        # get pmid, doi\n",
    "        # SOURCE = {'PMC', 'MED', 'ETH', 'PPR'}\n",
    "        if df[\"SOURCE\"][ind] != \"MED\": # SOURCE is not \"MED\" \n",
    "            if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # doi is np.nan\n",
    "                doi = np.nan\n",
    "                pmid = np.nan\n",
    "            else:\n",
    "                doi = str(df[\"DOI\"][ind]).strip()\n",
    "                pmid = plib.doi2pmid(doi)\n",
    "        else: # SOURCE is \"MED\"\n",
    "            # get doi, pmid\n",
    "            if df[\"EXTERNAL_ID\"][ind] != df[\"EXTERNAL_ID\"][ind]: # EXTERNAL_ID is np.nan\n",
    "                if df[\"DOI\"][ind] != df[\"DOI\"][ind](): # DOI is np.nan\n",
    "                    doi = np.nan\n",
    "                    pmid = np.nan\n",
    "                else: # DOI is not np.nan\n",
    "                    doi = str(df[\"DOI\"][ind]).strip()\n",
    "                    pmid = plib.doi2pmid(doi)\n",
    "            else: # EXTERNAL_ID is not np.nan\n",
    "                pmid = str(df[\"EXTERNAL_ID\"][ind]).strip()\n",
    "                if df[\"DOI\"][ind] != df[\"DOI\"][ind]: # DOI is np.nan\n",
    "                    doi, a = plib.pmid2doi_pmcid(pmid)\n",
    "                else: # DOI is not np.nan\n",
    "                    doi = str(df[\"DOI\"][ind]).strip()\n",
    "                \n",
    "        # get pmcid, full_text_url, full_text_source\n",
    "        if pmid != pmid: # pmid is np.nan\n",
    "            pmcid = df[\"PMCID\"][ind]\n",
    "            # if pmcid == pmcid: # pmcid is np.nan\n",
    "            #     full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "            #     full_text_source = \"PMC\"\n",
    "            # elif doi == doi: # doi is not np.nan\n",
    "            #     full_text_url = \"https://doi.org/\" + str(doi).strip()\n",
    "            #     full_text_source = \"DOI\"\n",
    "            # else:\n",
    "            #     full_text_url = np.nan\n",
    "            #     full_text_source = np.nan\n",
    "        else: # pmid is not np.nan\n",
    "            # request the webpage\n",
    "            url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "            # proxies = plib.get_proxies()\n",
    "            soup = plib.request_webpage(url)\n",
    "            # print(soup)\n",
    "\n",
    "            # get pmcid\n",
    "            try:\n",
    "                pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                pmcid = np.nan\n",
    "            # print(pmcid)\n",
    "            \n",
    "            # get full_text_url\n",
    "            # if pmcid == pmcid: # pmcid is not np.nan\n",
    "            #     full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "            #     full_text_source = \"PMC\"\n",
    "            # else: # pmcid is not np.nan\n",
    "            #     try:\n",
    "            #         full_text_url = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"href\"].strip()\n",
    "            #         full_text_source = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"data-ga-action\"].strip()\n",
    "            #     except:\n",
    "            #         full_text_url = np.nan\n",
    "            #         full_text_source = np.nan\n",
    "        \n",
    "        full_text_url = np.nan\n",
    "        pdf_url = np.nan\n",
    "        title = (df.at[ind, \"TITLE\"]).strip()\n",
    "        abstract = np.nan\n",
    "        keywords = np.nan\n",
    "        \n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"Title\": [title],\n",
    "            \"Abstract\": [abstract],\n",
    "            \"Keywords\": [keywords]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_eupmc\n",
    "# output_path = fpath.poten_litera_eupmc_processed\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, sep=',')\n",
    "# df = df[[\"SOURCE\", \"DOI\", \"EXTERNAL_ID\", \"PMCID\", \"TITLE\"]]\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "\n",
    "# col_one_list = set(df['SOURCE'].tolist())\n",
    "# print(col_one_list)\n",
    "# # ['PMC', 'MED', 'ETH', 'PPR']\n",
    "\n",
    "# print(df[\"SOURCE\"].isnull().values.any())\n",
    "# print(df[\"DOI\"].isnull().values.any())\n",
    "# print(df[\"EXTERNAL_ID\"].isnull().values.any())\n",
    "# print(df[\"PMCID\"].isnull().values.any())\n",
    "# print(df[\"TITLE\"].isnull().values.any())\n",
    "# # False, True, False, True, False\n",
    "# # SOURCE, EXTERNAL_ID, Title don't contain np.nan\n",
    "# # DOI, PMCID contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_eupmc(source_path, output_path, 0, 10)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_google_shcolar_step1(source_path, output_path, start, end):\n",
    "    print(\"Starting merging search results from Google Scholar...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, header=None, sep=',')\n",
    "    df.columns = [\"title\", \"url\", \"url_tag\", \"full_text_url\", \"full_text_tag\"]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # df[\"url_tag\"][ind]: {'[CITATION][C]', '[HTML][HTML]', '[PDF][PDF]', '[BOOK][B]', nan}\n",
    "        # we don't need books, as they are not likely to include connecivity information\n",
    "        if df.at[ind, \"url_tag\"] == \"[BOOK][B]\":\n",
    "            continue\n",
    "        \n",
    "        if (df.at[ind, \"url\"] != df.at[ind, \"url\"] or df.at[ind, \"title\"] != df.at[ind, \"title\"]) and (df.at[ind, \"full_text_tag\"] == \"[PDF]\" or df.at[ind, \"full_text_tag\"] == \"[HTML]\"):\n",
    "            raise Exception(ind, \": url or title are both nan, but full_text_tag is [PDF] or [HTML]!\")\n",
    "\n",
    "        # if url or title doesn't exsit AND full_text_url doesn't exist\n",
    "        if (df.at[ind, \"url\"] != df.at[ind, \"url\"] or df.at[ind, \"title\"] != df.at[ind, \"title\"]):\n",
    "            continue \n",
    "        \n",
    "        title = str(df[\"title\"][ind]).strip()\n",
    "\n",
    "        # now every row has at least title and url\n",
    "        if df[\"url_tag\"][ind] == \"[PDF][PDF]\": # {'[CITATION][C]', '[HTML][HTML]', '[PDF][PDF]', nan}\n",
    "            if df[\"full_text_tag\"][ind] == \"[HTML]\": # {'[PDF]', '[HTML]', nan}\n",
    "                link = str(df[\"full_text_url\"][ind]).strip()\n",
    "                full_text_url, status_code  = plib.get_final_redirected_url(link)\n",
    "                if full_text_url == full_text_url:\n",
    "                    full_text_source = full_text_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                else:\n",
    "                    full_text_source = np.nan\n",
    "            else:\n",
    "                full_text_url = np.nan\n",
    "                full_text_source = np.nan\n",
    "            # get pdf_url, pdf_source\n",
    "            link = str(df[\"url\"][ind]).strip()\n",
    "            pdf_url, status_code = plib.get_final_redirected_url(link)\n",
    "            # if pdf_url == pdf_url:\n",
    "            #     pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "            # else:\n",
    "            #     pdf_source = np.nan\n",
    "        else: # {'[CITATION][C]', '[HTML][HTML]', '[PDF][PDF]', nan}\n",
    "            link = str(df[\"url\"][ind]).strip()\n",
    "            full_text_url, status_code = plib.get_final_redirected_url(link)\n",
    "            if full_text_url == full_text_url:\n",
    "                full_text_source = full_text_url.split(\"://\")[1].split(\"/\")[0]\n",
    "            else:\n",
    "                full_text_source = np.nan\n",
    "            # get pdf_url, pdf_source\n",
    "            if df[\"full_text_tag\"][ind] == \"[PDF]\": # full_text_type = {'[HTML]', nan, '[PDF]'}\n",
    "                link = str(df[\"full_text_url\"][ind]).strip()\n",
    "                pdf_url, status_code  = plib.get_final_redirected_url(link)\n",
    "                # if pdf_url == pdf_url:\n",
    "                #     pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                # else:\n",
    "                #     pdf_source = np.nan\n",
    "            else:\n",
    "                pdf_url = np.nan\n",
    "        \n",
    "        columns = [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\"]\n",
    "        row = {\n",
    "            \"Title\": [title],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"full_text_source\": [full_text_source],\n",
    "            \"pdf_url\": [pdf_url]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_gs\n",
    "# output_path = fpath.poten_litera_gs_processed_step1\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"title\", \"url\", \"url_tag\", \"full_text_url\", \"full_text_tag\"]\n",
    "# # print(df.head(3))\n",
    "# print(df.shape)\n",
    "# # (980, 5)\n",
    "\n",
    "# url_type = set(df['url_tag'].tolist())\n",
    "# print(url_type)\n",
    "# # {'[CITATION][C]', '[HTML][HTML]', '[PDF][PDF]', '[BOOK][B]', nan}\n",
    "# full_text_tag = set(df['full_text_tag'].tolist())\n",
    "# print(full_text_tag)\n",
    "# # {'[PDF]', '[HTML]', nan}\n",
    "# # ---------------------end of test code---------------------\n",
    "\n",
    "# # --------------------start of test code--------------------\n",
    "# # [\"title\", \"url\", \"url_tag\", \"full_text_url\", \"full_text_tag\"]\n",
    "# print(df[\"title\"].isnull().any().any())\n",
    "# print(df[\"url\"].isnull().any().any())\n",
    "# print(df[\"url_tag\"].isnull().any().any())\n",
    "# print(df[\"full_text_url\"].isnull().any().any())\n",
    "# print(df[\"full_text_tag\"].isnull().any().any())\n",
    "# # True, True, True, True, True\n",
    "# # title, url, url_tag, full_text_url, full_text_tag, all contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_google_shcolar_step1(source_path, output_path, 0, 1000)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_google_shcolar_step2(source_path, output_path, start, end):\n",
    "    print(\"Starting merging search results from Google Scholar...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, header=None, sep=',')\n",
    "    df.columns = [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\"]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # get doi from url\n",
    "        if df[\"full_text_url\"][ind] == df[\"full_text_url\"][ind]: # there's a full_text_url\n",
    "            url = str(df[\"full_text_url\"][ind]).strip()\n",
    "            source = url.split(\"://\")[1].split(\"/\")[0]\n",
    "            # check if the full_text_url is one of our websites\n",
    "            flag = False\n",
    "            for website in params.websites_gs:\n",
    "                if website in source:\n",
    "                    flag = True\n",
    "                    break\n",
    "            if not flag:\n",
    "                continue\n",
    "            info = extra_info.extract_info_from_webpage(url, params.websites_gs)\n",
    "            doi = info[\"doi\"]\n",
    "            pmid = info[\"pmid\"]\n",
    "            pmcid = info[\"pmcid\"]\n",
    "        else:\n",
    "            url = np.nan\n",
    "            doi = np.nan\n",
    "            pmid = np.nan\n",
    "            pmcid = np.nan\n",
    "        \n",
    "        # # get pmid from DOI\n",
    "        # if doi == doi: # there's doi\n",
    "        #     pmid = plib.doi2pmid(doi)\n",
    "        # else: # doi not found\n",
    "        #     pmid = np.nan\n",
    "        # # get pmcid, full_text_url, full_text_source\n",
    "        # if pmid != pmid: # pmid is np.nan\n",
    "        #     pmcid = np.nan\n",
    "        #     if doi == doi: # doi is not np.nan\n",
    "        #         full_text_url = \"https://doi.org/\" + str(doi).strip()\n",
    "        #         full_text_source = \"DOI\"\n",
    "        #     else:\n",
    "        #         full_text_url = np.nan\n",
    "        #         full_text_source = np.nan\n",
    "        # else: # pmid is not np.nan\n",
    "        #     # request the webpage\n",
    "        #     url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "        #     # proxies = plib.get_proxies()\n",
    "        #     soup = plib.request_webpage(url)\n",
    "        #     # print(soup)\n",
    "\n",
    "        #     # get pmcid\n",
    "        #     try:\n",
    "        #         pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "        #     except:\n",
    "        #         pmcid = np.nan\n",
    "        #     # print(pmcid)\n",
    "            \n",
    "        #     # get full_text_url, full_text_source\n",
    "        #     if pmcid == pmcid: # pmcid is not np.nan\n",
    "        #         full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "        #         full_text_source = \"PMC\"\n",
    "        #     else: # pmcid is not np.nan\n",
    "        #         try:\n",
    "        #             full_text_url = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"href\"].strip()\n",
    "        #             full_text_source = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"data-ga-action\"].strip()\n",
    "        #         except:\n",
    "        #             full_text_url = np.nan\n",
    "        #             full_text_source = np.nan\n",
    "        \n",
    "        full_text_url = url\n",
    "        pdf_url = df.at[ind, \"pdf_url\"]\n",
    "        title = df.at[ind, \"Title\"]\n",
    "        abstract = np.nan\n",
    "        keywords = np.nan\n",
    "\n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"Title\": [title],\n",
    "            \"Abstract\": [abstract],\n",
    "            \"Keywords\": [keywords]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(doi)\n",
    "        if doi != doi:\n",
    "            print([df[\"full_text_url\"][ind]])\n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_gs_processed_step1\n",
    "# output_path = fpath.poten_litera_gs_processed_step2\n",
    "# plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\"]\n",
    "# # print(df.head(3))\n",
    "# print(df.shape)\n",
    "# # (926, 4)\n",
    "# full_text_source = set(df['full_text_source'].tolist())\n",
    "# print(full_text_source)\n",
    "# # {'www.elibrary.ru', 'n.neurology.org', 'jnnp.bmj.com', 'anatomypubs.onlinelibrary.wiley.com', 'academic.oup.com', \n",
    "# #  'nyaspubs.onlinelibrary.wiley.com', 'cir.nii.ac.jp', 'link.springer.com', 'www.mdpi.com', 'pure.mpg.de', \n",
    "# #  'bmcneurosci.biomedcentral.com', 'elibrary.ru', 'journals.sagepub.com', 'tbiomed.biomedcentral.com', \n",
    "# #  'onlinelibrary.wiley.com', 'www.cambridge.org', 'wakespace.lib.wfu.edu', nan, 'www.cell.com', 'europepmc.org', \n",
    "# #  'var.scholarpedia.org', 'jpet.aspetjournals.org', 'journal.psych.ac.cn', 'www.biorxiv.org', 'ieeexplore.ieee.org', \n",
    "# #  'www.jstor.org', 'www.cabdirect.org', 'royalsocietypublishing.org', 'analyticalsciencejournals.onlinelibrary.wiley.com', \n",
    "# #  'open.bu.edu', 'journals.lww.com', 'www.eneuro.org', 'www.jstage.jst.go.jp', 'journals.plos.org', 'www.ncbi.nlm.nih.gov', \n",
    "# #  'www.liebertpub.com', 'neuro.psychiatryonline.org', 'www.sciencedirect.com', 'psycnet.apa.org', 'www.taylorfrancis.com', \n",
    "# #  'www.degruyter.com', 'www.nature.com', 'jamanetwork.com', 'karger.com', 'www.tandfonline.com', 'journals.physiology.org', \n",
    "# #  'movementdisorders.onlinelibrary.wiley.com', 'www.pnas.org', 'www.jneurosci.org', 'thejns.org', 'pascal-francis.inist.fr', \n",
    "# #  'physoc.onlinelibrary.wiley.com', 'agro.icm.edu.pl', 'elifesciences.org', 'www.frontiersin.org', 'escholarship.mcgill.ca', \n",
    "# #  'ajp.psychiatryonline.org', 'www.science.org', 'books.google.de'}\n",
    "\n",
    "# # {'elibrary.ru', 'neurology.org', 'bmj.com', 'wiley.com', 'oup.com', 'cir.nii.ac.jp', 'springer.com', 'mdpi.com', 'mpg.de', \n",
    "# #  'biomedcentral.com', 'sagepub.com', 'cambridge.org', 'wfu.edu', nan, 'cell.com', 'europepmc.org', 'scholarpedia.org', \n",
    "# #  'aspetjournals.org', 'psych.ac.cn', 'biorxiv.org', 'ieee.org', 'jstor.org', 'cabdirect.org', 'royalsocietypublishing.org', \n",
    "# #  'bu.edu', 'lww.com', 'eneuro.org', 'jst.go.jp', 'plos.org', 'ncbi.nlm.nih.gov', 'liebertpub.com', 'psychiatryonline.org', \n",
    "# #  'sciencedirect.com', 'psycnet.apa.org', 'taylorfrancis.com', 'degruyter.com', 'nature.com', 'jamanetwork.com', \n",
    "# #  'karger.com', 'www.tandfonline.com', 'physiology.org', 'www.pnas.org', 'jneurosci.org', 'thejns.org', \n",
    "# #  'pascal-francis.inist.fr', 'agro.icm.edu.pl', 'elifesciences.org', 'frontiersin.org', 'mcgill.ca', \n",
    "# #  'science.org', 'books.google.de'}\n",
    "\n",
    "# # websites_gs = {\n",
    "# #     'neurology.org', 'bmj.com', 'wiley.com', 'oup.com', 'springer.com', 'mdpi.com', \n",
    "# #     'biomedcentral.com', 'sagepub.com', 'cambridge.org', 'wfu.edu', 'cell.com', 'europepmc.org', \n",
    "# #     'aspetjournals.org', 'psych.ac.cn', 'biorxiv.org', 'ieee.org', 'jstor.org', 'royalsocietypublishing.org', \n",
    "# #     'bu.edu', 'lww.com', 'eneuro.org', 'jst.go.jp', 'plos.org', 'ncbi.nlm.nih.gov', 'liebertpub.com', \n",
    "# #     'psychiatryonline.org', 'sciencedirect.com', 'psycnet.apa.org', 'degruyter.com', 'nature.com', 'jamanetwork.com', \n",
    "# #     'karger.com', 'tandfonline.com', 'physiology.org', 'pnas.org', 'jneurosci.org', 'thejns.org', \n",
    "# #     'agro.icm.edu.pl', 'elifesciences.org', 'frontiersin.org', 'science.org'}\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# # [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\"]\n",
    "# print(df[\"Title\"].isnull().any().any())\n",
    "# print(df[\"full_text_url\"].isnull().any().any())\n",
    "# print(df[\"full_text_source\"].isnull().any().any())\n",
    "# print(df[\"pdf_url\"].isnull().any().any())\n",
    "# # False, True, True, True\n",
    "# # full_text_url, full_text_source, pdf_url contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_google_shcolar_step2(source_path, output_path, 0, 905)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_seed_paper_spanning(source_path, output_path):\n",
    "    print(\"Starting preprocessing search results from spanning citations of seed paper...\")\n",
    "    return True\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cocomac_paper(source_path, output_path):\n",
    "    print(\"Starting preprocessing search results from CoCoMac papers...\")\n",
    "    return True\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(input, output_path):\n",
    "    # combine all results\n",
    "    df = pd.DataFrame()\n",
    "    for search_result in input:\n",
    "        df_single = pd.read_csv(search_result, header=None, sep = \",\")\n",
    "        # df = df.append(df_single, ignore_index=True, sort=False)\n",
    "        df = pd.concat([df, df_single], ignore_index=True, sort=False)\n",
    "    df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.to_csv(output_path, header=False, index=False)\n",
    "# --------------------start of test code--------------------\n",
    "# gos = fpath.poten_litera_gs_processed_step2\n",
    "# wos = fpath.poten_litera_wos_processed\n",
    "# pubmed = fpath.poten_litera_pubmed_processed\n",
    "# eupmc = fpath.poten_litera_eupmc_processed\n",
    "# input = [gos, wos, pubmed, eupmc]\n",
    "# output_path = fpath.poten_litera_combined\n",
    "# # plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# combine(input, output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "# # (14627, 8)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_in_identifiers(input_path, output_path, start, end):\n",
    "    df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "    df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "    \n",
    "    # fill in elements that are missing\n",
    "    for ind in range(start, end):\n",
    "        # if all 3 identifiers are missing, and full_text_url and pdf_url are missing, skip\n",
    "        if df.at[ind, \"DOI\"] != df.at[ind, \"DOI\"] and df.at[ind, \"PMID\"] != df.at[ind, \"PMID\"] and df.at[ind, \"PMCID\"] != df.at[ind, \"PMCID\"] and df.at[ind, \"full_text_url\"] != df.at[ind, \"full_text_url\"] and df.at[ind, \"pdf_url\"] != df.at[ind, \"pdf_url\"]:\n",
    "            continue\n",
    "        \n",
    "        # initialzie\n",
    "        doi = np.nan\n",
    "        pmid = np.nan\n",
    "        pmcid = np.nan\n",
    "        full_text_url = df.at[ind, \"full_text_url\"]\n",
    "        pdf_url = df.at[ind, \"pdf_url\"]\n",
    "        title = df.at[ind, \"Title\"]\n",
    "        abstract = df.at[ind, \"Abstract\"]\n",
    "        keywords = df.at[ind, \"Keywords\"]\n",
    "\n",
    "        # if all 3 identifiers are missing, and full_text_url is missing\n",
    "        if df.at[ind, \"DOI\"] != df.at[ind, \"DOI\"] and df.at[ind, \"PMID\"] != df.at[ind, \"PMID\"] and df.at[ind, \"PMCID\"] != df.at[ind, \"PMCID\"]:\n",
    "            columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "            row = {\n",
    "                \"DOI\": [doi],\n",
    "                \"PMID\": [pmid],\n",
    "                \"PMCID\": [pmcid],\n",
    "                \"full_text_url\": [full_text_url],\n",
    "                \"pdf_url\": [pdf_url],\n",
    "                \"Title\": [title],\n",
    "                \"Abstract\": [abstract],\n",
    "                \"Keywords\": [keywords]\n",
    "            }\n",
    "\n",
    "            if not plib.add_row_to_csv(output_path, row, columns):\n",
    "                print(\"Error detected when adding a row to csv!\")\n",
    "\n",
    "            print(ind)\n",
    "            continue\n",
    "        \n",
    "        # we have at least one of the 3 identifiers\n",
    "        # doi, pmid\n",
    "        if df[\"DOI\"][ind] == df[\"DOI\"][ind]: # DOI -> PMID\n",
    "            doi = str(df[\"DOI\"][ind]).strip().lower()\n",
    "            # print(doi)\n",
    "            if df[\"PMID\"][ind] == df[\"PMID\"][ind]:\n",
    "                pmid = str(df[\"PMID\"][ind]).strip()\n",
    "                # print(pmid)\n",
    "            else:\n",
    "                pmid = plib.doi2pmid(doi)\n",
    "                # print(pmid)\n",
    "                if pmid != pmid:\n",
    "                    pmid_cadidate = plib.title2pmid(title)\n",
    "                    # print(pmid_cadidate)\n",
    "                    if pmid_cadidate == pmid_cadidate:   \n",
    "                        doi_validate, a = plib.pmid2doi_pmcid(pmid_cadidate)\n",
    "                        if doi_validate == doi_validate:\n",
    "                            doi_validate = doi_validate.lower()\n",
    "                            if doi_validate == doi:\n",
    "                                pmid = pmid_cadidate\n",
    "                                # print(pmid)\n",
    "        elif df[\"PMID\"][ind] == df[\"PMID\"][ind]: # PMID -> DOI\n",
    "            pmid = str(int(df[\"PMID\"][ind])).strip()\n",
    "            # print(pmid)\n",
    "            doi, pmcid = plib.pmid2doi_pmcid(pmid)\n",
    "            # print(doi)\n",
    "        elif df[\"PMCID\"][ind] == df[\"PMCID\"][ind]: # PMCID -> DOI, PMID\n",
    "            pmcid = str(df[\"PMCID\"][ind]).strip()\n",
    "            try:\n",
    "                doi, pmid = plib.pmcid2doi_pmid(pmcid)\n",
    "            except:\n",
    "                doi = np.nan\n",
    "                pmid = np.nan\n",
    "            # print(doi)\n",
    "            # print(pmid)\n",
    "        else:\n",
    "            doi = np.nan\n",
    "            pmid = np.nan\n",
    "        # print(doi)\n",
    "        # print(pmid)\n",
    "        \n",
    "        # pmcid\n",
    "        if df[\"PMCID\"][ind] == df[\"PMCID\"][ind]:\n",
    "            pmcid = str(df[\"PMCID\"][ind]).strip()\n",
    "        elif pmid == pmid:\n",
    "            a, pmcid = plib.pmid2doi_pmcid(pmid)\n",
    "        else:\n",
    "            pmcid = np.nan\n",
    "        # print(pmcid)\n",
    "\n",
    "        # full_text_url\n",
    "        # if pmcid == pmcid:\n",
    "        #     full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "        # elif doi == doi:\n",
    "        #     full_text_url = plib.get_final_redirected_url(str(\"https://doi.org/\" + doi).strip())\n",
    "        # elif df[\"full_text_url\"][ind] == df[\"full_text_url\"][ind]:\n",
    "        #     full_text_url = plib.get_final_redirected_url(df[\"full_text_url\"][ind])\n",
    "        # else:\n",
    "        #     full_text_url = np.nan\n",
    "        # print(full_text_url)\n",
    "\n",
    "        # pdf_url\n",
    "        # if df[\"pdf_url\"][ind] == df[\"pdf_url\"][ind]:\n",
    "        #     pdf_url = plib.get_final_redirected_url(str(df[\"pdf_url\"][ind]).strip())\n",
    "        # else:\n",
    "        #     pdf_url = np.nan\n",
    "        # print(pdf_url)\n",
    "    \n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"Title\": [title],\n",
    "            \"Abstract\": [abstract],\n",
    "            \"Keywords\": [keywords]\n",
    "        }\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "\n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# input_path = fpath.poten_litera_combined\n",
    "# # output_path = fpath.poten_litera_filled\n",
    "# # plib.clear_file(output_path)\n",
    "# df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "# print(df.shape)\n",
    "# df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "# print(df[\"DOI\"].isnull().any().any())\n",
    "# print(df[\"PMID\"].isnull().any().any())\n",
    "# print(df[\"PMCID\"].isnull().any().any())\n",
    "# print(df[\"full_text_url\"].isnull().any().any())\n",
    "# print(df[\"pdf_url\"].isnull().any().any())\n",
    "# print(df[\"Title\"].isnull().any().any())\n",
    "# print(df[\"Abstract\"].isnull().any().any())\n",
    "# print(df[\"Keywords\"].isnull().any().any())\n",
    "# True, True, True, True, True, False, True, True\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# fill_in_identifiers(input_path, output_path, 0, 14690)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_remove_dupli(input_path, output_path, columns, identifiers): \n",
    "    df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "    df.columns = columns\n",
    "\n",
    "    # remove all duplicates\n",
    "    for identifier in identifiers:\n",
    "        remove_dup_by = identifier\n",
    "        df = df[df[remove_dup_by].isnull() | ~df[df[remove_dup_by].notnull()].duplicated(subset=remove_dup_by, keep='first')]\n",
    "        # df = df.drop_duplicates(subset=['DOI'])\n",
    "        # df = df.drop_duplicates(subset=['PMID'])\n",
    "        # df = df.drop_duplicates(subset=['PMCID'])\n",
    "\n",
    "    # reset index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df.to_csv(output_path, header=False, index=False)\n",
    "    print(\"Duplication in the potential related literature removed.\")\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_ids_filled\n",
    "# output_path = fpath.poten_litra_filtered\n",
    "# plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# merge all search results\n",
    "# identifiers = [\"DOI\", \"PMID\", \"PMCID\"]\n",
    "# columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "# merge_remove_dupli(source_path, output_path, identifiers)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_text_url_filling(input_path, output_path, start, end):\n",
    "    df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "    df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "    \n",
    "    for ind in range(start, end):\n",
    "        time.sleep(3)\n",
    "\n",
    "        doi = df.at[ind, \"DOI\"]\n",
    "        pmid = df.at[ind, \"PMID\"]\n",
    "        pmcid = df.at[ind, \"PMCID\"]\n",
    "        full_text_url = np.nan\n",
    "        pdf_url = df.at[ind, \"pdf_url\"]\n",
    "\n",
    "        # get full text link from pmcid\n",
    "        if full_text_url!= full_text_url and pmcid == pmcid:\n",
    "            url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + str(pmcid).strip() + \"/\"\n",
    "            full_text_url, status_code = plib.get_final_redirected_url(url)\n",
    "            if status_code == 200: # some papers are assined pmcid, but the pmc wepage isn't avaiable until 2024\n",
    "                full_text_url = full_text_url\n",
    "            # elif status_code == 403:\n",
    "            #     print(status_code, \"Error when trying to get final redirected url from\", url)\n",
    "            #     full_text_url = np.nan\n",
    "            else:\n",
    "                print(status_code, \"Error when trying to get final redirected url from\", url)\n",
    "                full_text_url = np.nan\n",
    "        \n",
    "        # get full text url from doi\n",
    "        if full_text_url != full_text_url and doi == doi:\n",
    "            url = \"https://doi.org/\" + str(doi).strip().lower()\n",
    "            full_text_url, status_code = plib.get_final_redirected_url(url)\n",
    "            if status_code == 200 or status_code == 403:\n",
    "                full_text_url = full_text_url\n",
    "            else:\n",
    "                print(status_code, \"Error when trying to get final redirected url from\", url)\n",
    "                full_text_url = np.nan\n",
    "                \n",
    "        # get full text url from pmid\n",
    "        if full_text_url != full_text_url and pmid == pmid:\n",
    "            url = \"https://pubmed.ncbi.nlm.nih.gov/\" + str(int(df.at[ind, \"PMID\"])).strip() + \"/\"\n",
    "            try:\n",
    "                soup = plib.request_webpage(url)\n",
    "                link = soup.find(\"div\", {\"class\": \"full-text-links-list\"}).find(\"a\", {\"class\": \"link-item pmc\"})[\"href\"]\n",
    "                full_text_url, status_code = plib.get_final_redirected_url(link)\n",
    "                if status_code == 200:\n",
    "                    full_text_url = full_text_url\n",
    "                # elif status_code == 403:\n",
    "                #     print(status_code, \"Error when trying to get final redirected url from\", url)\n",
    "                #     full_text_url = np.nan\n",
    "                else:\n",
    "                    print(status_code, \"Error when trying to get final redirected url from\", url)\n",
    "                    full_text_url = np.nan\n",
    "                    raise Exception()\n",
    "            except:\n",
    "                try:\n",
    "                    soup = plib.request_webpage(url)\n",
    "                    link = soup.find(\"div\", {\"class\": \"full-text-links-list\"}).find(\"a\", {\"class\": \"link-item dialog-focus\"})[\"href\"]\n",
    "                    full_text_url, status_code = plib.get_final_redirected_url(link)\n",
    "                    if status_code == 200 or status_code == 403:\n",
    "                        full_text_url = full_text_url\n",
    "                    else:\n",
    "                        print(status_code, \"Error when trying to get final redirected url from\", url)\n",
    "                        full_text_url = np.nan\n",
    "                        raise Exception()\n",
    "                except:\n",
    "                    full_text_url = np.nan\n",
    "               \n",
    "        # get full text url from df.at[ind, \"full_text_url\"] \n",
    "        if full_text_url != full_text_url and df.at[ind, \"full_text_url\"] == df.at[ind, \"full_text_url\"]:\n",
    "            full_text_url, status_code = plib.get_final_redirected_url(df.at[ind, \"full_text_url\"])\n",
    "            if status_code == 200 or status_code == 403:\n",
    "                full_text_url = full_text_url\n",
    "            else:\n",
    "                print(status_code, \"Error when trying to get final redirected url from\", df.at[ind, \"full_text_url\"])\n",
    "                full_text_url = np.nan                  \n",
    "        \n",
    "        # get full text url from pmid\n",
    "        if full_text_url != full_text_url and pmid == pmid:\n",
    "            url = \"https://pubmed.ncbi.nlm.nih.gov/\" + str(int(df.at[ind, \"PMID\"])).strip() + \"/\"\n",
    "            # time.sleep(3)\n",
    "            full_text_url, status_code = plib.get_final_redirected_url(url)\n",
    "            if status_code == 200:\n",
    "                full_text_url = full_text_url\n",
    "            else:\n",
    "                print(status_code, \"Error when trying to get final redirected url from\", url)\n",
    "                full_text_url = np.nan  \n",
    "        \n",
    "        # get pdf url, pdf source\n",
    "        if pdf_url == pdf_url:\n",
    "            try:\n",
    "                pdf_url, status_code = plib.get_final_redirected_url(pdf_url)\n",
    "                pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "            except:\n",
    "                print(\"error when getting final redirected url from: \", pdf_url)\n",
    "                pdf_url = np.nan\n",
    "                pdf_source = np.nan\n",
    "        else:\n",
    "            pdf_source = np.nan\n",
    "\n",
    "        # get full text source\n",
    "        if full_text_url == full_text_url:\n",
    "            full_text_source = full_text_url.split(\"://\")[1].split(\"/\")[0]\n",
    "        else:\n",
    "            full_text_source = np.nan\n",
    "        \n",
    "        # make sure doi is in lower case\n",
    "        if doi == doi:\n",
    "            doi = doi.lower()\n",
    "\n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"full_text_source\": [full_text_source],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"pdf_source\": [pdf_source],\n",
    "            \"Title\": [df.at[ind, \"Title\"]],\n",
    "            \"Abstract\": [df.at[ind, \"Abstract\"]],\n",
    "            \"Keywords\": [df.at[ind, \"Keywords\"]]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Main program: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from PubMed\n",
    "\n",
    "# source_path = fpath.poten_litera_pubmed\n",
    "# output_path = fpath.poten_litera_pubmed_processed\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from PubMed\n",
    "# # 2612 results\n",
    "# preprocess_pubmed(source_path, output_path, 0, 2612)\n",
    "# print(\"preprocessing results from PubMed succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from PubMed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine 2 files of search results from web of science\n",
    "# # clear the file\n",
    "# plib.clear_file(fpath.poten_litera_wos)\n",
    "\n",
    "# # combine the 2 files of search results from web of science\n",
    "# source_path_1 = fpath.poten_litera_wos_1\n",
    "# source_path_2 = fpath.poten_litera_wos_2\n",
    "# df_1 = pd.read_csv(source_path_1, sep=',')\n",
    "# df_2 = pd.read_csv(source_path_2, sep=',')\n",
    "# df_1.to_csv(fpath.poten_litera_wos, header=True, index=False, sep=\",\")\n",
    "# df_2.to_csv(fpath.poten_litera_wos, mode=\"a\", header=False, index=False, sep=\",\")\n",
    "# # --------------------start of test code--------------------\n",
    "# df = pd.read_csv(fpath.poten_litera_wos, sep=',')\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "# # (1993, 72)\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from Web of Science\n",
    "\n",
    "# source_path = fpath.poten_litera_wos\n",
    "# output_path = fpath.poten_litera_wos_processed\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from Web of Science\n",
    "# # 1993 results\n",
    "# preprocess_webofscience(source_path, output_path, 0, 1993)\n",
    "# print(\"preprocessing results from Web of Science succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from Web of Science!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from Europe PMC\n",
    "\n",
    "# source_path = fpath.poten_litera_eupmc\n",
    "# output_path = fpath.poten_litera_eupmc_processed\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from Europe PMC\n",
    "# preprocess_eupmc(source_path, output_path, 2980, 9178)\n",
    "# # 9178 results\n",
    "# print(\"preprocessing results from Europe PMC succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from Europe PMC!\")\n",
    "\n",
    "# # 2980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from Google Scholar step 1\n",
    "\n",
    "# source_path = fpath.poten_litera_gs\n",
    "# # 980 results\n",
    "# output_path = fpath.poten_litera_gs_processed_step1\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from Google Scholar\n",
    "# preprocess_google_shcolar_step1(source_path, output_path, 0, 980)\n",
    "# # 926 results\n",
    "# print(\"step 1 of preprocessing results from Google Scholar succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from Google Scholar step 1!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reset index for poten_litera_gs_processed_step1\n",
    "# input_path = fpath.poten_litera_gs_processed_step1\n",
    "# output_path = fpath.poten_litera_gs_processed_step1\n",
    "# df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df.to_csv(output_path, header=False, index=False)\n",
    "\n",
    "# input_path = fpath.poten_litera_gs_processed_step1\n",
    "# df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "# print(df.shape)\n",
    "# # (926, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from Google Scholar step 2\n",
    "\n",
    "# source_path = fpath.poten_litera_gs_processed_step1\n",
    "# # (926, 4)\n",
    "# output_path = fpath.poten_litera_gs_processed_step2\n",
    "\n",
    "\n",
    "# # clear the file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# # preprocess search results from Google Scholar\n",
    "# preprocess_google_shcolar_step2(source_path, output_path, 0, 926)\n",
    "# print(\"step 2 of preprocessing results from Google Scholar succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from Google Scholar step 2!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reset index for poten_litera_gs_processed_step2\n",
    "# input_path = fpath.poten_litera_gs_processed_step2\n",
    "# output_path = fpath.poten_litera_gs_processed_step2\n",
    "# df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df.to_csv(output_path, header=False, index=False)\n",
    "\n",
    "# input_path = fpath.poten_litera_gs_processed_step2\n",
    "# df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "# print(df.shape)\n",
    "# # (926, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from spanning citations of seed paper\n",
    "\n",
    "# preprocess_seed_paper_spanning(source_path, output_path, columns):\n",
    "# print(\"preprocessing results from spanning citations of seed papers succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from spanning citations of seed papers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess search results from CoCoMac papers\n",
    "\n",
    "# preprocess_cocomac_paper(source_path, output_path, columns)\n",
    "# print(\"preprocessing results from CoCoMac papers succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when preprocessing results from CoCoMac papers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # take a look at all the preprossed search results\n",
    "# gos = fpath.poten_litera_gs_processed_step2\n",
    "# wos = fpath.poten_litera_wos_processed\n",
    "# pubmed = fpath.poten_litera_pubmed_processed\n",
    "# eupmc = fpath.poten_litera_eupmc_processed\n",
    "\n",
    "# df_gs = pd.read_csv(gos, header=None, sep=',')\n",
    "# print(df_gs.shape)\n",
    "# # (907, 6)\n",
    "# df_wos = pd.read_csv(wos, header=None, sep=',')\n",
    "# print(df_wos.shape)\n",
    "# # (1993, 8)\n",
    "# df_pubmed = pd.read_csv(pubmed, header=None, sep=',')\n",
    "# print(df_pubmed.shape)\n",
    "# # (2612, 8)\n",
    "# df_eupmc = pd.read_csv(eupmc, header=None, sep=',')\n",
    "# print(df_eupmc.shape)\n",
    "# # (9178, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine all search results\n",
    "\n",
    "# gos = fpath.poten_litera_gs_processed_step2\n",
    "# wos = fpath.poten_litera_wos_processed\n",
    "# pubmed = fpath.poten_litera_pubmed_processed\n",
    "# eupmc = fpath.poten_litera_eupmc_processed\n",
    "# input = [gos, wos, pubmed, eupmc]\n",
    "# output_path = fpath.poten_litera_combined\n",
    "\n",
    "# # clear the file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# combine(input, output_path)\n",
    "# # (14627, 8)\n",
    "# print(\"Combining all search results succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when combining all search results!\")\n",
    "\n",
    "# df_combined = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df_combined.shape)\n",
    "# # (14690, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fill in missing identifiers\n",
    "# input_path = fpath.poten_litera_combined\n",
    "# output_path = fpath.poten_litera_ids_filled\n",
    "\n",
    "# # clear file\n",
    "# # plib.clear_file(output_path)\n",
    "\n",
    "# fill_in_identifiers(input_path, output_path, 0, 14690)\n",
    "# print(\"Filling in missing elements succeeded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mannualy check the missing elements\n",
    "# source_path = fpath.poten_litera_ids_filled\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "# # for ind in df.index:\n",
    "# #     if df.at[ind, \"DOI\"] != df.at[ind, \"DOI\"] and df.at[ind, \"PMID\"] != df.at[ind, \"PMID\"] and df.at[ind, \"PMCID\"] != df.at[ind, \"PMCID\"] and df.at[ind, \"full_text_url\"] != df.at[ind, \"full_text_url\"] and df.at[ind, \"pdf_url\"] != df.at[ind, \"pdf_url\"]:\n",
    "# #         print(ind)\n",
    "# #         print(df.at[ind, \"Title\"])\n",
    "# #         print(df.at[ind, \"full_text_url\"])\n",
    "# #         print(df.at[ind, \"pdf_url\"])\n",
    "\n",
    "# # for ind in df.index:\n",
    "# #     if df.at[ind, \"DOI\"] != df.at[ind, \"DOI\"] and df.at[ind, \"PMID\"] != df.at[ind, \"PMID\"] and df.at[ind, \"PMCID\"] != df.at[ind, \"PMCID\"] and df.at[ind, \"full_text_url\"] != df.at[ind, \"full_text_url\"]:\n",
    "# #         print(ind)\n",
    "# #         print(df.at[ind, \"Title\"])\n",
    "# #         print(df.at[ind, \"full_text_url\"])\n",
    "# #         print(df.at[ind, \"pdf_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge all search results and remove duplication by identifiers\n",
    "\n",
    "# source_path = fpath.poten_litera_ids_filled\n",
    "# output_path = fpath.poten_litra_filtered\n",
    "\n",
    "# # clear the file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# # merge all search results\n",
    "# identifiers = [\"DOI\", \"PMID\", \"PMCID\"]\n",
    "# columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "# merge_remove_dupli(source_path, output_path, columns, identifiers)\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litra_filtered\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# print(df.shape)\n",
    "# # (10982, 8)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mannualy check the missing elements\n",
    "# source_path = fpath.poten_litra_filtered\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"pdf_url\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "\n",
    "# # for ind in df.index:\n",
    "# #     if df.at[ind, \"DOI\"] != df.at[ind, \"DOI\"] and df.at[ind, \"PMID\"] != df.at[ind, \"PMID\"] and df.at[ind, \"PMCID\"] != df.at[ind, \"PMCID\"] and df.at[ind, \"full_text_url\"] != df.at[ind, \"full_text_url\"] and df.at[ind, \"pdf_url\"] != df.at[ind, \"pdf_url\"]:\n",
    "# #         print(ind)\n",
    "# #         print(df.at[ind, \"Title\"])\n",
    "# #         print(df.at[ind, \"full_text_url\"])\n",
    "# #         print(df.at[ind, \"pdf_url\"])\n",
    "\n",
    "# for ind in df.index:\n",
    "#     if df.at[ind, \"DOI\"] != df.at[ind, \"DOI\"] and df.at[ind, \"PMID\"] != df.at[ind, \"PMID\"] and df.at[ind, \"PMCID\"] != df.at[ind, \"PMCID\"] and df.at[ind, \"full_text_url\"] != df.at[ind, \"full_text_url\"]:\n",
    "#         print(ind)\n",
    "#         print(df.at[ind, \"Title\"])\n",
    "#         print(df.at[ind, \"full_text_url\"])\n",
    "#         print(df.at[ind, \"pdf_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in full_text_url\n",
    "\n",
    "source_path = fpath.poten_litra_filtered\n",
    "output_path = fpath.poten_litera_ids_ftl_filled\n",
    "\n",
    "# clear the file\n",
    "plib.clear_file(output_path)\n",
    "\n",
    "# merge all search results\n",
    "full_text_url_filling(source_path, output_path, 0, 10980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reset the index of poten_litera_ids_ftl_filled and add index\n",
    "# source_path = fpath.poten_litera_ids_ftl_filled\n",
    "# output_path = fpath.poten_litera_ids_ftl_filled\n",
    "\n",
    "# identifiers = [\"DOI\", \"PMID\", \"PMCID\"]\n",
    "# columns = [\"DOI\", \"PMID\", \"PMCID\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\", \"Title\", \"Abstract\", \"Keywords\"]\n",
    "# merge_remove_dupli(source_path, output_path, columns, identifiers)\n",
    "\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df.to_csv(output_path, header=False, index=True)\n",
    "# # --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_ids_ftl_filled\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# print(df.head(5))\n",
    "# print(df.shape)\n",
    "# (10980, 11)\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # select 300 random papers from poten_litera_ids_ftl_filled for testing\n",
    "# source_path = fpath.poten_litera_ids_ftl_filled\n",
    "# output_path = fpath.poten_litera_testing_set_300\n",
    "\n",
    "# # clear the file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "# df = df.sample(n=300, random_state=1, axis='index', ignore_index=False)\n",
    "# df.to_csv(output_path, header=True, index=False)\n",
    "# # --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_testing_set_300\n",
    "# df = pd.read_csv(source_path, header=0, sep=',')\n",
    "# print(df.shape)\n",
    "# # (300, 12)\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check all possible full_text_source\n",
    "# input_path = fpath.poten_litera_ids_ftl_filled\n",
    "# df = pd.read_csv(input_path, header=None, sep=\",\")\n",
    "# df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "# # (10980, 11)\n",
    "\n",
    "# # [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "# print(df[\"FULL_TEXT_URL\"].isnull().any().any()) # True\n",
    "# print(df[\"FULL_TEXT_SOURCE\"].isnull().any().any()) # True\n",
    "# print(df[\"TITLE\"].isnull().any().any()) # False\n",
    "\n",
    "# print(df[\"INDEX\"].dtypes) # int64\n",
    "# print(df[\"DOI\"].dtypes) # object\n",
    "# print(df[\"PMID\"].dtypes) # float64\n",
    "# print(df[\"PMCID\"].dtypes) # object\n",
    "# print(df[\"FULL_TEXT_URL\"].dtypes) # object\n",
    "# print(df[\"FULL_TEXT_SOURCE\"].dtypes) # object\n",
    "# print(df[\"PDF_URL\"].dtypes) # object\n",
    "# print(df[\"PDF_SOURCE\"].dtypes) # object\n",
    "# print(df[\"TITLE\"].dtypes) # object\n",
    "# print(df[\"ABSTRACT\"].dtypes) # object\n",
    "# print(df[\"KEYWORDS\"].dtypes) # object\n",
    "\n",
    "# full_text_source = set(df['FULL_TEXT_SOURCE'].tolist())\n",
    "# print(full_text_source)\n",
    "# # {'psycnet.apa.org:443', 'elifesciences.org', 'journals.physiology.org', 'thejns.org:443', 'www.jneurosci.org', \n",
    "# # 'opg.optica.org', 'europepmc.org', 'academic.oup.com', 'journals.lww.com', nan, 'pubmed.ncbi.nlm.nih.gov', \n",
    "# # 'ujms.net', 'thejns.org', 'www.mdpi.com', 'www.microbiologyresearch.org', 'ieeexplore.ieee.org', 'journals.plos.org', \n",
    "# # 'www.rbojournal.org', 'jnm.snmjournals.org', 'n.neurology.org', 'arjournals.annualreviews.org', 'www.thieme-connect.com', \n",
    "# # 'www.cambridge.org', 'pubs.acs.org', 'www.pnas.org', 'analyticalsciencejournals.onlinelibrary.wiley.com', \n",
    "# # 'journals.aps.org', 'papers.ssrn.com', 'www.ahajournals.org', 'www.ncbi.nlm.nih.gov', 'ajp.psychiatryonline.org', \n",
    "# # 'www.biorxiv.org', 'wakespace.lib.wfu.edu', 'symposium.cshlp.org', 'www.taylorfrancis.com', 'www.em-consulte.com', \n",
    "# # 'jamanetwork.com', 'direct.mit.edu', 'journals.biologists.com', 'journals.asm.org', 'www.science.org', 'karger.com', \n",
    "# # 'link.springer.com', 'www.embopress.org', 'www.frontiersin.org', 'www.imrpress.com', 'www.architalbiol.org', \n",
    "# # 'www.tandfonline.com', 'www.degruyter.com', 'www.jstage.jst.go.jp', 'pubs.aip.org', 'www.jstor.org', 'www.liebertpub.com', \n",
    "# # 'www.hindawi.com', 'royalsocietypublishing.org', 'cdnsciencepub.com', 'www.birpublications.org', 'neuro.psychiatryonline.org', \n",
    "# # 'content.iospress.com:443', 'journals.sagepub.com', 'linkinghub.elsevier.com', 'pubs.asahq.org', 'neurologia.com', \n",
    "# # 'www.ajtmh.org:443', 'www.nature.com', 'www.dovepress.com', 'jpet.aspetjournals.org', 'pharmrev.aspetjournals.org', \n",
    "# # 'ejnmmires.springeropen.com', 'iovs.arvojournals.org', 'journal.psych.ac.cn', 'open.bu.edu', 'www.researchsquare.com', \n",
    "# # 'www.ingentaconnect.com', 'www.hindawi.com:443', 'www.worldscientific.com', 'mathematical-neuroscience.springeropen.com', \n",
    "# # 'www.eneuro.org', 'webview.isho.jp', 'deepblue.lib.umich.edu', 'onlinelibrary.wiley.com'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # websites_hosts\n",
    "# # {'psycnet.apa.org:443', 'elifesciences.org', 'journals.physiology.org', 'thejns.org:443', 'www.jneurosci.org', \n",
    "# # 'opg.optica.org', 'europepmc.org', 'academic.oup.com', 'journals.lww.com', nan, 'pubmed.ncbi.nlm.nih.gov', \n",
    "# # 'ujms.net', 'thejns.org', 'www.mdpi.com', 'www.microbiologyresearch.org', 'ieeexplore.ieee.org', 'journals.plos.org', \n",
    "# # 'www.rbojournal.org', 'jnm.snmjournals.org', 'n.neurology.org', 'arjournals.annualreviews.org', 'www.thieme-connect.com', \n",
    "# # 'www.cambridge.org', 'pubs.acs.org', 'www.pnas.org', 'analyticalsciencejournals.onlinelibrary.wiley.com', \n",
    "# # 'journals.aps.org', 'papers.ssrn.com', 'www.ahajournals.org', 'www.ncbi.nlm.nih.gov', 'ajp.psychiatryonline.org', \n",
    "# # 'www.biorxiv.org', 'wakespace.lib.wfu.edu', 'symposium.cshlp.org', 'www.taylorfrancis.com', 'www.em-consulte.com', \n",
    "# # 'jamanetwork.com', 'direct.mit.edu', 'journals.biologists.com', 'journals.asm.org', 'www.science.org', 'karger.com', \n",
    "# # 'link.springer.com', 'www.embopress.org', 'www.frontiersin.org', 'www.imrpress.com', 'www.architalbiol.org', \n",
    "# # 'www.tandfonline.com', 'www.degruyter.com', 'www.jstage.jst.go.jp', 'pubs.aip.org', 'www.jstor.org', 'www.liebertpub.com', \n",
    "# # 'www.hindawi.com', 'royalsocietypublishing.org', 'cdnsciencepub.com', 'www.birpublications.org', 'neuro.psychiatryonline.org', \n",
    "# # 'content.iospress.com:443', 'journals.sagepub.com', 'linkinghub.elsevier.com', 'pubs.asahq.org', 'neurologia.com', \n",
    "# # 'www.ajtmh.org:443', 'www.nature.com', 'www.dovepress.com', 'jpet.aspetjournals.org', 'pharmrev.aspetjournals.org', \n",
    "# # 'ejnmmires.springeropen.com', 'iovs.arvojournals.org', 'journal.psych.ac.cn', 'open.bu.edu', 'www.researchsquare.com', \n",
    "# # 'www.ingentaconnect.com', 'www.hindawi.com:443', 'www.worldscientific.com', 'mathematical-neuroscience.springeropen.com', \n",
    "# # 'www.eneuro.org', 'webview.isho.jp', 'deepblue.lib.umich.edu', 'onlinelibrary.wiley.com'}\n",
    "\n",
    "websites_hosts = [\n",
    "    'psycnet.apa.org', 'elifesciences.org', 'physiology.org', 'jneurosci.org', \n",
    "    'opg.optica.org', 'europepmc.org', 'oup.com', 'lww.com', 'pubmed.ncbi.nlm.nih.gov', \n",
    "    'ujms.net', 'thejns.org', 'mdpi.com', 'microbiologyresearch.org', 'ieee.org', 'plos.org', \n",
    "    'rbojournal.org', 'snmjournals.org', 'neurology.org', 'annualreviews.org', 'thieme-connect.com', \n",
    "    'cambridge.org', 'acs.org', 'pnas.org', 'wiley.com', 'aps.org', 'ssrn.com', 'ahajournals.org', \n",
    "    'ncbi.nlm.nih.gov', 'psychiatryonline.org', 'biorxiv.org', 'lib.wfu.edu', 'symposium.cshlp.org', \n",
    "    'taylorfrancis.com', 'em-consulte.com', 'jamanetwork.com', 'mit.edu', 'biologists.com', 'asm.org', \n",
    "    'science.org', 'karger.com', 'springer.com', 'embopress.org', 'frontiersin.org', 'imrpress.com', \n",
    "    'architalbiol.org', 'tandfonline.com', 'degruyter.com', 'jstage.jst.go.jp', 'aip.org', 'jstor.org', \n",
    "    'liebertpub.com', 'hindawi.com', 'royalsocietypublishing.org', 'cdnsciencepub.com', 'birpublications.org', \n",
    "    'iospress.com', 'sagepub.com', 'elsevier.com', 'asahq.org', 'neurologia.com', 'ajtmh.org', 'nature.com', \n",
    "    'dovepress.com', 'aspetjournals.org', 'springeropen.com', 'arvojournals.org', 'psych.ac.cn', 'open.bu.edu', \n",
    "    'researchsquare.com', 'ingentaconnect.com', 'worldscientific.com', 'eneuro.org', 'isho.jp', 'lib.umich.edu',\n",
    "    'nan'\n",
    "]\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "if len(websites_hosts) == len(set(websites_hosts)):\n",
    "    print(\"There are no duplicates in the list.\")\n",
    "else:\n",
    "    print(\"There are duplicates in the list.\")\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the websites by the number of articles they have\n",
    "input_path = fpath.poten_litera_ids_ftl_filled\n",
    "df = pd.read_csv(input_path, header=None, sep=\",\")\n",
    "df.columns = [\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"FULL_TEXT_SOURCE\", \"PDF_URL\", \"PDF_SOURCE\", \"TITLE\", \"ABSTRACT\", \"KEYWORDS\"]\n",
    "func_dict = {website: 0 for website in websites_hosts}\n",
    "# print(func_dict)\n",
    "\n",
    "for ind in df.index:\n",
    "    if df.at[ind, \"FULL_TEXT_SOURCE\"] != df.at[ind, \"FULL_TEXT_SOURCE\"]:\n",
    "        func_dict[\"nan\"] += 1\n",
    "        continue\n",
    "    for website in websites_hosts:\n",
    "        if website in df.at[ind, \"FULL_TEXT_SOURCE\"]:\n",
    "            func_dict[website] += 1\n",
    "            break\n",
    "\n",
    "# Sort dictionary by values\n",
    "sorted_dict = dict(sorted(func_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "print(sorted_dict)\n",
    "# {'ncbi.nlm.nih.gov': 7781, 'elsevier.com': 1038, 'wiley.com': 713, 'springer.com': 292, 'physiology.org': 203, \n",
    "# 'oup.com': 157, 'pubmed.ncbi.nlm.nih.gov': 145, 'cambridge.org': 74, 'nature.com': 62, 'karger.com': 55, \n",
    "# 'lww.com': 50, 'nan': 34, 'science.org': 30, 'tandfonline.com': 27, 'sagepub.com': 27, 'frontiersin.org': 23, \n",
    "# 'jamanetwork.com': 20, 'neurology.org': 16, 'biorxiv.org': 16, 'europepmc.org': 14, 'royalsocietypublishing.org': 14, \n",
    "# 'arvojournals.org': 13, 'psycnet.apa.org': 12, 'psychiatryonline.org': 12, 'mit.edu': 11, 'jstage.jst.go.jp': 11, \n",
    "# 'mdpi.com': 10, 'acs.org': 10, 'annualreviews.org': 9, 'thejns.org': 8, 'snmjournals.org': 7, 'aspetjournals.org': 7, \n",
    "# 'jneurosci.org': 5, 'lib.umich.edu': 5, 'plos.org': 4, 'ahajournals.org': 4, 'architalbiol.org': 4, \n",
    "# 'elifesciences.org': 3, 'degruyter.com': 3, 'liebertpub.com': 3, 'iospress.com': 3, 'asahq.org': 3, \n",
    "# 'worldscientific.com': 3, 'opg.optica.org': 2, 'thieme-connect.com': 2, 'hindawi.com': 2, 'neurologia.com': 2, \n",
    "# 'dovepress.com': 2, 'springeropen.com': 2, 'eneuro.org': 2, 'ujms.net': 1, 'microbiologyresearch.org': 1, \n",
    "# 'ieee.org': 1, 'rbojournal.org': 1, 'pnas.org': 1, 'aps.org': 1, 'ssrn.com': 1, 'lib.wfu.edu': 1, \n",
    "# 'symposium.cshlp.org': 1, 'taylorfrancis.com': 1, 'em-consulte.com': 1, 'biologists.com': 1, 'asm.org': 1, \n",
    "# 'embopress.org': 1, 'imrpress.com': 1, 'aip.org': 1, 'jstor.org': 1, 'cdnsciencepub.com': 1, 'birpublications.org': 1, \n",
    "# 'ajtmh.org': 1, 'psych.ac.cn': 1, 'open.bu.edu': 1, 'researchsquare.com': 1, 'ingentaconnect.com': 1, 'isho.jp': 1}\n",
    "\n",
    "non_zero_keys = [key for key, value in sorted_dict.items() if value != 0]\n",
    "print(non_zero_keys)\n",
    "# ['ncbi.nlm.nih.gov', 'elsevier.com', 'wiley.com', 'springer.com', 'physiology.org', 'oup.com', \n",
    "# 'pubmed.ncbi.nlm.nih.gov', 'cambridge.org', 'nature.com', 'karger.com', 'lww.com', 'nan', \n",
    "# 'science.org', 'tandfonline.com', 'sagepub.com', 'frontiersin.org', 'jamanetwork.com', \n",
    "# 'neurology.org', 'biorxiv.org', 'europepmc.org', 'royalsocietypublishing.org', 'arvojournals.org', \n",
    "# 'psycnet.apa.org', 'psychiatryonline.org', 'mit.edu', 'jstage.jst.go.jp', 'mdpi.com', 'acs.org', \n",
    "# 'annualreviews.org', 'thejns.org', 'snmjournals.org', 'aspetjournals.org', 'jneurosci.org', \n",
    "# 'lib.umich.edu', 'plos.org', 'ahajournals.org', 'architalbiol.org', 'elifesciences.org', \n",
    "# 'degruyter.com', 'liebertpub.com', 'iospress.com', 'asahq.org', 'worldscientific.com', \n",
    "# 'opg.optica.org', 'thieme-connect.com', 'hindawi.com', 'neurologia.com', 'dovepress.com', \n",
    "# 'springeropen.com', 'eneuro.org', 'ujms.net', 'microbiologyresearch.org', 'ieee.org', 'rbojournal.org', \n",
    "# 'pnas.org', 'aps.org', 'ssrn.com', 'lib.wfu.edu', 'symposium.cshlp.org', 'taylorfrancis.com', \n",
    "# 'em-consulte.com', 'biologists.com', 'asm.org', 'embopress.org', 'imrpress.com', 'aip.org', \n",
    "# 'jstor.org', 'cdnsciencepub.com', 'birpublications.org', 'ajtmh.org', 'psych.ac.cn', 'open.bu.edu', \n",
    "# 'researchsquare.com', 'ingentaconnect.com', 'isho.jp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # websites\n",
    "websites = [\n",
    "    'ncbi.nlm.nih.gov', 'elsevier.com', 'wiley.com', 'springer.com', 'physiology.org', 'oup.com', \n",
    "    'pubmed.ncbi.nlm.nih.gov', 'cambridge.org', 'nature.com', 'karger.com', 'lww.com', 'nan', \n",
    "    'science.org', 'tandfonline.com', 'sagepub.com', 'frontiersin.org', 'jamanetwork.com', \n",
    "    'neurology.org', 'biorxiv.org', 'europepmc.org', 'royalsocietypublishing.org', 'arvojournals.org', \n",
    "    'psycnet.apa.org', 'psychiatryonline.org', 'mit.edu', 'jstage.jst.go.jp', 'mdpi.com', 'acs.org', \n",
    "    'annualreviews.org', 'thejns.org', 'snmjournals.org', 'aspetjournals.org', 'jneurosci.org', \n",
    "    'lib.umich.edu', 'plos.org', 'ahajournals.org', 'architalbiol.org', 'elifesciences.org', \n",
    "    'degruyter.com', 'liebertpub.com', 'iospress.com', 'asahq.org', 'worldscientific.com', \n",
    "    'opg.optica.org', 'thieme-connect.com', 'hindawi.com', 'neurologia.com', 'dovepress.com', \n",
    "    'springeropen.com', 'eneuro.org', 'ujms.net', 'microbiologyresearch.org', 'ieee.org', 'rbojournal.org', \n",
    "    'pnas.org', 'aps.org', 'ssrn.com', 'lib.wfu.edu', 'symposium.cshlp.org', 'taylorfrancis.com', \n",
    "    'em-consulte.com', 'biologists.com', 'asm.org', 'embopress.org', 'imrpress.com', 'aip.org', \n",
    "    'jstor.org', 'cdnsciencepub.com', 'birpublications.org', 'ajtmh.org', 'psych.ac.cn', 'open.bu.edu', \n",
    "    'researchsquare.com', 'ingentaconnect.com', 'isho.jp'\n",
    "]\n",
    "# # --------------------start of test code--------------------\n",
    "# if len(websites) == len(websites_hosts):\n",
    "#     print('The number of websites is correct')\n",
    "# # ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Next step: automatic filtering </h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
