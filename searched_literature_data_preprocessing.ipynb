{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Searched literature data preprocessing </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 14:51:59 Didis-MacBook-Pro.local metapub.config[41501] WARNING NCBI_API_KEY was not set.\n"
     ]
    }
   ],
   "source": [
    "# import internal .py modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Parameters: </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns of file: potential_related_literature.csv\n",
    "columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "# e.g., [\"10.1113/JP282626\", \"35851953\", \"PMC10087288\", \n",
    "#        \"Cortico-thalamocortical interactions for learning, memory and decision-making\",\n",
    "#        \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10087288/\", \"PMC\",\n",
    "#        \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10087288/pdf/TJP-601-25.pdf\", \"PMC\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Predefined fucntions: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pubmed(source_path, output_path, columns, start, end):\n",
    "    print(\"Starting merging search results from PubMed...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, sep=',')\n",
    "    df = df[[\"DOI\", \"PMID\", \"PMCID\", \"Title\"]]\n",
    "    \n",
    "    for ind in range(start, end):\n",
    "        # sleep to avoid to be blocked\n",
    "        time.sleep(random.randint(3,6))\n",
    "        # if(ind%50 == 0):\n",
    "        #     time.sleep(random.randint(10,15)*10)\n",
    "        \n",
    "        #request the webpage\n",
    "        # the columns PMID, Title don't contain np.nan\n",
    "        pmid = str(df[\"PMID\"][ind]).strip()\n",
    "        url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "        proxies = plib.get_proxies()\n",
    "        soup = plib.request_webpage(url, proxies)\n",
    "        # print(soup)\n",
    "        \n",
    "        # get pmcid\n",
    "        if df[\"PMCID\"][ind] is np.nan:\n",
    "            try:\n",
    "                pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                pmcid = np.nan\n",
    "        else:\n",
    "            pmcid = str(df[\"PMCID\"][ind]).strip()\n",
    "        # print(pmcid)\n",
    "\n",
    "        # get doi\n",
    "        if df[\"DOI\"][ind] is np.nan:\n",
    "            try:\n",
    "                doi = soup.find_all(\"span\", {\"class\": \"identifier doi\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except:\n",
    "                doi  = np.nan\n",
    "        else:\n",
    "            doi = str(df[\"DOI\"][ind]).strip()\n",
    "        # print(doi)\n",
    "\n",
    "        # get full_text_url, full_text_source\n",
    "        if pmcid is not np.nan:\n",
    "            full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "            full_text_source = \"PMC\"\n",
    "        else:\n",
    "            # PMC does not include this paper\n",
    "            try:\n",
    "                full_text_url = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"href\"].strip()\n",
    "                full_text_source = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"data-ga-action\"].strip()\n",
    "            except:\n",
    "                full_text_url = np.nan\n",
    "                full_text_source = np.nan\n",
    "        # print(full_text_url)\n",
    "        # print(full_text_source)\n",
    "        \n",
    "        # get pdf_url, pdf_source\n",
    "        pdf_url = np.nan\n",
    "        pdf_source = np.nan\n",
    "                \n",
    "        # columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"Title\": [str(df[\"Title\"][ind]).strip()],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"full_text_source\": [full_text_source],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"pdf_source\": [pdf_source]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_pubmed\n",
    "# output_path = fpath.poten_litera_pubmed_processed\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(source_path, sep=',')\n",
    "# print(df.shape)\n",
    "# df = df[[\"DOI\", \"PMID\", \"PMCID\", \"Title\"]]\n",
    "# print(df.head(3))\n",
    "\n",
    "# print(df[\"DOI\"].isnull().values.any())\n",
    "# print(df[\"PMID\"].isnull().values.any())\n",
    "# print(df[\"PMCID\"].isnull().values.any())\n",
    "# print(df[\"Title\"].isnull().values.any())\n",
    "# # True, False, True, Flase\n",
    "# # PMID, Title don't contain np.nan\n",
    "# # DOI, PMCID contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# merge_pubmed(source_path, output_path, columns)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            DOI  Pubmed Id  \\\n",
      "0  10.1016/0304-3940(96)12319-3  8929980.0   \n",
      "1  10.1016/0304-3940(93)90180-S  7689715.0   \n",
      "2         10.1002/cne.903440403  7523458.0   \n",
      "\n",
      "                                       Article Title  \n",
      "0  Crossed thalamo-cortical and cortico-thalamic ...  \n",
      "1  THE RETICULAR THALAMIC NUCLEUS PROJECTS TO THE...  \n",
      "2  CONTRALATERAL THALAMIC PROJECTIONS PREDOMINANT...  \n",
      "True\n",
      "True\n",
      "False\n",
      "Starting merging search results from Web of Science...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 14:53:23 Didis-MacBook-Pro.local metapub.findit[41501] INFO FindIt Cache initialized at /Users/didihou/.cache/findit.db\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 14:53:53 Didis-MacBook-Pro.local metapub.DxDOI[41501] INFO cached results for key 10.1007/BF00231845 (10.1007/BF00231845) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDC Converter: nan\n",
      "Metapub: 10.1007/BF00231845\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Results are different, check your functions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 117\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mprint\u001b[39m(df[\u001b[39m\"\u001b[39m\u001b[39mArticle Title\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39misnull()\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39many())\n\u001b[1;32m    110\u001b[0m \u001b[39m# True, True, False\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m# Article Title don't contain np.nan\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m# DOI, Pubmed Id contain np.nan\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m \n\u001b[1;32m    116\u001b[0m \u001b[39m# --------------------start of test code--------------------\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m merge_webofscience(source_path, output_path, columns)\n",
      "Cell \u001b[0;32mIn[5], line 35\u001b[0m, in \u001b[0;36mmerge_webofscience\u001b[0;34m(source_path, output_path, columns)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mIDC Converter:\u001b[39m\u001b[39m\"\u001b[39m, doi_1)\n\u001b[1;32m     34\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMetapub:\u001b[39m\u001b[39m\"\u001b[39m, doi_2)\n\u001b[0;32m---> 35\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mResults are different, check your functions.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     \u001b[39m# get pmid from doi if possible\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     doi \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(df[\u001b[39m\"\u001b[39m\u001b[39mDOI\u001b[39m\u001b[39m\"\u001b[39m][ind])\u001b[39m.\u001b[39mstrip()\n",
      "\u001b[0;31mException\u001b[0m: Results are different, check your functions."
     ]
    }
   ],
   "source": [
    "def merge_webofscience(source_path, output_path, columns):\n",
    "    print(\"Starting merging search results from Web of Science...\")\n",
    "    \n",
    "    df = pd.read_csv(source_path, sep=\";\")\n",
    "    df = df[[\"DOI\", \"Pubmed Id\", \"Article Title\"]]\n",
    "    df.rename(columns={\"DOI\": \"DOI\", \"Pubmed Id\": \"PMID\", \"Article Title\": \"Title\"}, inplace = True, errors= \"raise\")\n",
    "\n",
    "    for ind in df.index:\n",
    "        # sleep to avoid to be blocked\n",
    "        time.sleep(random.randint(5,10))\n",
    "        # if(ind%50 == 0):\n",
    "        #     time.sleep(random.randint(10,15)*10)\n",
    "        \n",
    "        # the columns Article Title don't contain np.nan\n",
    "        # the columns DOI and PMID might contain np.nan\n",
    "        # get pmid, doi\n",
    "        if df[\"PMID\"][ind] is np.nan:\n",
    "            if df[\"DOI\"][ind] is np.nan:\n",
    "                doi = np.nan\n",
    "                pmid = np.nan\n",
    "            else:\n",
    "                doi = str(df[\"DOI\"][ind]).strip()\n",
    "                pmid = plib.doi2pmid(doi)\n",
    "        else:\n",
    "            pmid = str(int(df[\"PMID\"][ind])).strip()\n",
    "            if df[\"DOI\"][ind] is np.nan:\n",
    "                doi = plib.pmid2doi(pmid)\n",
    "            else:\n",
    "                doi = str(df[\"DOI\"][ind]).strip()\n",
    "        \n",
    "        # get pmcid, full_text_url, full_text_source\n",
    "        if pmid is np.nan:\n",
    "            pmcid = np.nan\n",
    "            if doi == np.nan:\n",
    "                full_text_url = np.nan\n",
    "                full_text_source = np.nan\n",
    "            else:\n",
    "                full_text_url = \"https://doi.org/\" + str(doi).strip()\n",
    "                full_text_source = \"DOI\"\n",
    "        else:\n",
    "            # request the webpage\n",
    "            url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "            # proxies = plib.get_proxies()\n",
    "            soup = plib.request_webpage(url)\n",
    "            # print(soup)\n",
    "\n",
    "            # pmcid\n",
    "            pmcid = plib.id_converter_in_pubmed(pmid, \"pmcid\")\n",
    "            # print(pmcid)\n",
    "            \n",
    "            # get full_text_url, full_text_source\n",
    "            if pmcid is not np.nan:\n",
    "                full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "                full_text_source = \"PMC\"\n",
    "            else:\n",
    "                try:\n",
    "                    full_text_url = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"href\"].strip()\n",
    "                    full_text_source = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"data-ga-action\"].strip()\n",
    "                except:\n",
    "                    full_text_url = np.nan\n",
    "                    full_text_source = np.nan\n",
    "        \n",
    "        # get pdf_url, pdf_source\n",
    "        pdf_url = np.nan\n",
    "        pdf_source = np.nan\n",
    "\n",
    "        # columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"Title\": [str(df[\"Title\"][ind]).strip()],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"full_text_source\": [full_text_source],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"pdf_source\": [pdf_source]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "source_path = fpath.poten_litera_wos_1\n",
    "output_path = fpath.poten_litera_wos_processed\n",
    "plib.clear_file(output_path)\n",
    "# ---------------------end of test code--------------------- \n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "df = pd.read_csv(source_path, sep=';')\n",
    "df = df[[\"DOI\", \"Pubmed Id\", \"Article Title\"]]\n",
    "print(df.head(3))\n",
    "# ---------------------end of test code--------------------- \n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "print(df[\"DOI\"].isnull().values.any())\n",
    "print(df[\"Pubmed Id\"].isnull().values.any())\n",
    "print(df[\"Article Title\"].isnull().values.any())\n",
    "# True, True, False\n",
    "# Article Title don't contain np.nan\n",
    "# DOI, Pubmed Id contain np.nan\n",
    "# we need to fill in what are missing\n",
    "# ---------------------end of test code--------------------- \n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "merge_webofscience(source_path, output_path, columns)\n",
    "# ---------------------end of test code--------------------- \n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_eupmc(source_path, output_path, columns):\n",
    "    print(\"Starting merging search results from Europe PMC...\")\n",
    "    # process eupmc search results\n",
    "    df = pd.read_csv(source_path, sep = \",\")\n",
    "    df = df[[\"DOI\", \"EXTERNAL_ID\", \"PMCID\", \"TITLE\"]]\n",
    "    df = df.rename(columns={\"EXTERNAL_ID\": \"PMID\", \"TITLE\": \"Title\"}, errors = \"raise\")\n",
    "    for ind in df.index:\n",
    "        print(ind)\n",
    "        if(ind%10 == 0):\n",
    "            time.sleep(random.randint(3,6)*10)\n",
    "        proxies = plib.get_proxies()\n",
    "        pmid = str(df[\"PMID\"][ind])\n",
    "        # print(pmid)\n",
    "        url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "        regex = \"[a-zA-Z]\"\n",
    "        if len(re.findall(regex, pmid)) == 0:\n",
    "            # print(\"pmid\")\n",
    "            soup = plib.request_webpage(url)\n",
    "            # print(soup)\n",
    "\n",
    "            # get PMCID\n",
    "            # print(df[\"PMCID\"][ind])\n",
    "            if df[\"PMCID\"][ind] is np.nan:\n",
    "                try:\n",
    "                    pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "                except:\n",
    "                    pmcid  =\"not found\"\n",
    "            else:\n",
    "                pmcid = str(df[\"PMCID\"][ind])\n",
    "            # print(pmcid)\n",
    "            # get DOI\n",
    "            if df[\"DOI\"][ind] is np.nan:\n",
    "                try:\n",
    "                    doi = soup.find_all(\"span\", {\"class\": \"identifier doi\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "                except:\n",
    "                    doi  =\"not found\"\n",
    "            else:\n",
    "                doi = str(df[\"DOI\"][ind])\n",
    "            # get full_text_url\n",
    "            if pmcid != \"not found\":\n",
    "                full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "                full_text_source = \"PMC\"\n",
    "            else:\n",
    "                try:\n",
    "                    full_text_url = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"href\"].strip()\n",
    "                    full_text_source = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"data-ga-action\"].strip()\n",
    "                except:\n",
    "                    full_text_url = \"not found\"\n",
    "                    full_text_source = \"not found\"\n",
    "            # columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"First_Author\", \"full_text_url\", \"full_text_source\"]\n",
    "        else:\n",
    "            # print(\"not pmid\")\n",
    "            if df[\"DOI\"][ind] is np.nan:\n",
    "                doi  =\"not found\"\n",
    "            else:\n",
    "                doi = str(df[\"DOI\"][ind])\n",
    "            if df[\"PMCID\"][ind] is np.nan:\n",
    "                full_text_url = \"not found\"\n",
    "                full_text_source = \"not found\"\n",
    "                pmcid = \"not found\"\n",
    "            else:\n",
    "                full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "                full_text_source = \"PMC\"\n",
    "                pmcid = str(df[\"PMCID\"][ind])\n",
    "            first_author = \"not found\"\n",
    "            pmid = \"not found\"\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"Title\": [str(df[\"Title\"][ind])],\n",
    "            \"First_Author\": [first_author],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"full_text_source\": [full_text_source]\n",
    "        }\n",
    "        # print(row)\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_eupmc\n",
    "# output_path = fpath.poten_litera_eupmc_processed\n",
    "# plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(source_path, sep=',')\n",
    "# df = df[[\"SOURCE\", \"DOI\", \"EXTERNAL_ID\", \"PMCID\", \"TITLE\"]]\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# col_one_list = set(df['SOURCE'].tolist())\n",
    "# print(col_one_list)\n",
    "# \n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# print(df[\"SOURCE\"].isnull().values.any())\n",
    "# print(df[\"DOI\"].isnull().values.any())\n",
    "# print(df[\"EXTERNAL_ID\"].isnull().values.any())\n",
    "# print(df[\"PMCID\"].isnull().values.any())\n",
    "# print(df[\"TITLE\"].isnull().values.any())\n",
    "# # PMID, Title don't contain np.nan\n",
    "# # DOI, PMCID contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# merge_eupmc(source_path, output_path, columns)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_google_shcolar(source_path, output_path, columns):\n",
    "    print(\"Starting merging search results from Google Scholar...\")\n",
    "    return True\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_gs_test\n",
    "# output_path = fpath.poten_litera_gs_processed\n",
    "# plib.clear_file(output_path)\n",
    "# df = pd.read_csv(source_path, header = None, sep=',')\n",
    "# df = df[[\"SOURCE\", \"DOI\", \"EXTERNAL_ID\", \"PMCID\", \"TITLE\"]]\n",
    "# print(df.head(5))\n",
    "# col_one_list = set(df['SOURCE'].tolist())\n",
    "# print(col_one_list)\n",
    "# print(df[\"SOURCE\"].isnull().values.any())\n",
    "# print(df[\"DOI\"].isnull().values.any())\n",
    "# print(df[\"EXTERNAL_ID\"].isnull().values.any())\n",
    "# print(df[\"PMCID\"].isnull().values.any())\n",
    "# print(df[\"TITLE\"].isnull().values.any())\n",
    "# # the columns PMID, Title don't contain np.nan\n",
    "# # the columns DOI, PMCID contain np.nan, we need to fill in what are missing\n",
    "# # we also need to reenter the full name of the first author\n",
    "# merge_google_shcolar(source_path, output_path, columns)\n",
    "\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_seed_paper_spanning(source_path, output_path, columns):\n",
    "    print(\"Starting merging search results from spanning citations of seed paper...\")\n",
    "    return True\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_cocomac_paper(source_path, output_path, columns):\n",
    "    print(\"Starting merging search results from CoCoMac papers...\")\n",
    "    return True\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure at least PMID and PMCID is present as two of the four identifiers, otherwise manually fill in\n",
    "def fill_in_elements(file_path):\n",
    "    # PMID -> PMCID\n",
    "    # done already\n",
    "    # PMCID -> PMID\n",
    "    # done already\n",
    "    # PMID -> DOI\n",
    "    df = pd.read_csv(file_path, sep = \",\")\n",
    "    for ind in df.index:\n",
    "        if (df[\"PMID\"][ind] is not np.nan) and (df[\"DOI\"][ind] is np.nan):\n",
    "            pmid = df[\"PMID\"][ind]\n",
    "            url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "            print(url)\n",
    "            response = requests.get(url, headers = plib.headers)\n",
    "            if response.status_code != 200:\n",
    "                raise Exception(\"Error when request webpages!\")\n",
    "            soup = BeautifulSoup(response.content, \"lxml\")\n",
    "            l = soup.find_all(\"a\", {\"class: id-link\"}, {\"data-ga-action\": \"DOI\"})\n",
    "            if(len(l) != 0):\n",
    "                # print(l[0].get_text().strip())\n",
    "                df.at[ind, \"DOI\"] = l[0].get_text().strip()\n",
    "            else:\n",
    "                df.at[ind, \"DOI\"] = np.nan\n",
    "    df.to_csv(fpath.poten_litera_csv, header = True, index = False)\n",
    "    print(\"All 3 identifiers: DOI, PMID, and PMCID filled in when possible.\")\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplciations based on identifiers in the potential related literature\n",
    "def remove_dupli(file_path):\n",
    "    df = pd.read_csv(file_path, sep = \",\")\n",
    "    print(len(df))\n",
    "    df = df.drop_duplicates(subset=['DOI'])\n",
    "    df = df.drop_duplicates(subset=['PMID'])\n",
    "    df = df.drop_duplicates(subset=['PMCID'])\n",
    "    print(len(df))\n",
    "    # plib.clear_file(fpath.poten_litera_csv)\n",
    "    # df.csv(fpath.poten_litera_csv, idnex = None)\n",
    "    print(\"Duplication in the potential related literature removed.\")\n",
    "    print(\"Found \" + len(df) + \" potential related literature in total.\")\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Main program: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the file\n",
    "# source_path = fpath.poten_litera_pubmed\n",
    "# output_path = fpath.poten_litera_pubmed_processed\n",
    "# plib.clear_file(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge search results from PubMed\n",
    "# # 2606 results\n",
    "# merge_pubmed(source_path, output_path, columns, 2542, 2606)\n",
    "# print(\"Merging results from PubMed succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when merging results from PubMed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clear the file\n",
    "# source_path = fpath.poten_litera_wos_1\n",
    "# output_path = fpath.poten_litera_wos_processed\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# # merge search results from Web of Science\n",
    "# # 1000 results\n",
    "# merge_webofscience(source_path, output_path, columns)\n",
    "# print(\"Merging results from Web of Science part 1 succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when merging results from Web of Science part 1!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clear the file\n",
    "# source_path = fpath.poten_litera_wos_2\n",
    "# output_path = fpath.poten_litera_wos_processed\n",
    "\n",
    "# # merge search results from Web of Science\n",
    "# # 976 results\n",
    "# merge_webofscience(source_path, output_path, columns)\n",
    "# print(\"Merging results from Web of Science part 2 succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when merging results from Web of Science part 2!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clear the file\n",
    "# source_path = fpath.poten_litera_eupmc\n",
    "# output_path = fpath.poten_litera_eupmc_processed\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# # merge search results from Europe PMC\n",
    "# merge_eupmc(source_path, output_path, columns)\n",
    "# # 9139 results\n",
    "# print(\"Merging results from Europe PMC succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when merging results from Europe PMC!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clear the file\n",
    "# source_path = fpath.poten_litera_gs_test\n",
    "# output_path = fpath.poten_litera_gs_processed\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# # merge search results from Google Scholar\n",
    "# merge_google_shcolar(source_path, output_path, columns)\n",
    "# print(\"Merging results from Google Scholar succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when merging results from Google Scholar!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge search results from spanning citations of seed paper\n",
    "# merge_seed_paper_spanning(source_path, output_path, columns):\n",
    "# print(\"Merging results from spanning citations of seed papers succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when merging results from spanning citations of seed papers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge search results from CoCoMac papers\n",
    "# merge_cocomac_paper(source_path, output_path, columns)\n",
    "# print(\"Merging results from CoCoMac papers succeeded!\")\n",
    "# # print(\"Attention! Something went wrong when merging results from CoCoMac papers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fill in all elements in the columns when possible, if not, fill in \"not found\"\n",
    "# fill_in_elements(fpath.poten_litera_csv, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifier = [\"DOI\", \"PMID\", \"PMCID\"]\n",
    "# remove_dupli(fpath.poten_litera_csv, identifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Next step: automatic filtering </h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
