{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the information sheet for the final manually read article list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib\n",
    "import parameters as params\n",
    "import dataframe_columns as df_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predefined functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_sent(sent):\n",
    "    if not (sent.endswith('.') or sent.endswith('?') or sent.endswith('!')):\n",
    "        sent += '.'\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sent_from_text(text, kw_group):\n",
    "    if text != text or text == \"\":\n",
    "        return 'NA'\n",
    "    # preprocess text\n",
    "    text = plib.process_text(text, lower=True) \n",
    "    \n",
    "    sents = []\n",
    "\n",
    "    # sentence tokenize\n",
    "    all_sentences = sent_tokenize(text)\n",
    "\n",
    "    for sent in all_sentences:\n",
    "        # if the sentence has been added, continue\n",
    "        compl_sent = complete_sent(sent)\n",
    "        if compl_sent in sents:\n",
    "            continue\n",
    "        \n",
    "        # else, match and add\n",
    "        flag = False # this sentence is not matched yet\n",
    "\n",
    "        for keyword in kw_group:\n",
    "            keyword = keyword.lower()\n",
    "            # print(keyword)\n",
    "            \n",
    "            # match keyword\n",
    "            if keyword in params.exact_match_kw_list: # If the keyword is in exact match keyword list, use exact match\n",
    "                words = word_tokenize(compl_sent)\n",
    "                \n",
    "                for word in words:\n",
    "                    if word == keyword:\n",
    "                        sents.append(compl_sent)\n",
    "                        flag = True\n",
    "                        break\n",
    "                if flag:\n",
    "                    break\n",
    "            elif keyword in compl_sent: # If the keyword is not in exact match keyword list, use fuzzy match\n",
    "                sents.append(compl_sent)\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    # convert set to string\n",
    "    sents_combined = ' '.join(sent for sent in sents)\n",
    "    sents_combined = sents_combined.strip()\n",
    "\n",
    "    return sents_combined\n",
    "# --------------------Start of test code--------------------\n",
    "# text = \"Effect of Attentive Fixation in macaque thalamus and Cortex. D. B. BENDER AND M. YOUAKIM Department of Physiology and Biophysics, School of Medicine and Biomedical Sciences, University at Buffalo, State University of New York, Buffalo, New York 14214 Received 29 December 1999; accepted in final form 21 September 2000 Bender, D. B. and M. Youakim. Effect of attentive fixation in macaque thalamus and cortex. J Neurophysiol 85: 219234, 2001. Attentional modulation of neuronal responsiveness is common in many areas of visual cortex. We examined whether attentional modulation in the visual thalamus was quantitatively similar to that in cortex. Identical procedures and apparatus were used to compare attentional modulation of single neurons in seven different areas of the visual system: the lateral geniculate, three visual subdivisions of the pulvinar [inferior, lateral, dorsomedial part of lateral pulvinar (Pdm)], and three areas of extrastriate cortex representing early, intermediate, and late stages of cortical processing (V2, V4/PM, area 7a). A simple fixation task controlled transitions among three attentive states. The animal waited for a fixation point to appear (ready state), fixated the point until it dimmed (fixation state), and then waited idly to begin the next trial (idle state). Attentional modulation was estimated by flashing an identical, irrelevant stimulus in a neurons receptive field during each of the three states; the three responses defined a response vector whose deviation from the line of equal response in all three states (the main diagonal) indicated the character and magnitude of attentional modulation. Attentional modulation was present in all visual areas except the lateral geniculate, indicating that modulation was of central origin. Prevalence of modulation was modest (26%) in pulvinar, and increased from 21% in V2 to 43% in 7a. Modulation had a push-pull character (as many cells facilitated as suppressed) with respect to the fixation state in all areas except Pdm where all cells were suppressed during fixation. The absolute magnitude of attentional modulation, measured by the angle between response vector and main diagonal expressed as a percent of the maximum possible angle, differed among brain areas. Magnitude of modulation was modest in the pulvinar (1926%), and increased from 22% in V2 to 41% in 7a. However, average trial-to-trial variability of response, measured by the coefficient of variation, also increased across brain areas so that its difference among areas accounted for more than 90% of the difference in modulation magnitude among areas. We also measured attentional modulation by the ratio of cell discharge due to attention divided by discharge variability. The resulting signal-tonoise ratio of attention was small and constant, 1.3 6 10%, across all areas of pulvinar and cortex. We conclude that the pulvinar, but not the lateral geniculate, is as strongly affected by attentional state as any area of visual cortex we studied and that attentional modulation amplitude is closely tied to intrinsic variability of response. INTRODUCTION It is now clear that attention can affect the responsiveness of neurons throughout visual cortex. Visually responsive cortex includes a number of distinct areas beyond striate cortex, or V1. Beginning with V2, these extrastriate areas are organized into two partially segregated, roughly hierarchical systems (reviews in Felleman and Van Essen 1991; Maunsell and Newsome 1987; Ungerleider and Mishkin 1982; Van Essen 1985). One includes dorsally located areas such as V3A, MT, and MST and leads into area 7a in the inferior parietal lobule. The other includes more ventrally located areas such as V4 and TEO and leads into area TE in the temporal lobe. Recordings from single neurons in many of these areas show that neuronal excitability depends on the animals attentive state (reviews in Colby 1991; Desimone and Duncan 1995; Lock and Bender 1999; Maunsell 1995; Motter 1998). Typically the effect of attention is modest: a small increase or decrease in magnitude of response to a visual stimulus relative to a control condition. Such modulation can be found at virtually every level of the cortical hierarchy, including V1. A variety of behavioral paradigms have been used to manipulate attention, and these show that the prevalence and magnitude of attentional modulation can depend substantially on both the behavioral paradigm and the cortical area in which its effects are measured. Furthermore factors such as task difficulty, the extent to which a task engages the functions of an area, and whether multiple stimuli compete for attention all can affect the modulation (Luck et al. 1997; Motter 1993; Richmond and Sato 1987). To what extent does the thalamus contribute to, or participate in, the attentional modulation that is so widespread throughout visual cortex? Three thalamic nuclei are closely interrelated with visual cortex: the lateral geniculate nucleus, the pulvinar, and the reticular nucleus of the thalamus. All have been thought to be involved in one form of attention or another (e.g., Guillery et al. 1998; Koch and Ullman 1985; Olshausen et al. 1993). The lateral geniculate projects almost exclusively to V1 with little or no output to extrastriate cortex. Layer 6 of both extrastriate and striate cortex project back to the geniculate, potentially modulating transmission through it. The pulvinar has at least three distinct visual subdivisions. The inferior (PI) and lateral pulvinar (PL) contain two separate visuotopic maps (Bender 1981). PI is driven by input from V1 (Bender 1983) but also receives input from extrastriate cortex and the superior colliculus. It projects mainly to V2, V3, V3A, and MT. PL likewise receives input from V1 and extrastriate cortex, but may have a particular affinity\"\n",
    "# kw_group = ['Cortex', 'Thalamus', 'Macaque']\n",
    "# sents = extract_sent_from_text(text, kw_group)\n",
    "# print(sents)\n",
    "# ---------------------End of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(index, title, abstract, keywords):\n",
    "    txt_file_name = str(index) + \".txt\"\n",
    "    txt_path = os.path.join(fpath.text_folder, txt_file_name)\n",
    "    txt_500_path = os.path.join(fpath.processed_texts_of_length_500_folder, txt_file_name)\n",
    "    \n",
    "    text_tak = \"\"           # text from title, abstract, and keywords\n",
    "    text_500 = \"\"           # text from full text 500\n",
    "    text_full = \"\"          # text from full text\n",
    "\n",
    "    # from title, abstract, and keywords\n",
    "    # extract first 500 words from text_full and text_tak, if they are longer than 500 words\n",
    "    # if they are shorter than 500 words, expand them to 500 words by repeating them\n",
    "    if title == title:\n",
    "        text_tak = text_tak + title + \" \"\n",
    "    else:\n",
    "        pass  \n",
    "    if abstract == abstract:\n",
    "        text_tak = text_tak + abstract + \" \"\n",
    "    else:\n",
    "        pass\n",
    "    if keywords == keywords:\n",
    "        text_tak = text_tak + keywords + \" \"\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    text_tak = plib.process_text(text_tak, lower=True)\n",
    "        \n",
    "    if len(text_tak.split()) != 0:\n",
    "        text_tak = plib.process_text(text_tak, lower=True)\n",
    "        while len(text_tak.split()) < params.text_length_to_extract:\n",
    "            text_tak = text_tak + \" \" + text_tak\n",
    "        text_tak = ' '.join(text_tak.split()[:params.text_length_to_extract])\n",
    "        text_tak = plib.process_text(text_tak, lower=True)\n",
    "    else:\n",
    "        text_tak = \"\"\n",
    "    # print(text_tak)\n",
    "    # print(len(text_tak.split()))\n",
    "\n",
    "    # from limited length full text\n",
    "    if os.path.exists(txt_500_path):\n",
    "        with open(txt_500_path, \"r\", encoding='ascii') as f:\n",
    "            text_500 = f.read()\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    if len(text_500.split()) != 0:\n",
    "        text_500 = plib.process_text(text_500, lower=True)\n",
    "        while len(text_500.split()) < params.text_length_to_extract:\n",
    "            text_500 = text_500 + \" \" + text_500\n",
    "        text_500 = ' '.join(text_500.split()[:params.text_length_to_extract])\n",
    "        text_500 = plib.process_text(text_500, lower=True)\n",
    "    else:\n",
    "        text_500 = \"\"\n",
    "    # print(text_500)\n",
    "    # print(len(text_500.split()))\n",
    "    \n",
    "    # from full text\n",
    "    if os.path.exists(txt_path):\n",
    "        with open(txt_path, \"r\", encoding='ascii') as f:\n",
    "            text_full = f.read()\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    if len(text_full.split()) != 0:\n",
    "        text_full = plib.process_text(text_full, lower=True)\n",
    "    else:\n",
    "        text_full = \"\"\n",
    "    # print(text_full)\n",
    "    # print(len(text_full.split()))\n",
    "    \n",
    "    return text_tak, text_500, text_full\n",
    "# --------------------Start of test code--------------------\n",
    "# index = 0\n",
    "# title = \"hello\"\n",
    "# abstract = \"world\"\n",
    "# keywords =\"!\"\n",
    "# text_tak, text_500, text_full = get_texts(index, title, abstract, keywords)\n",
    "# ---------------------End of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_large_df(df):\n",
    "    # Number of rows per split\n",
    "    rows_per_split = 1000\n",
    "\n",
    "    # Calculate total number of splits needed\n",
    "    num_splits = (len(df) + rows_per_split - 1) // rows_per_split  # This ensures any remainder is handled\n",
    "\n",
    "    # Create and save the smaller DataFrames\n",
    "    for i in range(num_splits):\n",
    "        start_index = i * rows_per_split\n",
    "        end_index = start_index + rows_per_split\n",
    "        split_df = df.iloc[start_index:end_index]\n",
    "\n",
    "        # Save each split to a CSV file\n",
    "        split_df.to_csv(f'./datasets/literature_search_datasets/final_manually_read_csv_{i+1}.csv', sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sents_and_record(input_path, output_path):\n",
    "    df_input = pd.read_csv(input_path, header=0, sep='\\t')\n",
    "    \n",
    "    df_output = df_input.copy()\n",
    "    \n",
    "    for ind in df_input.index:\n",
    "        # get texts\n",
    "        index = int(df_input.at[ind, \"INDEX\"])\n",
    "        title = df_input.at[ind, \"TITLE\"]\n",
    "        abstract = df_input.at[ind, \"ABSTRACT\"]\n",
    "        keywords = df_input.at[ind, \"KEYWORDS\"]\n",
    "        text_tak, text_500, text_full = get_texts(index, title, abstract, keywords)\n",
    "\n",
    "        # text columns\n",
    "        text_column = df_col.text_columns_to_add\n",
    "        columns_to_fill = df_col.columns_to_fill\n",
    "        keys_list = list(params.ranking_kw_groups.keys())\n",
    "\n",
    "        # decide for the text\n",
    "        if text_full == text_full and text_full != \"\": # if full text is available, use full text\n",
    "            text = text_full\n",
    "        else: # otherwise, use tak (title + abstract + keywords)\n",
    "            text = text_tak\n",
    "\n",
    "        # extract sentences from text\n",
    "        text_list = []\n",
    "        for key in keys_list:\n",
    "            sents = extract_sent_from_text(text, params.ranking_kw_groups[key])\n",
    "            text_list.append(sents)\n",
    "        \n",
    "        # fill in column content\n",
    "        for i in range(len(text_column)): # add key value pair of ranking_kw_groups and values in text_group\n",
    "            key = text_column[i]\n",
    "            text_value = text_list[i]\n",
    "            df_output.at[ind, key] = text_value\n",
    "        \n",
    "        # fill in columns to add\n",
    "        for i in range(len(columns_to_fill)): # add key value pair of ranking_kw_groups and values in text_group\n",
    "            key = columns_to_fill[i]\n",
    "            value = np.nan\n",
    "            df_output.at[ind, key] = value\n",
    "\n",
    "        # display the progress \n",
    "        print(\"\\n\")       \n",
    "        print(\"ind:\", ind, \"index:\", index)\n",
    "        \n",
    "    # read the csv file and reset index and add header\n",
    "    df_output = df_output[df_col.final_manually_read_df_columns]\n",
    "    df_output.reset_index(drop=True, inplace=True)\n",
    "    save_large_df(df_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main program:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Select colunmns from the db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the 120% of the articles above the threshold\n",
    "# df_db = pd.read_csv(fpath.poten_litera_db, header=0, sep='\\t')\n",
    "\n",
    "# output_path = fpath.final_confirm_article_list\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# output_df = pd.DataFrame(columns = df_col.db_columns)\n",
    "\n",
    "# # get the list of articles to manually read\n",
    "# with open(fpath.article_list_to_manually_read, 'r') as file:\n",
    "#     list_as_string = file.read()\n",
    "# l = list_as_string.strip().split(',')\n",
    "# article_list_to_manually_read = [int(i) for i in l]\n",
    "# print(len(article_list_to_manually_read))\n",
    "# print(article_list_to_manually_read)\n",
    "\n",
    "# # Iterate through the list and find matching rows\n",
    "# for index in article_list_to_manually_read:\n",
    "#     matching_row = df_db[df_db['INDEX'].astype(int) == index]\n",
    "#     output_df = pd.concat([output_df, matching_row], ignore_index=True)\n",
    "    \n",
    "# # List of columns to select\n",
    "# selected_columns = df_col.index + df_col.identifier + df_col.url + df_col.tak # Replace with your column names\n",
    "\n",
    "# # Selecting specified columns\n",
    "# selected_df = output_df[selected_columns]\n",
    "# selected_df.reset_index(inplace=True, drop=True)\n",
    "# selected_df.to_csv(output_path, header=True, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the relevant reviews and articles whose tak and full text are not available\n",
    "# df_db = pd.read_csv(fpath.poten_litera_db, header=0, sep='\\t')\n",
    "\n",
    "# output_path = fpath.relevant_reviews_and_tak_not_avaialble\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# output_df = pd.DataFrame(columns = df_col.db_columns)\n",
    "\n",
    "# # get the list of relevant review articles\n",
    "# with open(fpath.relevant_article_and_is_review, 'r') as file:\n",
    "#     list_as_string = file.read()\n",
    "# l = list_as_string.strip().split(',')\n",
    "# review_and_tak_not_avail = [int(i) for i in l]\n",
    "# print(len(review_and_tak_not_avail))\n",
    "# print(review_and_tak_not_avail)\n",
    "\n",
    "# # get the list of articles whose tak and full text are not available\n",
    "# df = pd.read_csv(fpath.poten_litera_tak_not_available, header=None, sep='\\t')\n",
    "# df.columns = df_col.db_columns\n",
    "# for ind in df.index:\n",
    "#     index = int(df.at[ind, \"INDEX\"])\n",
    "#     review_and_tak_not_avail.append(index)\n",
    "# print(len(review_and_tak_not_avail))\n",
    "# print(review_and_tak_not_avail)\n",
    "\n",
    "# # Iterate through the list and find matching rows\n",
    "# for index in review_and_tak_not_avail:\n",
    "#     matching_row = df_db[df_db['INDEX'].astype(int) == index]\n",
    "#     output_df = pd.concat([output_df, matching_row], ignore_index=True)\n",
    "    \n",
    "# # List of columns to select\n",
    "# selected_columns = df_col.index + df_col.identifier + df_col.url + df_col.tak # Replace with your column names\n",
    "\n",
    "# # Selecting specified columns\n",
    "# selected_df = output_df[selected_columns]\n",
    "# selected_df.reset_index(inplace=True, drop=True)\n",
    "# selected_df.to_csv(output_path, header=True, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # articels with abstract and full text not available\n",
    "# db_path = fpath.poten_litera_db\n",
    "# df_db = pd.read_csv(db_path, header=0, sep='\\t')\n",
    "\n",
    "# output_path = fpath.final_manually_read_csv_abstract_full_text_not_available\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# output_df = pd.DataFrame(columns = df_col.db_columns)\n",
    "\n",
    "# # get the list of articles to manually read\n",
    "# df = pd.read_csv(fpath.poten_litera_abstract_or_text_not_available, header=0, sep='\\t')\n",
    "# for ind in df.index:\n",
    "#     index = int(df.at[ind, \"INDEX\"])\n",
    "#     output_df = pd.concat([output_df, df_db[df_db['INDEX'].astype(int) == index]], ignore_index=True)\n",
    "    \n",
    "# # List of columns to select\n",
    "# selected_columns = df_col.index + df_col.identifier + df_col.url + df_col.tak # Replace with your column names\n",
    "\n",
    "# # Selecting specified columns\n",
    "# selected_df = output_df[selected_columns]\n",
    "# selected_df.reset_index(inplace=True, drop=True)\n",
    "# selected_df.to_csv(output_path, header=True, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract sentences\n",
    "# input_path = fpath.final_confirm_article_list\n",
    "# output_path = fpath.final_confirm_article_list\n",
    "\n",
    "# extract_sents_and_record(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract sentences\n",
    "# input_path = fpath.relevant_reviews_and_tak_not_avaialble\n",
    "# output_path = fpath.relevant_reviews_and_tak_not_avaialble\n",
    "\n",
    "# extract_sents_and_record(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(fpath.poten_litera_testing_set_1000_labeled_complete, header=0, sep='\\t')\n",
    "# df['REVIEW(Y/N)'] = np.nan\n",
    "\n",
    "# df.to_csv(fpath.poten_litera_testing_set_1000_labeled_complete, header=True, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_in_info(output_path, info_path):\n",
    "    df_info = pd.read_csv(info_path, header=0, sep='\\t')\n",
    "    df_output = pd.read_csv(output_path, header=0, sep='\\t')\n",
    "    \n",
    "    # Iterate over the index of df_test_1000\n",
    "    for ind in df_info.index:\n",
    "        index = int(df_info.at[ind, \"INDEX\"])\n",
    "            \n",
    "        # Iterate over the columns specified in df_col.columns_to_fill_0\n",
    "        for col in df_col.columns_to_fill:\n",
    "            # Update df_input where 'INDEX' matches\n",
    "            df_output.loc[df_output['INDEX'].astype(int) == index, col] = df_info.at[ind, col]\n",
    "    \n",
    "    df_output.columns = df_col.final_confirm_df_columns\n",
    "    df_output.to_csv(output_path, header=True, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fill in information in the test_1000 set to final manually read set\n",
    "# test_1000_path = fpath.poten_litera_testing_set_1000_labeled_complete\n",
    "\n",
    "# for i in range(1, 5):\n",
    "#     input_path = os.path.join(fpath.literature_datasets_folder, \"final_manually_read_csv\" + \"_\" + str(i) + \".csv\")\n",
    "#     # print(input_path)\n",
    "#     fill_in_info(input_path, test_1000_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fill in information in the labeled set to final manually read set\n",
    "# info_path = fpath.poten_litera_testing_set_1000_labeled_complete\n",
    "# input_path = fpath.final_confirm_article_list\n",
    "# fill_in_info(input_path, info_path)\n",
    "# input_path = fpath.relevant_reviews_and_tak_not_avaialble\n",
    "# fill_in_info(input_path, info_path)\n",
    "\n",
    "# info_path = fpath.final_manually_read_csv_1_labeled\n",
    "# input_path = fpath.final_confirm_article_list\n",
    "# fill_in_info(input_path, info_path)\n",
    "# input_path = fpath.relevant_reviews_and_tak_not_avaialble\n",
    "# fill_in_info(input_path, info_path)\n",
    "\n",
    "# info_path = fpath.final_manually_read_csv_2_labeled\n",
    "# input_path = fpath.final_confirm_article_list\n",
    "# fill_in_info(input_path, info_path)\n",
    "# input_path = fpath.relevant_reviews_and_tak_not_avaialble\n",
    "# fill_in_info(input_path, info_path)\n",
    "\n",
    "# info_path = fpath.final_manually_read_csv_3_labeled\n",
    "# input_path = fpath.final_confirm_article_list\n",
    "# fill_in_info(input_path, info_path)\n",
    "# input_path = fpath.relevant_reviews_and_tak_not_avaialble\n",
    "# fill_in_info(input_path, info_path)\n",
    "\n",
    "# info_path = fpath.final_manually_read_csv_4_labeled\n",
    "# input_path = fpath.final_confirm_article_list\n",
    "# fill_in_info(input_path, info_path)\n",
    "# input_path = fpath.relevant_reviews_and_tak_not_avaialble\n",
    "# fill_in_info(input_path, info_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_input_Y = 0\n",
    "# count_input_N = 0\n",
    "# count_input_MB = 0\n",
    "# count_input_NA = 0\n",
    "\n",
    "# count_verify_Y = 0\n",
    "\n",
    "# with open(fpath.article_list_to_manually_read, 'r') as file:\n",
    "#     l = file.read().strip().split(',')\n",
    "#     l = [int(i) for i in l]\n",
    "\n",
    "# for i in range(1, 5):\n",
    "#     input_path = os.path.join(fpath.literature_datasets_folder, \"final_manually_read_csv\" + \"_\" + str(i) + \".csv\")\n",
    "#     df_input = pd.read_csv(input_path, header=0, sep='\\t')\n",
    "    \n",
    "#     verify_path = fpath.poten_litera_testing_set_1000_labeled\n",
    "#     df_verify = pd.read_csv(verify_path, header=0, sep=',')\n",
    "    \n",
    "#     for ind in df_input.index:\n",
    "#         index = int(df_input.at[ind, \"INDEX\"])\n",
    "        \n",
    "#         if index not in l:\n",
    "#             raise Exception(\"index not in article_list_to_manually_read\")\n",
    "        \n",
    "#         if df_input.at[ind, 'RELEVANT?(Y/N/MB/NA)'] == 'Y':\n",
    "#             count_input_Y += 1  \n",
    "#         elif df_input.at[ind, 'RELEVANT?(Y/N/MB/NA)'] == 'N':\n",
    "#             count_input_N += 1   \n",
    "#         elif df_input.at[ind, 'RELEVANT?(Y/N/MB/NA)'] == 'MB':\n",
    "#             count_input_MB += 1\n",
    "#         elif df_input.at[ind, 'RELEVANT?(Y/N/MB/NA)'] != df_input.at[ind, 'RELEVANT?(Y/N/MB/NA)']:\n",
    "#             count_input_NA += 1\n",
    "#         else:\n",
    "#             raise Exception(\"RELEVANT?(Y/N/MB/NA) is not Y, N, MB, or NA\")\n",
    "            \n",
    "# for ind in df_verify.index:\n",
    "#     if df_verify.at[ind, 'RELEVANT?(Y/N/MB/NA)'] == 'Y':\n",
    "#         count_verify_Y += 1\n",
    "\n",
    "# print(count_input_Y)\n",
    "# print(count_input_N)\n",
    "# print(count_input_MB)\n",
    "# print(count_input_NA)\n",
    "# print(count_input_Y + count_input_N + count_input_MB + count_input_NA)\n",
    "\n",
    "# print(count_verify_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_input_Y = 0\n",
    "# count_input_N = 0\n",
    "# count_input_MB = 0\n",
    "# count_input_NA = 0\n",
    "\n",
    "# count_verify_Y = 0\n",
    "\n",
    "# with open(fpath.article_list_to_manually_read, 'r') as file:\n",
    "#     l = file.read().strip().split(',')\n",
    "#     l = [int(i) for i in l]\n",
    "\n",
    "# for i in range(1, 5):\n",
    "#     input_path = os.path.join(fpath.literature_datasets_folder, \"final_manually_read_csv\" + \"_\" + str(i) + \".csv\")\n",
    "#     df_input = pd.read_csv(input_path, header=0, sep='\\t')\n",
    "    \n",
    "#     verify_path = fpath.poten_litera_testing_set_1000_labeled\n",
    "#     df_verify = pd.read_csv(verify_path, header=0, sep=',')\n",
    "    \n",
    "#     for ind in df_input.index:\n",
    "#         index = int(df_input.at[ind, \"INDEX\"])\n",
    "        \n",
    "#         if index not in l:\n",
    "#             raise Exception(\"index not in article_list_to_manually_read\")\n",
    "        \n",
    "#         if df_input.at[ind, 'RELEVANT?(Y/N/MB/NA)'] == 'Y':\n",
    "#             count_input_Y += 1  \n",
    "#         elif df_input.at[ind, 'RELEVANT?(Y/N/MB/NA)'] == 'N':\n",
    "#             count_input_N += 1   \n",
    "#         elif df_input.at[ind, 'RELEVANT?(Y/N/MB/NA)'] == 'MB':\n",
    "#             count_input_MB += 1\n",
    "#         elif df_input.at[ind, 'RELEVANT?(Y/N/MB/NA)'] != df_input.at[ind, 'RELEVANT?(Y/N/MB/NA)']:\n",
    "#             count_input_NA += 1\n",
    "#         else:\n",
    "#             raise Exception(\"RELEVANT?(Y/N/MB/NA) is not Y, N, MB, or NA\")\n",
    "            \n",
    "# for ind in df_verify.index:\n",
    "#     if df_verify.at[ind, 'RELEVANT?(Y/N/MB/NA)'] == 'Y':\n",
    "#         count_verify_Y += 1\n",
    "\n",
    "# print(count_input_Y)\n",
    "# print(count_input_N)\n",
    "# print(count_input_MB)\n",
    "# print(count_input_NA)\n",
    "# print(count_input_Y + count_input_N + count_input_MB + count_input_NA)\n",
    "\n",
    "# print(count_verify_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
