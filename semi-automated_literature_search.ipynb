{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "864ee2e7",
   "metadata": {},
   "source": [
    "<h3> Description of the Notebook </h3> \n",
    "This is some text descriping the work done by this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92cba0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal .py modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37eb8cd1-594b-4af8-aff3-cc6a9c91b488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed2a1fb-4380-4179-bbdf-8192056b8b4c",
   "metadata": {},
   "source": [
    "<h3> Specifying the parameters </h3>\n",
    "In the next cell, we present all parameters that might have an effect on the search results, including:<br>\n",
    "1. searching keyword lexicon<br>\n",
    "2. academic databases<br>\n",
    "3. initial urls when searching academic databases<br>\n",
    "4. seed paper list for spanning citations<br>\n",
    "5. conenctome database<br>\n",
    "6. seaching queries of the connectome database<br>\n",
    "7. on-topic keyword lexicon<br>\n",
    "8. weights of on-topic keywords when calculating relatedness of a literature<br>\n",
    "9. ChatGPT queries for relatedness of topic<br>\n",
    "10. meta categories when extracting information of related literature<br>\n",
    "11. keywords for searching meta categories<br>\n",
    "12. ChatGPT queries for extracting information of meta categories of related literature<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b6b2882-23e5-4670-8359-3d7367e1107b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "# searching keywords lexicon\n",
    "search_kws_lexicon = \"macaque AND (thalamus OR thalamocortical OR thalamo-cortical)\" # in all fields\n",
    "\n",
    "# academic databases\n",
    "# Google Scholar: \"https://scholar.google.com/\"\n",
    "# macaque thalamus OR thalamocortical OR thalamo-cortical\n",
    "# 78100 results\n",
    "# Web of Science: \"https://www.webofscience.com/wos/woscc/advanced-search\" # can be exported to excel file\n",
    "# (ALL=(thalamus) OR ALL=(thalamocortical) OR ALL=(thalamo-cortical)) AND ALL=(macaque)\n",
    "# 880 results\n",
    "# PubMed Central PMC: \"https://pubmed.ncbi.nlm.nih.gov/advanced/\" # can be exported to .csv file\n",
    "# ((thalamus) OR (thalamocortical) OR (thalamo-cortical)) AND (macaque)\n",
    "# 2448 results\n",
    "# Europe PMC = \"https://europepmc.org/advancesearch\" # search resuts can be exported to .csv file\n",
    "# (\"macaque\") AND (\"thalamus\" OR \"thalamocortical\" OR \"thalamo-cortical\") AND (LANG:\"eng\" OR LANG:\"en\" OR LANG:\"us\")\n",
    "# 5130 results\n",
    "acad_dbs = [\"Google Scholar\", \"Web of Science\", \"PubMed Central PMC\", \"Europe PMC\"]\n",
    "\n",
    "# initial urls for specified searching keyword lexicon and all academic databases\n",
    "init_urls = {\n",
    "    \"gs\": \"https://scholar.google.com/scholar?start=0&q=macaque+thalamus+OR+thalamocortical+OR+thalamo-cortical&hl=en&as_sdt=1,5\",\n",
    "    \"wos\": \"https://www.webofscience.com/wos/woscc/summary/3a00a41f-3135-4142-a950-c8d6eb3b20a7-99be93b8/relevance/1\",\n",
    "    \"pmc\": \"https://pubmed.ncbi.nlm.nih.gov/?term=((thalamus)%20OR%20(thalamocortical)%20OR%20(thalamo-cortical))%20AND%20(macaque)&sort=relevance&page=1\",\n",
    "    \"eupmc\": \"https://europepmc.org/search?query=%28%22macaque%22%29%20AND%20%28%22thalamus%22%20OR%20%22thalamocortical%22%20OR%20%22thalamo-cortical%22%29%20AND%20%28LANG%3A%22eng%22%20OR%20LANG%3A%22en%22%20OR%20LANG%3A%22us%22%29&page=1\"\n",
    "}\n",
    "\n",
    "# seed literature list\n",
    "seed_litera_list = []\n",
    "\n",
    "# cocomac literature list\n",
    "cocomac_litera_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8ee718e-e776-44d4-ba81-7f24ef51c866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Scholar searched 78100 results displayed in 7810 pages.\n",
      "Searching Google Scholar complated!\n"
     ]
    }
   ],
   "source": [
    "def search_google_scholar(init_url):\n",
    "    # create a .txt file to record the urls of google scholar search results, clear the file if already exists\n",
    "    f = open(fpath.poten_litera_gs, \"w\")\n",
    "    f.truncate()\n",
    "    f.close()\n",
    "\n",
    "    # request the first page and extract the number of pages of the search results\n",
    "    first_page = init_url\n",
    "    response = requests.get(first_page, headers = plib.headers)\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "    # print(soup)\n",
    "    num_results_str_list = soup.select(\"div\", {\"class\": \"gs_ab_mdw\"})\n",
    "    for item in num_results_str_list:\n",
    "        if \"results\" in item.get_text():\n",
    "            num_results_str = item.get_text().split()\n",
    "    num_results_str = num_results_str[1]\n",
    "    # print(num_results_str)\n",
    "    # print(int(num_results_str))\n",
    "    num_results = int(re.sub(r\"[^\\w\\s]\", \"\", num_results_str))\n",
    "    pages = int(num_results/10)\n",
    "    print(\"Google Scholar searched \" + str(num_results) + \" results\" + \" displayed in \" + str(pages) + \" pages.\")\n",
    "    \n",
    "    # iterate all pages and record the results\n",
    "    # pages = 5\n",
    "    for page in range(pages):\n",
    "        time.sleep(random.randint(1, 5))\n",
    "        time.sleep(random.randint(5, 10))\n",
    "        start = page * 10\n",
    "        page_url = init_url.split(\"?start=\")[0] + \"?start=\" + str(start) + \"&q=\" + init_url.split(\"?start=\")[1].split(\"&q=\")[1]\n",
    "        # print(page_url)\n",
    "        # search a page\n",
    "        response = requests.get(page_url, headers = plib.headers)\n",
    "        soup = BeautifulSoup(response.content, \"lxml\")\n",
    "        # print(soup)\n",
    "        # print(soup.select(\"[data-lid]\")) \n",
    "        for item in soup.select(\"[data-lid]\"):\n",
    "            # print(item)\n",
    "            try:\n",
    "                add_title = item.select(\"h3\")[0].select(\"a\", href = True)[0].get_text()\n",
    "                # print(add_title)\n",
    "            except:\n",
    "                add_title = \"not found\"\n",
    "            try:\n",
    "                add_url = item.select(\"h3\")[0].select(\"a\", href = True)[0][\"href\"]\n",
    "                # print(add_url)\n",
    "            except:\n",
    "                add_url = \"not found\"\n",
    "            try:\n",
    "                add_full_text_link = item.find_all(\"div\", {'class': \"gs_or_ggsm\"})[0].find_all(\"a\", href = True)[0][\"href\"]\n",
    "                # print(\"1\")\n",
    "                # print(add_full_text_link)\n",
    "            except:\n",
    "                add_full_text_link = \"not found\"\n",
    "            # print(add_full_text_link)\n",
    "            row = {\n",
    "                \"title\": [add_title],\n",
    "                \"url\": [add_url],\n",
    "                \"full_text_url\": [add_full_text_link]\n",
    "            }\n",
    "            columns = [\"title\", \"url\", \"full_text_url\"]\n",
    "            plib.add_row_to_csv(fpath.poten_litera_gs, row, columns)\n",
    "    print(\"Searching Google Scholar complated!\")\n",
    "\n",
    "def search_webofscience(init_url, headers):\n",
    "    print(\"Searching Web of Science complated!\")\n",
    "\n",
    "def search_PubMed_Central_PMC(iinit_url, headers):\n",
    "    print(\"Searching PubMedd Central PMC complated!\")\n",
    "\n",
    "def search_Europe_PMC(init_url, headers):\n",
    "    print(\"Searching Europe PMC complated!\")\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# init_url = init_urls[\"gs\"]\n",
    "# headers = plib.headers\n",
    "# search_google_scholar(init_url)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f6b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_webofscience(init_url):\n",
    "    # search in the website and export the search results\n",
    "    print(\"Searching Web of Science complated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b492a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_pmc(init_url):\n",
    "    # search in the website and export the search results\n",
    "    print(\"Searching PubMedd Central PMC complated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab5590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_eupmc(init_url):\n",
    "    # search in the website and export the search results\n",
    "    print(\"Searching Europe PMC complated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ddc49e-446e-4c4b-a8a7-a79b1335039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search academic databases, record the urls as a line in a .txt file from the webpages\n",
    "def search_acad_dbs(acad_dbs, init_urls):\n",
    "    try:\n",
    "        for acad_db in acad_dbs:\n",
    "            if acad_db == \"Google Scholar\":\n",
    "                print(\"Searching Google Scholar...\")\n",
    "                search_google_scholar(init_urls[\"gs\"])\n",
    "            elif acad_db == \"Web of Science\":\n",
    "                print(\"Searching Web of Science...\")\n",
    "                search_webofscience(init_urls[\"wos\"])\n",
    "            elif acad_db == \"PubMed Central PMC\":\n",
    "                print(\"Searching PubMed Central PMC...\")\n",
    "                search_pmc(init_urls[\"pmc\"])\n",
    "            elif acad_db == \"Europe_PMC\":\n",
    "                print(\"Searching Europe PMC...\")\n",
    "                search_eupmc(init_urls[\"eupmc\"])\n",
    "            else:\n",
    "                print(\"Searching the specified academic database: \" + acad_db + \" is not supported by this function.\")\n",
    "                print(\"Plese choose one of the following databases:\",)\n",
    "                for db in [\"Google Scholar\", \"Web of Science\", \"PubMed Central PMC\", \"Europe PMC\"]:\n",
    "                    print(db)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def span_citations(seed_litera_list, num_span_time):\n",
    "    None\n",
    "    \n",
    "def search_conne_db():\n",
    "    None\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23fec62-fd3a-49c4-987c-143a20f524f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_search_results():\n",
    "    try:\n",
    "        # google scholar search results\n",
    "        # not processed for now\n",
    "\n",
    "        # process web of science search results\n",
    "\n",
    "\n",
    "        # process pmc search results\n",
    "\n",
    "\n",
    "        # process eupmc search results\n",
    "\n",
    "\n",
    "        # process seed literature citation spanning results\n",
    "        # not processed for now\n",
    "        # process connctome database literature results\n",
    "        # not processed for now\n",
    "        with open(fpath.gs_poten_urls, \"r\") as file:\n",
    "            lines = []\n",
    "            for line in file:\n",
    "                print(line)\n",
    "                line = line.strip()\n",
    "                lines.append(line)\n",
    "        print(len(lines))\n",
    "        doi_list = []\n",
    "        for url in lines:\n",
    "            response = requests.get(url, headers = plib.headers)\n",
    "            soup = BeautifulSoup(response.content,\"lxml\")\n",
    "            # print(soup)\n",
    "            num_results_str = soup.select(\"a\", href = True)\n",
    "            for href in num_results_str:\n",
    "                if \"//doi.org/\" in href:\n",
    "                    doi_list.append(href)\n",
    "        doi_df = pd.DataFrame({\"DOI\": doi_list})\n",
    "        plib.clear_file(fpath.path_poten_csv)\n",
    "        doi_df.to_csv(fpath.path_poten_csv, index=False)\n",
    "        \n",
    "        # process wos_poten_urls\n",
    "        doi_df = pd.read_csv(fpath.wos_poten_urls, sep=\";\")\n",
    "        # print(doi_df.columns)\n",
    "        # print(doi_df.head())\n",
    "        doi_df = doi_df[[\"DOI\"]]\n",
    "        doi_df.to_csv(fpath.path_poten_csv, mode=\"a\", index=False, header=False)\n",
    "        \n",
    "        # process pubmed_pmc_poten_urls\n",
    "        doi_df = pd.read_csv(fpath.pubmed_pmc_poten_urls)\n",
    "        doi_df = doi_df[[\"DOI\"]]\n",
    "        doi_df.to_csv(fpath.path_poten_csv, mode=\"a\", index=False, header=False)\n",
    "        \n",
    "        # process eupmc_poten_urls\n",
    "        doi_df = pd.read_csv(fpath.eupmc_poten_urls)\n",
    "        doi_df = doi_df[[\"DOI\"]]\n",
    "        doi_df.to_csv(fpath.path_poten_csv, mode=\"a\", index=False, header=False)\n",
    "        \n",
    "        # eliminate duplicates\n",
    "        doi_df = pd.read_csv(fpath.path_poten_csv)\n",
    "        print(len(doi_df))\n",
    "        doi_df = doi_df.drop_duplicates(subset = \"DOI\")\n",
    "        print(len(doi_df))\n",
    "        doi_df.to_csv(fpath.path_poten_csv, index=False)\n",
    "        # end of merge_search_results\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# test code\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc0eb4e",
   "metadata": {},
   "source": [
    "<h3> Main program </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df256b-106e-4d5d-847e-b068fbade613",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first we need to search all related literature that might include data or information of thalamocortical connections\n",
    "# search for potentially related literature using the listed 3 methods\n",
    "\n",
    "# method 1: search acdemic databases using keywords\n",
    "if search_acad_dbs(acad_dbs, init_urls):\n",
    "    print(\"Searching academic databases completed!\")\n",
    "else:\n",
    "    print(\"Attention! Something went wrong when searching academic databases completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a4879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # method 2: spanning citations of seed papers\n",
    "# if span_citations(seed_papers, num_span_time):\n",
    "#     print(\"Spanning citations of seed literature list completed!\")\n",
    "# else:\n",
    "#     print(\"Attention! Something went wrong when spanning citations of seed literature list!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c0ff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # method 3: search existing connectome databases\n",
    "# if search_conne_db(connec_db, connec_db_quries):\n",
    "#     print(\"Searching connectome databases completed!\")\n",
    "# else:\n",
    "#     print(\"Attention! Something went wrong when searching connectome databases!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8379912b-0688-4d9e-8641-50753f32a54e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge all results\n",
    "if merge_search_results():\n",
    "    print(\"Merging all results completed!\")\n",
    "else:\n",
    "    print(\"Attention! Something went wrong when merging all results completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bf1484-5217-4a14-9c48-ff176f46d57b",
   "metadata": {},
   "source": [
    "Now we have a list of actually related literature stored in list_of_related_literature.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f23abd-ef1d-44b0-8658-3ce0bc705bbd",
   "metadata": {},
   "source": [
    "Next step: we perform a information search on the list of related literature\n",
    "We have a list of actually related literature at the moment, now we need to extract information we need from the literature. We intend to achieve this with a combination of automated searching and manual extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00a5cd2-5c93-4fa2-ada2-138b94cee0a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# some test code, should comment-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4813f66-c983-4ecd-979a-3de2c97c5a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test the redirect of the urls\n",
    "# headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9\"}\n",
    "\n",
    "# response_pdf = requests.get(\"https://doi.org/10.1016/j.neuron.2020.01.005\", allow_redirects=True, headers = headers)\n",
    "# print(response_pdf.history)\n",
    "# print(response_pdf.url)\n",
    "# response_pdf_1 = requests.get(\"https://linkinghub.elsevier.com/retrieve/pii/S0896627320300052\", allow_redirects=True, headers = headers)\n",
    "# print(response_pdf_1.history)\n",
    "# print(response_pdf_1.url)\n",
    "\n",
    "\n",
    "# response_pdf = requests.get(\"https://onlinelibrary.wiley.com/doi/10.1111/ejn.13910\", headers = headers)\n",
    "# soup_pdf = BeautifulSoup(response_pdf.content,\"lxml\")\n",
    "# print(soup_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d39c9c-623c-4e2c-b0b4-a44d79b96f88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# if \"//doi.org/\" in \"https://doi.org/10.1016/0165-0173(96)00003-3\":\n",
    "#     print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40067374-c25b-40f8-ba36-8405a24e9a90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test extract doi from url\n",
    "with open(fpath.gs_poten_urls, \"r\") as file:\n",
    "    lines = []\n",
    "    for line in file:\n",
    "        print(line)\n",
    "        line = line.strip()\n",
    "        lines.append(line)\n",
    "print(len(lines))\n",
    "doi_list = []\n",
    "for url in lines:\n",
    "    response = requests.get(url, headers = plib.headers)\n",
    "    soup = BeautifulSoup(response.content,\"lxml\")\n",
    "    # print(soup)\n",
    "    num_results_str = soup.select(\"a\", href = True)\n",
    "    print(num_results_str)\n",
    "    for href in num_results_str:\n",
    "        if \"//doi.org/\" in href[\"href\"]:\n",
    "            doi_list.append(href[\"href\"])\n",
    "            print(href[\"href\"])\n",
    "        else:\n",
    "            print(\"Ops! Did't find DOI on this page!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f15ed5-64f4-489f-97a6-242bccfa7c69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find DOI\n",
    "# this link does not have \"DOI\" in href form but text from\n",
    "url = \"https://www.jneurosci.org/content/28/43/11042.short\"\n",
    "# url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2613515/\"\n",
    "response = requests.get(url, headers = plib.headers)\n",
    "soup = BeautifulSoup(response.content,\"lxml\")\n",
    "# print(soup)\n",
    "doi_list = []\n",
    "num_results_str = soup.select(\"a\", href = True)\n",
    "# print(num_results_str)\n",
    "for item in num_results_str:\n",
    "    if \"//doi.org/\" in item[\"href\"]:\n",
    "        print(item[\"href\"])\n",
    "        doi_list.append(item[\"href\"].split(\"//doi.org/\")[1])\n",
    "\n",
    "print(doi_list)\n",
    "        \n",
    "if len(doi_list) == 0:\n",
    "    print(\"Ops! Did't find DOI on this page!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79686627-676e-4eec-bad4-dd64cbffd90b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test redirect when access the doi link\n",
    "from elsapy.elsdoc import FullDoc, AbsDoc\n",
    "from elsapy.elsclient import ElsClient\n",
    "import json\n",
    "headers = {\n",
    "    \"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9\", \n",
    "    \"X-ELS-APIKEY\": \"310946e6e005957982c2c9cad6833ad3\",\n",
    "    \"Accept\": \"application/pdf\",\n",
    "    \"X-ELS-Insttoken\": \"instToken\",\n",
    "    \"view\": \"FULL\"\n",
    "} \n",
    "# url = \"https://www.jneurosci.org/content/28/43/11042.short\"\n",
    " #url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2613515/\"\n",
    "\n",
    "# Journal of Neurophysiology\n",
    "# url = \"https://doi.org/10.1152/jn.2001.85.1.219\"\n",
    "# url = \"https://journals.physiology.org/doi/10.1152/jn.2001.85.1.219\"\n",
    "\n",
    "# science direct\n",
    "# url = \"https://doi.org/10.1016/j.biopsych.2004.10.014\"\n",
    "# url = \"https://linkinghub.elsevier.com/retrieve/pii/S0006322304010947\"\n",
    "# url = \"https://www.sciencedirect.com/science/article/pii/S0006322304010947?via%3Dihub\"\n",
    "url = \"https://api.elsevier.com/content/article/doi/{10.1016/j.biopsych.2004.10.014}\"\n",
    "\n",
    "# response = requests.get(url, headers = headers)\n",
    "# soup = BeautifulSoup(response.content,\"lxml\")\n",
    "# print(soup)\n",
    "# print(response.history)\n",
    "# print(response.url)\n",
    "# # Load configuration\n",
    "# con_file = open(\"config.json\")\n",
    "# config = json.load(con_file)\n",
    "# con_file.close()\n",
    "\n",
    "# response = requests.get(url, headers = headers)\n",
    "# print(response)\n",
    "\n",
    "# ## Initialize client\n",
    "# client = ElsClient(config[\"apikey\"])\n",
    "\n",
    "# ## ScienceDirect (full-text) document example using DOI\n",
    "# doi_doc = FullDoc(doi = \"10.1016/j.biopsych.2004.10.014\")\n",
    "# print(doi_doc)\n",
    "# if doi_doc.read(client):\n",
    "#     print (\"doi_doc.title: \", doi_doc.title)\n",
    "#     doi_doc.write(\"doi_doc\")   \n",
    "# else:\n",
    "#     print (\"Read document failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd81a715-ee56-4f75-a193-08d92c9fcd48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test pdf to text\n",
    "from PyPDF2 import PdfReader\n",
    "  \n",
    "# creating a pdf reader object\n",
    "reader = PdfReader(\"/Users/didihou/Downloads/fncir-09-00079.pdf\")\n",
    "  \n",
    "# printing number of pages in pdf file\n",
    "print(len(reader.pages))\n",
    "  \n",
    "# getting a specific page from the pdf file\n",
    "page = reader.pages[0]\n",
    "  \n",
    "# extracting text from page\n",
    "text = \"\".join(page.extract_text().splitlines())\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efb22af-4181-4ec0-af99-34d50e8a0c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test request\n",
    "url = 'https://journals.physiology.org/doi/pdf/10.1152/jn.2001.85.1.219'\n",
    "\n",
    "response = requests.get(url, headers = plib.headers)\n",
    "soup = BeautifulSoup(response.content,\"lxml\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da49f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://journals.physiology.org/doi/epdf/10.1152/jn.2001.85.1.219'\n",
    "r = requests.get(url, headers = plib.headers) \n",
    "print(r.url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1145f43",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'full_text_url'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'full_text_url'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(fpath\u001b[39m.\u001b[39mpoten_litera_gs)\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(df[\u001b[39m\"\u001b[39;49m\u001b[39mfull_text_url\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'full_text_url'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(fpath.poten_litera_gs)\n",
    "print(df[\"full_text_url\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
