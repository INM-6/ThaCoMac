{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "864ee2e7",
   "metadata": {},
   "source": [
    "<h2> Semi-automatic literature search </h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b426c100",
   "metadata": {},
   "source": [
    "<h3> Notebook description: </h3>\n",
    "Some description text here to write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92cba0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal .py modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37eb8cd1-594b-4af8-aff3-cc6a9c91b488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "from numpy import NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed2a1fb-4380-4179-bbdf-8192056b8b4c",
   "metadata": {},
   "source": [
    "<h3> Parameters: </h3>\n",
    "In the next cell, we present all parameters that might have an effect on the search results, including:<br>\n",
    "1. searching keyword lexicon<br>\n",
    "2. academic databases<br>\n",
    "3. initial urls when searching academic databases<br>\n",
    "4. seed paper list for spanning citations<br>\n",
    "5. conenctome database<br>\n",
    "6. seaching queries of the connectome database<br>\n",
    "7. on-topic keyword lexicon<br>\n",
    "8. weights of on-topic keywords when calculating relatedness of a literature<br>\n",
    "9. ChatGPT queries for relatedness of topic<br>\n",
    "10. meta categories when extracting information of related literature<br>\n",
    "11. keywords for searching meta categories<br>\n",
    "12. ChatGPT queries for extracting information of meta categories of related literature<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b6b2882-23e5-4670-8359-3d7367e1107b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# searching keywords lexicon\n",
    "search_kws_lexicon = \"macaque AND (thalamus OR thalamocortical OR thalamo-cortical)\" # in all fields\n",
    "\n",
    "# academic databases\n",
    "# Google Scholar: \"https://scholar.google.com/\"\n",
    "# macaque thalamus OR thalamocortical OR thalamo-cortical\n",
    "# 78100 results\n",
    "# Web of Science: \"https://www.webofscience.com/wos/woscc/advanced-search\" # can be exported to excel file\n",
    "# (ALL=(thalamus) OR ALL=(thalamocortical) OR ALL=(thalamo-cortical)) AND ALL=(macaque)\n",
    "# 880 results\n",
    "# PubMed Central PMC: \"https://pubmed.ncbi.nlm.nih.gov/advanced/\" # can be exported to .csv file\n",
    "# ((thalamus) OR (thalamocortical) OR (thalamo-cortical)) AND (macaque)\n",
    "# 2448 results\n",
    "# Europe PMC = \"https://europepmc.org/advancesearch\" # search resuts can be exported to .csv file\n",
    "# (\"macaque\") AND (\"thalamus\" OR \"thalamocortical\" OR \"thalamo-cortical\") AND (LANG:\"eng\" OR LANG:\"en\" OR LANG:\"us\")\n",
    "# 5130 results\n",
    "acad_dbs = [\"Google Scholar\", \"Web of Science\", \"PubMed Central PMC\", \"Europe PMC\"]\n",
    "\n",
    "# initial urls for specified searching keyword lexicon and all academic databases\n",
    "init_urls = {\n",
    "    \"gs\": \"https://scholar.google.com/scholar?start=0&q=macaque+thalamus+OR+thalamocortical+OR+thalamo-cortical&hl=en&as_sdt=1,5\",\n",
    "    \"wos\": \"https://www.webofscience.com/wos/woscc/summary/3a00a41f-3135-4142-a950-c8d6eb3b20a7-99be93b8/relevance/1\",\n",
    "    \"pmc\": \"https://pubmed.ncbi.nlm.nih.gov/?term=((thalamus)%20OR%20(thalamocortical)%20OR%20(thalamo-cortical))%20AND%20(macaque)&sort=relevance&page=1\",\n",
    "    \"eupmc\": \"https://europepmc.org/search?query=%28%22macaque%22%29%20AND%20%28%22thalamus%22%20OR%20%22thalamocortical%22%20OR%20%22thalamo-cortical%22%29%20AND%20%28LANG%3A%22eng%22%20OR%20LANG%3A%22en%22%20OR%20LANG%3A%22us%22%29&page=1\"\n",
    "}\n",
    "\n",
    "# seed literature list\n",
    "seed_litera_list = []\n",
    "\n",
    "# cocomac literature list\n",
    "cocomac_litera_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116f15dc",
   "metadata": {},
   "source": [
    "<h3> Predefined fucntions: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ee718e-e776-44d4-ba81-7f24ef51c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search academic database google scholar given a initial url and return the search results\n",
    "# the given initial url is manually obtained by input search query in the google scholar\n",
    "# the search results include title, url, full_text_url\n",
    "def search_google_scholar(init_url):\n",
    "    # create a .txt file to record the urls of google scholar search results, clear the file if already exists\n",
    "    f = open(fpath.poten_litera_gs, \"w\")\n",
    "    f.truncate()\n",
    "    f.close()\n",
    "\n",
    "    # request the first page and extract the number of pages of the search results\n",
    "    first_page = init_url\n",
    "    # request the webpage unitl the status code is 200\n",
    "    response = requests.get(first_page, headers = plib.headers, proxies = plib.get_proxies())\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Error when request webpages!\")\n",
    "    # while(response.status_code != 200):\n",
    "    #     # sleep for 5 minutes\n",
    "    #     time.sleep(300)\n",
    "    #     response = requests.get(first_page, headers = plib.headers, proxies = plib.get_proxies())\n",
    "    # parse the webpage\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "    # print(soup)\n",
    "    num_results_str_list = soup.select(\"div\", {\"class\": \"gs_ab_mdw\"})\n",
    "    for item in num_results_str_list:\n",
    "        if \"results\" in item.get_text():\n",
    "            num_results_str = item.get_text().split()\n",
    "    num_results_str = num_results_str[1]\n",
    "    # print(num_results_str)\n",
    "    # print(int(num_results_str))\n",
    "    num_results = int(re.sub(r\"[^\\w\\s]\", \"\", num_results_str))\n",
    "    pages = int(num_results/10)\n",
    "    print(\"Google Scholar searched \" + str(num_results) + \" results\" + \" displayed in \" + str(pages) + \" pages.\")\n",
    "    \n",
    "    # iterate all pages and record the results\n",
    "    # pages = 5\n",
    "    for page in range(pages):\n",
    "        time.sleep(random.randint(1, 10))\n",
    "        time.sleep(random.randint(1*60, 10*60))\n",
    "        start = page * 10\n",
    "        page_url = init_url.split(\"?start=\")[0] + \"?start=\" + str(start) + \"&q=\" + init_url.split(\"?start=\")[1].split(\"&q=\")[1]\n",
    "        # print(page_url)\n",
    "        # search a page\n",
    "        response = requests.get(page_url, headers = plib.headers, proxies = plib.get_proxies())\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(\"Error when request webpages!\")\n",
    "        # while(response.status_code != 200):\n",
    "        #     # sleep for 5 minutes\n",
    "        #     time.sleep(300)\n",
    "        #     response = requests.get(page_url, headers = plib.headers, proxies = plib.get_proxies())\n",
    "        soup = BeautifulSoup(response.content, \"lxml\")\n",
    "        # print(soup)\n",
    "        # print(soup.select(\"[data-lid]\")) \n",
    "        for item in soup.select(\"[data-lid]\"):\n",
    "            # print(item)\n",
    "            try:\n",
    "                add_title = item.select(\"h3\")[0].select(\"a\", href = True)[0].get_text()\n",
    "                # print(add_title)\n",
    "            except:\n",
    "                add_title = \"not found\"\n",
    "            try:\n",
    "                add_url = item.select(\"h3\")[0].select(\"a\", href = True)[0][\"href\"]\n",
    "                # print(add_url)\n",
    "            except:\n",
    "                add_url = \"not found\"\n",
    "            try:\n",
    "                add_full_text_link = item.find_all(\"div\", {'class': \"gs_or_ggsm\"})[0].find_all(\"a\", href = True)[0][\"href\"]\n",
    "                # print(\"1\")\n",
    "                # print(add_full_text_link)\n",
    "            except:\n",
    "                add_full_text_link = \"not found\"\n",
    "            # print(add_full_text_link)\n",
    "            row = {\n",
    "                \"title\": [add_title],\n",
    "                \"url\": [add_url],\n",
    "                \"full_text_url\": [add_full_text_link]\n",
    "            }\n",
    "            columns = [\"title\", \"url\", \"full_text_url\"]\n",
    "            plib.add_row_to_csv(fpath.poten_litera_gs, row, columns)\n",
    "    print(\"Searching Google Scholar complated!\")\n",
    "# --------------------start of test code--------------------\n",
    "# init_url = init_urls[\"gs\"]\n",
    "# headers = plib.headers\n",
    "# search_google_scholar(init_url)\n",
    "# ---------------------end of test code---------------------\n",
    "# end of search_google_scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f6b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_webofscience(init_url):\n",
    "    # search in the website and export the search results\n",
    "    print(\"Searching Web of Science complated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b492a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_pmc(init_url):\n",
    "    # search in the website and export the search results\n",
    "    print(\"Searching PubMedd Central PMC complated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab5590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_eupmc(init_url):\n",
    "    # search in the website and export the search results\n",
    "    print(\"Searching Europe PMC complated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ddc49e-446e-4c4b-a8a7-a79b1335039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search academic databases, record the urls as a line in a .txt file from the webpages\n",
    "def search_acad_dbs(acad_dbs, init_urls):\n",
    "    try:\n",
    "        for acad_db in acad_dbs:\n",
    "            if acad_db == \"Google Scholar\":\n",
    "                print(\"Searching Google Scholar...\")\n",
    "                search_google_scholar(init_urls[\"gs\"])\n",
    "            elif acad_db == \"Web of Science\":\n",
    "                print(\"Searching Web of Science...\")\n",
    "                search_webofscience(init_urls[\"wos\"])\n",
    "            elif acad_db == \"PubMed Central PMC\":\n",
    "                print(\"Searching PubMed Central PMC...\")\n",
    "                search_pmc(init_urls[\"pmc\"])\n",
    "            elif acad_db == \"Europe PMC\":\n",
    "                print(\"Searching Europe PMC...\")\n",
    "                search_eupmc(init_urls[\"eupmc\"])\n",
    "            else:\n",
    "                print(\"Searching the specified academic database: \" + acad_db + \" is not supported by this function.\")\n",
    "                print(\"Plese choose one of the following databases:\",)\n",
    "                for db in [\"Google Scholar\", \"Web of Science\", \"PubMed Central PMC\", \"Europe PMC\"]:\n",
    "                    print(db)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "# --------------------start of test code--------------------\n",
    "# if search_acad_dbs(acad_dbs, init_urls):\n",
    "#     print(\"Searching academic databases completed!\")\n",
    "# else:\n",
    "#     print(\"Attention! Something went wrong when searching academic databases completed!\")\n",
    "# ---------------------end of test code---------------------\n",
    "# end of search_acad_dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ad09b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def span_citations(seed_litera_list, num_span_time):\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411867ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_conne_db():\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e23fec62-fd3a-49c4-987c-143a20f524f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10133512/\n"
     ]
    },
    {
     "ename": "ProxyError",
     "evalue": "HTTPSConnectionPool(host='www.ncbi.nlm.nih.gov', port=443): Max retries exceeded with url: /pmc/articles/PMC10133512/ (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 400 Bad Request')))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/urllib3/connectionpool.py:711\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[39mif\u001b[39;00m is_new_proxy_conn \u001b[39mand\u001b[39;00m http_tunnel_required:\n\u001b[0;32m--> 711\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/urllib3/connectionpool.py:1007\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._prepare_proxy\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     conn\u001b[39m.\u001b[39mtls_in_tls_required \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m-> 1007\u001b[0m conn\u001b[39m.\u001b[39;49mconnect()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/urllib3/connection.py:374\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39m# Calls self._set_hostport(), so self.host is\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[39m# self._tunnel_host below.\u001b[39;00m\n\u001b[0;32m--> 374\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tunnel()\n\u001b[1;32m    375\u001b[0m \u001b[39m# Mark this connection as not reusable\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/http/client.py:925\u001b[0m, in \u001b[0;36mHTTPConnection._tunnel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n\u001b[0;32m--> 925\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTunnel connection failed: \u001b[39m\u001b[39m{\u001b[39;00mcode\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mmessage\u001b[39m.\u001b[39mstrip()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    926\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: Tunnel connection failed: 400 Bad Request",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/requests/adapters.py:487\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 487\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    488\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    489\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    490\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    491\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    492\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    497\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    498\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    499\u001b[0m     )\n\u001b[1;32m    501\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/urllib3/connectionpool.py:798\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    796\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 798\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    799\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    800\u001b[0m )\n\u001b[1;32m    801\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.ncbi.nlm.nih.gov', port=443): Max retries exceeded with url: /pmc/articles/PMC10133512/ (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 400 Bad Request')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProxyError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 104\u001b[0m\n\u001b[1;32m     88\u001b[0m     df_eupmc\u001b[39m.\u001b[39mto_csv(merged_file_path, header \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, index \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     90\u001b[0m     \u001b[39m# process seed literature citation spanning results\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     \u001b[39m# not processed for now\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39m# end of merge_search_results\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39m# --------------------start of test code--------------------\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m merge_search_results(fpath\u001b[39m.\u001b[39;49mpoten_litera_csv)\n",
      "Cell \u001b[0;32mIn[11], line 70\u001b[0m, in \u001b[0;36mmerge_search_results\u001b[0;34m(merged_file_path)\u001b[0m\n\u001b[1;32m     68\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://www.ncbi.nlm.nih.gov/pmc/articles/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m pmcid \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m \u001b[39mprint\u001b[39m(url)\n\u001b[0;32m---> 70\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(url, headers \u001b[39m=\u001b[39;49m plib\u001b[39m.\u001b[39;49mheaders, proxies \u001b[39m=\u001b[39;49m plib\u001b[39m.\u001b[39;49mget_proxies())\n\u001b[1;32m     71\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mprint\u001b[39m(response\u001b[39m.\u001b[39mstatus_code)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/requests/adapters.py:514\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[39mraise\u001b[39;00m RetryError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    513\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _ProxyError):\n\u001b[0;32m--> 514\u001b[0m     \u001b[39mraise\u001b[39;00m ProxyError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    516\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    517\u001b[0m     \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    518\u001b[0m     \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n",
      "\u001b[0;31mProxyError\u001b[0m: HTTPSConnectionPool(host='www.ncbi.nlm.nih.gov', port=443): Max retries exceeded with url: /pmc/articles/PMC10133512/ (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 400 Bad Request')))"
     ]
    }
   ],
   "source": [
    "# merge all searched literature results\n",
    "def merge_search_results(merged_file_path):\n",
    "    # clear the file\n",
    "    f = open(merged_file_path, \"w\")\n",
    "    f.truncate()\n",
    "    f.close()\n",
    "\n",
    "    # DOI, PMID, PMCID, title\n",
    "    columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\"]\n",
    "\n",
    "    # google scholar search results\n",
    "    # df_gs = pd.read_csv(fpath.poten_litera_gs, sep = \",\")\n",
    "\n",
    "    # # process web of science search results\n",
    "    # df_wos = pd.read_csv(fpath.poten_litera_wos, sep = \";\")\n",
    "    # df_wos = df_wos[[\"DOI\", \"Pubmed Id\", \"Article Title\"]]\n",
    "    # df_wos.rename(columns={\"DOI\": \"DOI\", \"Pubmed Id\": \"PMID\", \"Article Title\": \"Title\"}, inplace = True)\n",
    "    # df_wos[\"PMID\"] = df_wos[\"PMID\"].fillna(0)\n",
    "    # df_wos[\"PMID\"] = df_wos[\"PMID\"].astype(int)\n",
    "    # df_wos[\"PMID\"] = df_wos[\"PMID\"].astype(str)\n",
    "    # # print(df_wos.head(5))\n",
    "    # # print(df_wos.dtypes)\n",
    "    # pmcid = []\n",
    "    # for ind in df_wos.index:\n",
    "    #     # print(df_wos[\"PMID\"][ind])\n",
    "    #     if df_wos[\"PMID\"][ind] != \"0\":\n",
    "    #         pmid = df_wos[\"PMID\"][ind]\n",
    "    #         # print(pmid)\n",
    "    #         df_wos[\"PMID\"][ind] = pmid\n",
    "    #         url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "    #         # print(url)\n",
    "    #         response = requests.get(url, headers = plib.headers, proxies = plib.get_proxies())\n",
    "    #         if response.status_code != 200:\n",
    "    #             raise Exception(\"Error when request webpages!\")\n",
    "    #         soup = BeautifulSoup(response.content, \"lxml\")\n",
    "    #         l = soup.find_all(\"a\", {\"data-ga-action\": \"PMCID\"})\n",
    "    #         if(len(l) != 0):\n",
    "    #             # print(l[0].get_text().strip())\n",
    "    #             pmcid.append(l[0].get_text().strip())\n",
    "    #         else:\n",
    "    #             pmcid.append(NaN)\n",
    "    #     else:\n",
    "    #         pmcid.append(NaN)\n",
    "    #     # print(df_wos[ind])\n",
    "    # df_wos[\"PMCID\"] = pmcid\n",
    "    # df_wos[\"PMCID\"].replace(\"0\", NaN)\n",
    "    # # print(df_wos.head(5))\n",
    "    # df_wos = df_wos[columns]\n",
    "    # df_wos.to_csv(merged_file_path, header = True, index = None)\n",
    "\n",
    "    # # process pmc search results\n",
    "    # df_pmc = pd.read_csv(fpath.poten_litera_pmc, sep=',')\n",
    "    # doi = df_pmc[[\"DOI\", \"PMID\", \"PMCID\", \"Title\"]]\n",
    "    # doi.to_csv(merged_file_path, mode = \"a\", header = None, index = None)\n",
    "\n",
    "    # process eupmc search results\n",
    "    df_eupmc = pd.read_csv(fpath.poten_litera_eupmc, sep = \",\")\n",
    "    df_eupmc = df_eupmc[[\"DOI\", \"PMCID\", \"TITLE\"]]\n",
    "    df_eupmc = df_eupmc.rename(columns={\"TITLE\": \"Title\"}, errors = \"raise\")\n",
    "    # print(df_eupmc.head(5))\n",
    "    pmid = []\n",
    "    for ind in df_eupmc.index:\n",
    "        # print(df_eupmc[\"PMCID\"][ind])\n",
    "        if df_eupmc[\"PMCID\"][ind] is not NaN:\n",
    "            pmcid = str(df_eupmc[\"PMCID\"][ind])\n",
    "            # print(pmcid)\n",
    "            df_eupmc[\"PMCID\"][ind] = pmcid\n",
    "            url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "            print(url)\n",
    "            response = requests.get(url, headers = plib.headers, proxies = plib.get_proxies())\n",
    "            if response.status_code != 200:\n",
    "                print(response.status_code)\n",
    "                raise Exception(\"Error when request webpages!\")\n",
    "            soup = BeautifulSoup(response.content, \"lxml\")\n",
    "            l = soup.find_all(\"div\", {\"class\": \"fm-citation-pmid\"})\n",
    "            if (len(l)) != 0:\n",
    "                ll = l[0].find_all(\"a\", href = True)\n",
    "                if(len(ll) != 0):\n",
    "                    # print(ll[0].get_text().strip())\n",
    "                    pmid.append(ll[0].get_text().strip())\n",
    "            else:\n",
    "                pmid.append(NaN)\n",
    "        else:\n",
    "            pmid.append(NaN)\n",
    "        # print(df_wos[ind])\n",
    "    df_eupmc[\"PMID\"] = pmid\n",
    "    df_eupmc = df_eupmc[columns]\n",
    "    df_eupmc.to_csv(merged_file_path, header = None, index = None)\n",
    "\n",
    "    # process seed literature citation spanning results\n",
    "    # not processed for now\n",
    "\n",
    "    # process connctome database literature results\n",
    "    # not processed for now\n",
    "    \n",
    "    # eliminate duplicates\n",
    "    # df_merged = pd.read_csv(merged_file_path)\n",
    "    # print(len(df_merged))\n",
    "    # doi_df = df_merged.drop_duplicates(subset = \"DOI\")\n",
    "    # print(len(df_merged))\n",
    "    # df_merged.to_csv(merged_file_path, index=False)\n",
    "# end of merge_search_results\n",
    "# --------------------start of test code--------------------\n",
    "merge_search_results(fpath.poten_litera_csv)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc0eb4e",
   "metadata": {},
   "source": [
    "<h3> Main program: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df256b-106e-4d5d-847e-b068fbade613",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first we need to search all related literature that might include data or information of thalamocortical connections\n",
    "# search for potentially related literature using the listed 3 methods\n",
    "\n",
    "# method 1: search acdemic databases using keywords\n",
    "# if search_acad_dbs(acad_dbs, init_urls):\n",
    "#     print(\"Searching academic databases completed!\")\n",
    "# else:\n",
    "#     print(\"Attention! Something went wrong when searching academic databases completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a4879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # method 2: spanning citations of seed papers\n",
    "# if span_citations(seed_papers, num_span_time):\n",
    "#     print(\"Spanning citations of seed literature list completed!\")\n",
    "# else:\n",
    "#     print(\"Attention! Something went wrong when spanning citations of seed literature list!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c0ff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # method 3: search existing connectome databases\n",
    "# if search_conne_db(connec_db, connec_db_quries):\n",
    "#     print(\"Searching connectome databases completed!\")\n",
    "# else:\n",
    "#     print(\"Attention! Something went wrong when searching connectome databases!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8379912b-0688-4d9e-8641-50753f32a54e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # merge all results\n",
    "# if merge_search_results():\n",
    "#     print(\"Merging all results completed!\")\n",
    "# else:\n",
    "#     print(\"Attention! Something went wrong when merging all results completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bf1484-5217-4a14-9c48-ff176f46d57b",
   "metadata": {},
   "source": [
    "Next step: automatic filtering the potential related literature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61c1533",
   "metadata": {},
   "source": [
    "<h3> Some test code, please ignore: </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cb18dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test request\n",
    "# url = 'https://journals.physiology.org/doi/pdf/10.1152/jn.2001.85.1.219'\n",
    "\n",
    "# response = requests.get(url, headers = plib.headers)\n",
    "# soup = BeautifulSoup(response.content,\"lxml\")\n",
    "# print(soup)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
