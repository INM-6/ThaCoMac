{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to search all related literature that might include data or information of thalamocortical connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import pandas as pd\n",
    "from urllib.request import Request, urlopen\n",
    "import urllib.request\n",
    "import re\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import time\n",
    "import os.path\n",
    "import urllib.request\n",
    "import csv\n",
    "# from scholarly import scholarly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we present all parameters that might have an effect on the search results, including:<br>\n",
    "1. searching keyword lexicon\n",
    "2. on-topic keyword lexicon\n",
    "3. academic databases\n",
    "4. seed papers\n",
    "5. connectome database and \n",
    "6. connectome database queries\n",
    "7. ChatPDF queries for relatedness of topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "# searching keyword lexicon\n",
    "# (macaque OR Macaque) AND (thalamocortical OR thalamocortical OR corticothalamic OR 'cortico-thalamic' OR thalamus OR cortex)\n",
    "search_kws_lexicon = 'macaque AND (thalamus OR cortex OR thalamocortical OR thalamo-cortical OR corticothalamic OR cortico-thalamic)'\n",
    "\n",
    "# on-topic keyword lexicon\n",
    "on_topic_kws = ['thalamocortical', 'thalamo-cortical', 'corticothalamic', 'cortico-thalamic',\n",
    "                'tracing', 'tracer', 'tract tracing', 'tract-tracing', 'axonal tracing', 'neural anatomical tracing', \n",
    "                'connection', 'projection', 'connectivity', 'connectome', \n",
    "                'thalamus', 'cortex']\n",
    "\n",
    "# academic databases\n",
    "Semantic_Scholar = 'https://www.semanticscholar.org/'\n",
    "Google_Scholar = 'https://scholar.google.com/'\n",
    "# arXiv = 'https://arxiv.org/'\n",
    "Web_of_Science = 'https://www.webofscience.com/wos/woscc/basic-search' # can be expoted to excel file\n",
    "PubMed_Central_PMC = 'https://www.ncbi.nlm.nih.gov/pmc/' # can be exported to .csv file and abstract.txt file\n",
    "Europe_PMC = 'https://europepmc.org/' # can be exported to .csv file or abstract and full open access file .xml\n",
    "acad_dbs = [Semantic_Scholar, Google_Scholar, Web_of_Science, PubMed_Central_PMC, Europe_PMC]\n",
    "\n",
    "# seed papers specification\n",
    "seed_papers = []\n",
    "\n",
    "# connectome database and queries specification\n",
    "# we search the CoCoMac\n",
    "connec_db = 'http://cocomac.g-node.org/sitemap/index.php?path=CoCoMac|Search'\n",
    "connec_db_quries = []\n",
    "\n",
    "# ChatPDF, queries for relatedness of topic\n",
    "ChatPDF_related_queries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the file paths and request headers\n",
    "path_urls = '/Users/didihou/myProjects/didihou_master_project/list_of_literature_urls.txt'\n",
    "path_potential = '/Users/didihou/myProjects/didihou_master_project/list_of_potential_related_literature.csv'\n",
    "path_related_urls = '/Users/didihou/myProjects/didihou_master_project/list_of_related_literature.txt'\n",
    "pdf_folder_path = '/Users/didihou/myProjects/liter_pdfs'\n",
    "\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from serpapi import GoogleSearch\n",
    "params = {\n",
    "  \"engine\": \"google_scholar\",\n",
    "  \"q\": \"macaque thalamus OR thalamocortical OR thalamo-cortical\",\n",
    "  \"api_key\": \"c397fc278e5a3483e64735af275a21bf6f78b17a8084ea7f507d933bb4d4b90a\"\n",
    "}\n",
    "\n",
    "search = GoogleSearch(params)\n",
    "results = search.get_dict()\n",
    "organic_results = results[\"organic_results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "organic_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1: searching academic databases using searching keyword lexicon\n",
    "def search_acad_db(keywords, acad_db):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2: spanning citations of seed papers\n",
    "def expand_citation(seed_papers):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 3: querying the existing connectome database \n",
    "def search_conne_db(connec_db, connec_db_quries):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# searching for related literature and write them into file 'list_of_potential_related_literature.txt'\n",
    "def record_liter(doi_or_url, path):\n",
    "    with open(path, 'a+') as url_file:\n",
    "        url_file.seek(0)\n",
    "        # If file is not empty then append '\\n'\n",
    "        data = url_file.read(100)\n",
    "        if len(data) > 0 :\n",
    "            url_file.write('\\n')\n",
    "        # Append text at the end of file\n",
    "        url_file.write(doi_or_url)\n",
    "        \n",
    "'''\n",
    "# code for testing function: record_liter(doi_or_url, path)\n",
    "doi_or_url = 'http://cocomac.g-node.org/main/index.php'\n",
    "for i in range(10):\n",
    "    record_liter(doi_or_url)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read line by line the urls of related literature in the tex file and access the content\n",
    "# seach for on-topic keywords and record the number of times they appear and record to a .csv file\n",
    "def scan_poten_urls(path_urls, on_topic_kws):\n",
    "    with open(path_urls, 'r') as url_file:\n",
    "        for line in url_file:\n",
    "            # Process each line here\n",
    "            for i in range(10):\n",
    "                response = requests.get(line)\n",
    "                if response.status_code == 200:\n",
    "                    content = response.text\n",
    "                    # process the content of the webpage here\n",
    "                    print(content)\n",
    "                    # code\n",
    "                    content = response.text\n",
    "            if(i == 9):\n",
    "                print(\"Error accessing the webpage. Status code:\", response.status_code)\n",
    "                 \n",
    "\n",
    "# code for testing function: scan_poten_urls(path_urls, on_topic_kws)\n",
    "line = 'https://onlinelibrary.wiley.com/doi/epdf/10.1002/cne.902570211'\n",
    "for i in range(10):\n",
    "    req = Request(\n",
    "    url=line, \n",
    "    headers={'User-Agent':'Mozilla/5.0'})\n",
    "    webpage = urlopen(req).read()\n",
    "    response = requests.get(line)\n",
    "    if response.status_code == 200:\n",
    "        content = response.text\n",
    "        # process the content of the webpage here\n",
    "        break\n",
    "if(i == 9):\n",
    "    print(\"Error accessing the webpage. Status code:\", response.status_code)\n",
    "else:\n",
    "    print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# web scraping\n",
    "# URL from which pdfs to be downloaded\n",
    "url = \"https://medium.com/analytics-vidhya/webscraping-a-site-with-pagination-using-beautifulsoup-fa0a09804445\"\n",
    "\n",
    "# Requests URL and get response object\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse text obtained\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all hyperlinks present on webpage\n",
    "links = soup.find_all('a')\n",
    "\n",
    "i = 0\n",
    "\n",
    "# From all links check for pdf link and\n",
    "# if present download file\n",
    "for link in links:\n",
    "    print(link)\n",
    "    if ('.pdf' in link.get('href', []) or 'pdf.' in link.get('href', [])):\n",
    "        i += 1\n",
    "        print(\"Downloading file: \", i)\n",
    "\n",
    "        # Get response object for link\n",
    "        response = requests.get(link.get('href'))\n",
    "\n",
    "        # Write content in pdf file\n",
    "        pdf = open(\"pdf\"+str(i)+\".pdf\", 'wb')\n",
    "        pdf.write(response.content)\n",
    "        pdf.close()\n",
    "        print(\"File \", i, \" downloaded\")\n",
    "\n",
    "print(\"All PDF files downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def search_acad_dbs(acad_db_name, path_urls):\n",
    "    if acad_db_name == 'Google Scholar':\n",
    "        url = 'https://scholar.google.com/scholar?start=0&q=macaque+thalamus+OR+thalamocortical+OR+thalamo-cortical&hl=en&as_sdt=1,5'\n",
    "        response = requests.get(url, headers = headers)\n",
    "        soup = BeautifulSoup(response.content,'lxml')\n",
    "        num_results_str = soup.find_all('div', {'class': 'gs_ab_mdw'})[1].get_text().split()[1]\n",
    "        # print(int(num_results_str))\n",
    "        num_results = int(re.sub(r'[^\\w\\s]', '', num_results_str))\n",
    "        pages = int(num_results/10)\n",
    "        pages = 10\n",
    "        # print(pages)\n",
    "        # search all pages\n",
    "        for page in range(pages):\n",
    "            time.sleep(2)\n",
    "            start = page * 10\n",
    "            # google scholar\n",
    "            page_url = 'https://scholar.google.com/scholar?start=' + str(start) + '&q=macaque+thalamus+OR+thalamocortical+OR+thalamo-cortical&hl=en&as_sdt=1,5'\n",
    "            # search a page\n",
    "            response = requests.get(page_url, headers = headers)\n",
    "            # print(url)\n",
    "            soup = BeautifulSoup(response.content,'lxml') \n",
    "            # print(soup.select('[data-lid]')) \n",
    "            for item in soup.select('[data-lid]'): \n",
    "                try: \n",
    "                    with open(path_urls, 'a+') as url_file:\n",
    "                        # append text at the end of file\n",
    "                        url_file.write(item.select('h3')[0].find_all('a', href=True)[0]['href'])\n",
    "                        url_file.write('\\n')\n",
    "                except Exception as e: \n",
    "                    #raise e\n",
    "                    print(\"error\")\n",
    "    elif acad_db_name == 'PubMed':\n",
    "        url = 'https://pubmed.ncbi.nlm.nih.gov/?term=macaque%20AND%20(thalamus%20OR%20cortex%20OR%20thalamocortical%20OR%20thalamo-cortical%20or%20corticothalamic%20OR%20cortico-thalamic)&page=1'\n",
    "        response = requests.get(url, headers = headers)\n",
    "        soup = BeautifulSoup(response.content,'lxml')\n",
    "        # print(soup)\n",
    "        num_results_str = soup.find_all('span', {'class': 'value'})[0].get_text()\n",
    "        print(num_results_str)\n",
    "        num_results = int(re.sub(r'[^\\w\\s]', '', num_results_str))\n",
    "        pages = int(num_results/10)\n",
    "        print(pages)\n",
    "    else:\n",
    "        None\n",
    "    \n",
    "    '''\n",
    "    for page in range(pages):\n",
    "        time.sleep(2)\n",
    "        start = page * 10\n",
    "        # google scholar\n",
    "        url = 'https://scholar.google.com/scholar?start=' + str(start) + '&q=macaque+thalamus+OR+thalamocortical+OR+thalamo-cortical&hl=en&as_sdt=1,5'\n",
    "        # pubmed\n",
    "        url = 'https://pubmed.ncbi.nlm.nih.gov/?term=macaque%20AND%20(thalamus%20OR%20cortex%20OR%20thalamocortical%20OR%20thalamo-cortical%20or%20corticothalamic%20OR%20cortico-thalamic)&page=1\n",
    "        response = requests.get(url,headers = headers)\n",
    "        # print(url)\n",
    "        soup = BeautifulSoup(response.content,'lxml') \n",
    "        #print(soup.select('[data-lid]')) \n",
    "        for item in soup.select('[data-lid]'): \n",
    "            try: \n",
    "                # print('----------------------------------------') \n",
    "                # print(item)  \n",
    "                # print(item.select('h3')[0])\n",
    "                with open(path_urls, 'a+') as url_file:\n",
    "                    url_file.seek(0)\n",
    "                    # If file is not empty then append '\\n'\n",
    "                    data = url_file.read(100)\n",
    "                    if len(data) > 0 :\n",
    "                        url_file.write('\\n')\n",
    "                        # Append text at the end of file\n",
    "                    url_file.write('----------------------------------------\\n')\n",
    "                    url_file.write(item.select('h3')[0].get_text())\n",
    "                    url_file.write('\\n')\n",
    "                    # print(item.select('h3')[0].get_text())\n",
    "                    for a in item.select('h3')[0].find_all('a', href=True):\n",
    "                        # print(a['href'])\n",
    "                        url_file.write(a['href'])\n",
    "                        url_file.write('\\n')\n",
    "                        # print(item.select('a'))\n",
    "                        # print(\"PDF link:\")\n",
    "                    url_file.write(item.select('a')[0]['href'])\n",
    "                    url_file.write('\\n')\n",
    "                    # print(item.select('a')[0]['href'])\n",
    "                    # print(item.select('.gs_rs')[0].get_text()) \n",
    "                    # print('----------------------------------------') \n",
    "            except Exception as e: \n",
    "                #raise e \n",
    "                print('')\n",
    "    '''\n",
    "    \n",
    "# test code\n",
    "acad_db_name = 'Google Scholar'\n",
    "search_acad_dbs(acad_db_name, path_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# test code\\ntext = 'This apple 6i7s very tasty？、  2but th&e banana is not delicious at all.6'\\nkeyword = 'is'\\ncount = count_keyword(text, keyword)\\nprint(count)\\n\""
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of times that certain on-topic keyword appear in a given text\n",
    "def count_keyword(text: str, keyword: str) -> int:\n",
    "    # print(text)\n",
    "    word_count = 0\n",
    "    for word in text.strip().split(\" \"):\n",
    "        # print(word)\n",
    "        if word == keyword:\n",
    "            word_count += 1\n",
    "    # print(f\"I found {word_count} words\")\n",
    "    return word_count\n",
    "\n",
    "'''\n",
    "# test code\n",
    "text = 'This apple 6i7s very tasty？、  2but th&e banana is not delicious at all.6'\n",
    "keyword = 'is'\n",
    "count = count_keyword(text, keyword)\n",
    "print(count)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download pdf given pdf_url\n",
    "def download_pdf_file(pdf_url: str, pdf_folder_path: str, file_name: str) -> bool:\n",
    "    # param url: The url of the PDF file to be downloaded\n",
    "    # return: True if PDF file was successfully downloaded, otherwise False.\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9'} \n",
    "    response = requests.get(pdf_url, stream=True, headers = headers)\n",
    "    \n",
    "    # download the .pdf file to the pdf_file_path folder\n",
    "    # write content in pdf file\n",
    "    pdf_path = os.path.join(pdf_folder_path, file_name)\n",
    "    if response.status_code == 200:\n",
    "        # save in current working directory\n",
    "        with open(pdf_path, 'wb') as pdf_object:\n",
    "            pdf_object.write(response.content)\n",
    "            print(f'{file_name} was successfully saved!')\n",
    "            return True\n",
    "    else:\n",
    "        print(f'Uh oh! Could not download {file_name},')\n",
    "        print(f'HTTP response status code: {response.status_code}')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# code for testing function: ChatPDF_relatedness(path_urls, ChatPDF_related_queries)\\n\\n'"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read line by line the urls of related literature in the tex file and access the content\n",
    "# seach for on-topic keywords and record the number of times they appear and record to a .csv file\n",
    "def ChatPDF_relatedness(path_urls, ChatPDF_related_queries):\n",
    "    with open(path_urls, 'r') as url_file:\n",
    "        for line in url_file:\n",
    "            a = 1\n",
    "            # code\n",
    "            \n",
    "'''\n",
    "# code for testing function: ChatPDF_relatedness(path_urls, ChatPDF_related_queries)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# test\\nurl = 'https://pubmed.ncbi.nlm.nih.gov/32053769/'\\npdf_folder_path = '/Users/didihou/myProjects/liter_pdfs'\\ninfo_json_ele = count_freq_from_liter(url, on_topic_kws)\\nfile_name = 'test.pdf'\\ndownload_pdf_file(info_json_ele['pdf_link'], pdf_folder_path, file_name)\\nprint(info_json_ele)\\n\""
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract text of title, abstract, keywords, introduction from given url and download PDF of the paper\n",
    "# return a json object consisting: DOI, url, keyword:frequency pair, relatedness of answers from ChatGPT\n",
    "\n",
    "def count_freq_from_liter(url, on_topic_kws):\n",
    "    print(url)\n",
    "    # access the url by web scraping\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9'} \n",
    "    response = requests.get(url, headers = headers)\n",
    "    soup = BeautifulSoup(response.content,'lxml')\n",
    "    print(len(soup))\n",
    "    \n",
    "    # extract DOI\n",
    "    # print(soup.find_all(\"a\", {'class': 'id-link'}, href = True)[1]['href'])\n",
    "    doi = soup.find_all(\"a\", {'class': 'id-link'}, href = True)[1]['href']\n",
    "    \n",
    "    # extract title\n",
    "    title = soup.select('h1')[0].get_text().strip()\n",
    "    title = re.sub(' +', ' ', title).capitalize()\n",
    "    \n",
    "    '''\n",
    "    # extract PDF link if exists\n",
    "    print(doi)\n",
    "    response_pdf = requests.get(doi, headers = headers)\n",
    "    print(response_pdf.url)\n",
    "    pdf_page_link = response_pdf.url\n",
    "        \n",
    "    # pdf_page = soup.find_all(\"a\", {'class':'link-item dialog-focus'}, href = True)[0]['href']\n",
    "    \n",
    "    # print(pdf_page_link)\n",
    "    pdf_page = requests.get(pdf_page_link, headers = headers)\n",
    "    soup_pdf = BeautifulSoup(pdf_page.content,'lxml')\n",
    "    print(len(soup_pdf.find_all(\"a\", href = True)))\n",
    "    pdf_link = soup_pdf.find_all(\"a\", href = True)[0]['href']\n",
    "    \n",
    "    \n",
    "    # print(pdf_link)\n",
    "    pdf_link = 'https://www.ncbi.nlm.nih.gov' + pdf_link\n",
    "    '''\n",
    "    # extract title, abstract, keywords, introduction from the returned html file\n",
    "    # count keywords from abstract + keywords\n",
    "    abs_kws = soup.find_all(\"div\", {'class': 'abstract'})[0].get_text()\n",
    "    abs_kws = abs_kws.strip()\n",
    "    abs_kws = re.sub(' +', ' ', abs_kws)\n",
    "    text = title + ' ' + abs_kws\n",
    "    text = re.sub(r\"[^a-zA-Z' ']\",\"\",text).lower()\n",
    "    \n",
    "    # record the information into json\n",
    "    info_json = {}\n",
    "    info_json['DOI'] = doi,\n",
    "    info_json['url'] = url,\n",
    "    info_json['title'] = title\n",
    "    # info_json['pdf_link'] = pdf_link\n",
    "    # count the on-topic keywords or calculate the frequency\n",
    "    for i in range(len(on_topic_kws)):\n",
    "        word_count = count_keyword(text, on_topic_kws[i])\n",
    "        info_json[on_topic_kws[i]] = word_count\n",
    "    \n",
    "    return info_json\n",
    "  \n",
    "'''\n",
    "# test\n",
    "url = 'https://pubmed.ncbi.nlm.nih.gov/32053769/'\n",
    "pdf_folder_path = '/Users/didihou/myProjects/liter_pdfs'\n",
    "info_json_ele = count_freq_from_liter(url, on_topic_kws)\n",
    "file_name = 'test.pdf'\n",
    "download_pdf_file(info_json_ele['pdf_link'], pdf_folder_path, file_name)\n",
    "print(info_json_ele)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nresponse_pdf = requests.get('https://doi.org/10.1016/j.neuron.2020.01.005', allow_redirects=True, headers = headers)\\nprint(response_pdf.history)\\nprint(response_pdf.url)\\nresponse_pdf_1 = requests.get('https://linkinghub.elsevier.com/retrieve/pii/S0896627320300052', allow_redirects=True, headers = headers)\\nprint(response_pdf_1.history)\\nprint(response_pdf_1.url)\\n\\n\\nresponse_pdf = requests.get('https://onlinelibrary.wiley.com/doi/10.1111/ejn.13910', headers = headers)\\nsoup_pdf = BeautifulSoup(response_pdf.content,'lxml')\\nprint(soup_pdf)\\n\""
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the redirect of the urls\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9'} \n",
    "'''\n",
    "response_pdf = requests.get('https://doi.org/10.1016/j.neuron.2020.01.005', allow_redirects=True, headers = headers)\n",
    "print(response_pdf.history)\n",
    "print(response_pdf.url)\n",
    "response_pdf_1 = requests.get('https://linkinghub.elsevier.com/retrieve/pii/S0896627320300052', allow_redirects=True, headers = headers)\n",
    "print(response_pdf_1.history)\n",
    "print(response_pdf_1.url)\n",
    "\n",
    "\n",
    "response_pdf = requests.get('https://onlinelibrary.wiley.com/doi/10.1111/ejn.13910', headers = headers)\n",
    "soup_pdf = BeautifulSoup(response_pdf.content,'lxml')\n",
    "print(soup_pdf)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatic filtering\n",
    "def auto_filter(df_potencial_liter):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual filtering\n",
    "# manual filtering is performed by reading the tilte, abstract, keywords, introduction section of each publication \n",
    "# in the order of sorted list by the possiblity of relatedness\n",
    "def manual_filter(path_potential):\n",
    "    # read and munual confirm the related literature\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add the information to list_of_potential_related_literature.csv\n",
    "def add_rows_to_csv(path_potential, info_json, columns):\n",
    "    new_row = info_json\n",
    "    df_new_row = pd.DataFrame(new_row, columns = columns)\n",
    "    \n",
    "    with open(path_potential, 'r') as csvfile:\n",
    "        csv_dict = [row for row in csv.DictReader(csvfile)]\n",
    "        if len(csv_dict) == 0:\n",
    "            df_new_row.to_csv(path_potential, index = False, header = True)\n",
    "        else:\n",
    "            df_new_row.to_csv(path_potential, mode = 'a', index = False, header = False)\n",
    "    # df_new_row.to_csv(path_potential, index = False)\n",
    "    # df_liter = df_liter.append(new_row, ignore_index=False)\n",
    "    # print(df_liter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scan url in list_of_literature_urls.txt and record information and download pdf\n",
    "def scan_record_download(path_urls, on_topic_kws, pdf_folder_path, columns):\n",
    "    file_index = 0\n",
    "    with open(path_urls, 'r') as url_file:\n",
    "        for url in url_file:\n",
    "            # print(url)\n",
    "            info_json = {}\n",
    "            info_json = count_freq_from_liter(url.strip(), on_topic_kws)\n",
    "            add_rows_to_csv(path_potential, info_json, columns)\n",
    "            # download_pdf_file(info_json['pdf_link'], pdf_folder_path, str(file_index))\n",
    "            file_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pubmed.ncbi.nlm.nih.gov/32053769/\n",
      "2\n",
      "https://pubmed.ncbi.nlm.nih.gov/34524542/\n",
      "2\n",
      "https://pubmed.ncbi.nlm.nih.gov/35851953/\n",
      "2\n",
      "https://pubmed.ncbi.nlm.nih.gov/29542210/\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# method 2\\nliter_list_2 = []\\n# liter_list_2 = expand_citation(seed_papers)\\nfor doi_or_url in liter_list_2:\\n    record_liter(doi_or_url, path_urls)\\n\\n# method 3\\nliter_list_3 = []\\n# liter_list_3 = search_conne_db(connec_db, connec_db_quries)\\nfor doi_or_url in liter_list_3:\\n    record_liter(doi_or_url, path_urls)\\n'"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main program\n",
    "'''\n",
    "# clear all files\n",
    "with open(path_urls, 'w') as f:\n",
    "    f.truncate()\n",
    "    f.close()\n",
    "    \n",
    "with open(path_related_urls, 'w') as f:\n",
    "    f.truncate()\n",
    "    f.close()\n",
    "'''\n",
    "# search for potentially related literature using the listed 3 methods\n",
    "# method 1\n",
    "# search_acad_db(keywords, acad_db)\n",
    "columns = ['DOI', 'url', 'title'] + on_topic_kws\n",
    "with open(path_potential, 'w') as f:\n",
    "    f.truncate()\n",
    "    f.close()\n",
    "\n",
    "scan_record_download(path_urls, on_topic_kws, pdf_folder_path, columns)\n",
    "\n",
    "'''\n",
    "# method 2\n",
    "liter_list_2 = []\n",
    "# liter_list_2 = expand_citation(seed_papers)\n",
    "for doi_or_url in liter_list_2:\n",
    "    record_liter(doi_or_url, path_urls)\n",
    "\n",
    "# method 3\n",
    "liter_list_3 = []\n",
    "# liter_list_3 = search_conne_db(connec_db, connec_db_quries)\n",
    "for doi_or_url in liter_list_3:\n",
    "    record_liter(doi_or_url, path_urls)\n",
    "'''\n",
    "\n",
    "# eleminate duplications\n",
    "# code\n",
    "\n",
    "# scan all the potential related literature and searching for the on-topic keywords and \n",
    "# record the number of times they appear\n",
    "# scan_poten_urls(path_urls, on_topic_kws)\n",
    "\n",
    "# send .PDF publication of all potential related literatures to ChatPDF.con and ask for relatedness \n",
    "# then record the answer to the list_of_potential_related_literature.csv as well\n",
    "#ChatPDF_relatedness(path_urls, chatpdf_related_queries)\n",
    "\n",
    "# now we have a list of potential related literature and the information about relatedness \n",
    "# stored in the file \"list_of_potential_related_literature.csv\"\n",
    "# now we may perform a automatic filtering and manual filtering of the literature\n",
    "\n",
    "# automatic filtering\n",
    "#auto_filter(path_potential)\n",
    "\n",
    "# manual filtering\n",
    "# manual_filter(path_potential, path_related_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of semi-automated literature search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of actually related literature stored in list_of_related_literature.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step: we perform a information search on the list of related literature, see the notebook named \"semi_automated_information_search.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
