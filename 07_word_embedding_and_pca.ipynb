{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 03:30:23 GM01X680 metapub.config[20938] WARNING NCBI_API_KEY was not set.\n"
     ]
    }
   ],
   "source": [
    "# import internal modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib\n",
    "import parameters as params\n",
    "import dataframe_columns as df_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predefined functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(sentence):\n",
    "    tokens = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        sentence_embedding = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "    \n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embedding(text):\n",
    "    # Split the text into sentences\n",
    "    sentences = sent_tokenize(text)  # You may need to use a more robust sentence tokenizer\n",
    "\n",
    "    # Initialize a list to store sentence embeddings\n",
    "    sentence_embeddings = []\n",
    "\n",
    "    # Tokenize and embed each sentence\n",
    "    for sentence in sentences:\n",
    "        embedding = sentence_embedding(sentence)\n",
    "        sentence_embeddings.append(embedding)\n",
    "\n",
    "    # Average pooling to obtain a single vector for the entire document\n",
    "    text_embedding = torch.mean(torch.stack(sentence_embeddings), dim=0)\n",
    "    \n",
    "    return text_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_sent(sent):\n",
    "    if not (sent.endswith('.') or sent.endswith('?') or sent.endswith('!')):\n",
    "        sent += '.'\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_t_a_k(index):\n",
    "    db_path = fpath.poten_litera_db\n",
    "    df = pd.read_csv(db_path, header=0, sep=\"\\t\")\n",
    "    \n",
    "    # locate the title in the row where column INDEX has the value index\n",
    "    title = df.loc[df[\"INDEX\"].astype(int) == index, \"TITLE\"].values[0]\n",
    "    abstract = df.loc[df[\"INDEX\"].astype(int) == index, \"ABSTRACT\"].values[0]\n",
    "    keywords = df.loc[df[\"INDEX\"].astype(int) == index, \"KEYWORDS\"].values[0]\n",
    "    \n",
    "    if title != title:\n",
    "        title = \"\"\n",
    "        \n",
    "    if abstract != abstract:\n",
    "        abstract = \"\"\n",
    "        \n",
    "    if keywords != keywords:\n",
    "        keywords = \"\"\n",
    "    \n",
    "    return title, abstract, keywords\n",
    "# --------------------Start of test code--------------------\n",
    "# index = 4\n",
    "# title, abstract, keywords = get_t_a_k(index)\n",
    "# print(title)\n",
    "# print(abstract)\n",
    "# print(keywords)\n",
    "# ---------------------End of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(index, title, abstract, keywords):\n",
    "    txt_file_name = str(index) + \".txt\"\n",
    "    txt_500_path = os.path.join(fpath.processed_texts_of_length_500_folder, txt_file_name)\n",
    "    text_relevant_path = os.path.join(fpath.relevant_text_folder, txt_file_name)\n",
    "\n",
    "    text_tak = \"\"\n",
    "    text_500 = \"\"\n",
    "    text_tak_500 = \"\"\n",
    "    text_relevant = \"\"\n",
    "    \n",
    "    # text_tak\n",
    "    if abstract == \"\":\n",
    "        pass\n",
    "    else:\n",
    "        if title != \"\":\n",
    "            text_tak = text_tak + complete_sent(title) + \" \"\n",
    "        else:\n",
    "            pass  \n",
    "        if abstract != \"\":\n",
    "            text_tak = text_tak + complete_sent(abstract) + \" \"\n",
    "        else:\n",
    "            pass\n",
    "        if keywords != \"\":\n",
    "            text_tak = text_tak + complete_sent(keywords) + \" \"\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        text_tak = plib.process_text(text_tak, lower=True)\n",
    "    \n",
    "    # text_500\n",
    "    if os.path.exists(txt_500_path):\n",
    "        with open(txt_500_path, \"r\", encoding='ascii') as f:\n",
    "            text_500 = f.read()    \n",
    "        text_500 = plib.process_text(text_500, lower=True)\n",
    "    else:\n",
    "        text_500 = \"\"\n",
    "    \n",
    "    # text_relevant\n",
    "    if os.path.exists(text_relevant_path):\n",
    "        with open(text_relevant_path, \"r\", encoding='ascii') as f:\n",
    "            text_relevant = f.read()    \n",
    "        text_relevant = plib.process_text(text_relevant, lower=True)\n",
    "    else:\n",
    "        text_relevant = \"\"\n",
    "    # print(text_relevant)\n",
    "    \n",
    "    # text_tak_500\n",
    "    if text_tak != \"\":\n",
    "        text_tak_500 = text_tak\n",
    "    elif text_500 != \"\":\n",
    "        text_tak_500 = text_500\n",
    "    else:\n",
    "        text_tak_500 = \"\"\n",
    "        \n",
    "    return text_tak_500, text_relevant\n",
    "# --------------------Start of test code--------------------\n",
    "# index = 0\n",
    "# title, abstract, keywords = get_t_a_k(index)\n",
    "# text_tak_500, text_relevant = get_text(index, title, abstract, keywords)\n",
    "# print(text_tak_500)\n",
    "# print(text_relevant)\n",
    "# ---------------------End of test code---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if my words are within the BERT vocabulary\n",
    "# db_path = fpath.poten_litera_db\n",
    "# df = pd.read_csv(db_path, header=None, sep=\",\")\n",
    "# df.columns = df_col.db_columns\n",
    "\n",
    "# for ind in df.index:\n",
    "#     index = int(df.at[ind, \"INDEX\"])\n",
    "    \n",
    "#     txt_file_name = str(index) + \".txt\"\n",
    "#     txt_500_path = os.path.join(fpath.processed_texts_of_length_500_folder, txt_file_name)\n",
    "    \n",
    "#     if os.path.exists(txt_500_path):\n",
    "#         with open(txt_500_path, \"r\", encoding='ascii') as f:\n",
    "#             txt_500 = f.read()\n",
    "        \n",
    "#         sentences = sent_tokenize(txt_500)\n",
    "#         for sentence in sentences:\n",
    "#             words_to_check = tokenizer(txt_500)\n",
    "#             print(words_to_check)\n",
    "\n",
    "#             # Check if each word is in the BERT vocabulary\n",
    "#             for word in words_to_check:\n",
    "#                 if word in tokenizer.get_vocab():\n",
    "#                     # print(f\"'{word}' is in the BERT vocabulary.\")\n",
    "#                     pass\n",
    "#                 else:\n",
    "#                     print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_1000 = fpath.poten_litera_testing_set_1000_labeled\n",
    "# df_1000 = pd.read_csv(test_1000, header=0, sep=\",\")\n",
    "\n",
    "# # tak embeddings and index list\n",
    "# tak_embeddings = []\n",
    "# tak_embeddings_index_list = []\n",
    "\n",
    "# # relevant text embeddings and index list\n",
    "# relevant_text_embeddings = []\n",
    "# relevant_text_embeddings_index_list = []\n",
    "\n",
    "# for ind in df_1000.index:\n",
    "#     # get the text\n",
    "#     index = int(df_1000.at[ind, \"INDEX\"])\n",
    "#     title, abstract, keywords = get_t_a_k(index)\n",
    "#     text_tak_500, text_relevant = get_text(index, title, abstract, keywords)\n",
    "    \n",
    "#     if text_tak_500 == text_tak_500 and text_tak_500 != \"\":\n",
    "#         tak_embedding = text_embedding(text_tak_500)\n",
    "#         tak_embeddings.append(tak_embedding)\n",
    "#         tak_embeddings_index_list.append(index)\n",
    "        \n",
    "#     if text_relevant == text_relevant and text_relevant != \"\":\n",
    "#         relevant_text_embedding = text_embedding(text_relevant)\n",
    "#         relevant_text_embeddings.append(relevant_text_embedding)\n",
    "#         relevant_text_embeddings_index_list.append(index)\n",
    "\n",
    "# # Save the results\n",
    "# datasets_folder = fpath.datasets_folder\n",
    "\n",
    "# # Save the tak embeddings\n",
    "# data_array_tak_embeddings = np.array(tak_embeddings)\n",
    "# np.save(os.path.join(datasets_folder, 'tak_embeddings.npy'), data_array_tak_embeddings)    # .npy extension is added if not given\n",
    "# with open(os.path.join(datasets_folder, 'tak_embeddings_index_list.txt'), 'w') as f:\n",
    "#     for item in tak_embeddings_index_list:\n",
    "#         f.write(\"%s\\n\" % item)\n",
    "\n",
    "# # Save the relevant text embeddings     \n",
    "# data_array_relevant_text_embeddings = np.array(relevant_text_embeddings)\n",
    "# np.save(os.path.join(datasets_folder, 'relevant_text_embeddings.npy'), data_array_relevant_text_embeddings)    # .npy extension is added if not given\n",
    "# with open(os.path.join(datasets_folder, 'relevant_text_embeddings_index_list.txt'), 'w') as f:\n",
    "#     for item in relevant_text_embeddings_index_list:\n",
    "#         f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords count transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate tht rows of poten_litera_db_kw_count and perform a function on the number of keywords in each row\n",
    "input_path = fpath.poten_litera_db_kw_count\n",
    "df = pd.read_csv(input_path, header=0, sep='\\t')\n",
    "\n",
    "key_list = list(params.ranking_kw_groups.keys())\n",
    "\n",
    "count_list_tak = []\n",
    "count_list_tak_index_list = []\n",
    "count_list_500 = []\n",
    "count_list_500_index_list = []\n",
    "count_list_full_text = []\n",
    "count_list_full_text_index_list = []\n",
    "\n",
    "for ind in df.index:\n",
    "    index = int(df.at[ind, \"INDEX\"])\n",
    "    # print(index)\n",
    "    \n",
    "    count_tak = []\n",
    "    count_500 = []\n",
    "    count_full_text = []\n",
    "    \n",
    "    if df.at[ind, \"MACAQUE_COUNT_IN_TAK\"] == df.at[ind, \"MACAQUE_COUNT_IN_TAK\"]:\n",
    "        count_list_tak_index_list.append(index)\n",
    "        for key in key_list:\n",
    "            count_tak.append(int(df.at[ind, key+\"_COUNT_IN_TAK\"]))\n",
    "        count_list_tak.append(count_tak)\n",
    "    \n",
    "    if df.at[ind, \"MACAQUE_COUNT_IN_500\"] == df.at[ind, \"MACAQUE_COUNT_IN_500\"]:\n",
    "        count_list_500_index_list.append(index)\n",
    "        for key in key_list:\n",
    "            count_500.append(int(df.at[ind, key+\"_COUNT_IN_500\"]))\n",
    "        count_list_500.append(count_500)\n",
    "                \n",
    "    if df.at[ind, \"MACAQUE_COUNT_IN_FULL_TEXT\"] == df.at[ind, \"MACAQUE_COUNT_IN_FULL_TEXT\"]:\n",
    "        count_list_full_text_index_list.append(index)\n",
    "        for key in key_list:\n",
    "            count_full_text.append(int(df.at[ind, key+\"_COUNT_IN_FULL_TEXT\"]))\n",
    "        count_list_full_text.append(count_full_text)\n",
    "    \n",
    "# Save the results\n",
    "datasets_folder = fpath.datasets_folder\n",
    "\n",
    "# Save the count_list_tak\n",
    "data_array_count_list_tak = np.array(count_list_tak)\n",
    "np.save(os.path.join(datasets_folder, 'count_list_tak.npy'), data_array_count_list_tak)    # .npy extension is added if not given\n",
    "with open(os.path.join(datasets_folder, 'count_list_tak_index_list.txt'), 'w') as f:\n",
    "    for item in count_list_tak_index_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "# Save the count_list_500\n",
    "# for l in count_list_500:\n",
    "#     print(l)    \n",
    "data_array_count_list_500 = np.array(count_list_500)\n",
    "# print(count_list_500)\n",
    "np.save(os.path.join(datasets_folder, 'count_list_500.npy'), data_array_count_list_500)    # .npy extension is added if not given\n",
    "with open(os.path.join(datasets_folder, 'count_list_500_index_list.txt'), 'w') as f:\n",
    "    for item in count_list_500_index_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "        \n",
    "# Save the count_list_full_text     \n",
    "data_array_count_list_full_text = np.array(count_list_full_text)\n",
    "np.save(os.path.join(datasets_folder, 'count_list_full_text.npy'), data_array_count_list_full_text)    # .npy extension is added if not given\n",
    "with open(os.path.join(datasets_folder, 'count_list_full_text_index_list.txt'), 'w') as f:\n",
    "    for item in count_list_full_text_index_list:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_and_save_array(input_file):\n",
    "    input_file_path = os.path.join(datasets_folder, input_file+'.npy')\n",
    "    output_file_path = os.path.join(datasets_folder, 'trans_'+input_file + '.npy')\n",
    "    \n",
    "    # Load the array from the input .npy file\n",
    "    array = np.load(input_file_path)\n",
    "\n",
    "    # Apply the transformation\n",
    "    transformed_array = np.log(np.minimum(array + 1, 10)) / np.log(10)\n",
    "\n",
    "    # Save the transformed array to the output .npy file\n",
    "    np.save(output_file_path, transformed_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the counts and save the array\n",
    "trans_count_list_tak = transform_and_save_array('count_list_tak')\n",
    "trans_count_list_500 = transform_and_save_array('count_list_500')\n",
    "trans_count_full_text = transform_and_save_array('count_list_full_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def perform_pca(original_array_name, pca_array_name, n_components):\n",
    "    original_path = os.path.join(datasets_folder, original_array_name + '.npy')\n",
    "    array = np.load(original_path)\n",
    "    # print(array)\n",
    "    \n",
    "    # Standardizing the Data\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(array)\n",
    "    \n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(array)\n",
    "    \n",
    "    # Transform the data\n",
    "    transformed_data = pca.transform(array)\n",
    "    \n",
    "    # save the pca\n",
    "    pca_file_path = os.path.join(datasets_folder, pca_array_name + '.npy')\n",
    "    np.save(pca_file_path, transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PCA on the tak_embeddings\n",
    "# perform_pca('tak_embeddings', 'pca_tak_embeddings_2', 2)\n",
    "# perform_pca('tak_embeddings', 'pca_tak_embeddings_3', 3)\n",
    "\n",
    "# # PCA on the relevant_text_embeddings\n",
    "# perform_pca('relevant_text_embeddings', 'pca_relevant_text_embeddings_2', 2)\n",
    "# perform_pca('relevant_text_embeddings', 'pca_relevant_text_embeddings_3', 3)\n",
    "\n",
    "# PCA on the count_list_tak\n",
    "perform_pca('count_list_tak', 'pca_count_list_tak_2', 2)\n",
    "perform_pca('count_list_tak', 'pca_count_list_tak_3', 3)\n",
    "\n",
    "# PCA on the count_list_500\n",
    "perform_pca('count_list_500', 'pca_count_list_500_2', 2)\n",
    "perform_pca('count_list_500', 'pca_count_list_500_3', 3)\n",
    "\n",
    "# PCA on the count_list_full_text\n",
    "perform_pca('count_list_full_text', 'pca_count_list_full_text_2', 2)\n",
    "perform_pca('count_list_full_text', 'pca_count_list_full_text_3', 3)\n",
    "\n",
    "# PCA on the trans_count_list_tak\n",
    "perform_pca('trans_count_list_tak', 'pca_trans_count_list_tak_2', 2)\n",
    "perform_pca('trans_count_list_tak', 'pca_trans_count_list_tak_3', 3)\n",
    "\n",
    "# PCA on the trans_count_list_500\n",
    "perform_pca('trans_count_list_500', 'pca_trans_count_list_500_2', 2)\n",
    "perform_pca('trans_count_list_500', 'pca_trans_count_list_500_3', 3)\n",
    "\n",
    "# PCA on the trans_count_list_full_text\n",
    "perform_pca('trans_count_list_full_text', 'trans_pca_count_list_full_text_2', 2)\n",
    "perform_pca('trans_count_list_full_text', 'trans_pca_count_list_full_text_3', 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
