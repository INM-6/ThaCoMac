{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib\n",
    "import parameters as params\n",
    "import dataframe_columns as df_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predefined functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(sentence):\n",
    "    tokens = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        sentence_embedding = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "    \n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embedding(text):\n",
    "    # Split the text into sentences\n",
    "    sentences = sent_tokenize(text)  # You may need to use a more robust sentence tokenizer\n",
    "\n",
    "    # Initialize a list to store sentence embeddings\n",
    "    sentence_embeddings = []\n",
    "\n",
    "    # Tokenize and embed each sentence\n",
    "    for sentence in sentences:\n",
    "        sentence_embedding = sentence_embedding(sentence)\n",
    "        sentence_embeddings.append(sentence_embedding)\n",
    "\n",
    "    # Average pooling to obtain a single vector for the entire document\n",
    "    text_embedding = torch.mean(torch.stack(sentence_embeddings), dim=0)\n",
    "    \n",
    "    return text_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if my words are within the BERT vocabulary\n",
    "db_path = fpath.poten_litera_db\n",
    "df = pd.read_csv(db_path, header=None, sep=\",\")\n",
    "df.columns = df_col.db_columns\n",
    "\n",
    "for ind in df.index:\n",
    "    index = int(df.at[ind, \"INDEX\"])\n",
    "    \n",
    "    txt_file_name = str(index) + \".txt\"\n",
    "    txt_500_path = os.path.join(fpath.processed_texts_of_length_500_folder, txt_file_name)\n",
    "    \n",
    "    with open(txt_500_path, \"r\", encoding='ascii') as f:\n",
    "        txt_500 = f.read()\n",
    "        \n",
    "    words_to_check = tokenizer(txt_500)\n",
    "\n",
    "    # Check if each word is in the BERT vocabulary\n",
    "    for word in words_to_check:\n",
    "        if word in tokenizer.get_vocab():\n",
    "            # print(f\"'{word}' is in the BERT vocabulary.\")\n",
    "            pass\n",
    "        else:\n",
    "            print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = fpath.poten_litera_db\n",
    "df = pd.read_csv(db_path, header=None, sep=\",\")\n",
    "df.columns = df_col.db_columns\n",
    "\n",
    "text_embeddings = []\n",
    "\n",
    "for ind in df.index:\n",
    "    index = int(df.at[ind, \"INDEX\"])\n",
    "    \n",
    "    txt_file_name = str(index) + \".txt\"\n",
    "    txt_500_path = os.path.join(fpath.processed_texts_of_length_500_folder, txt_file_name)\n",
    "    \n",
    "    with open(txt_500_path, \"r\", encoding='ascii') as f:\n",
    "        txt_500 = f.read()\n",
    "        \n",
    "    txt_500 = plib.process_text(txt_500, lower=True)\n",
    "    \n",
    "    text_embed = text_embedding(txt_500)\n",
    "    text_embeddings.append(text_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords count transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate tht rows of poten_litera_db_kw_count and perform a function on the number of keywords in each row\n",
    "input_path = fpath.poten_litera_db_kw_count\n",
    "df = pd.read_csv(input_path, header=0, sep=',')\n",
    "df.columns = df_col.db_count_columns\n",
    "\n",
    "key_list = list(params.ranking_kw_groups.keys())\n",
    "\n",
    "count_500_list = []\n",
    "trans_count_500_list = []\n",
    "count_full_text_list = []\n",
    "trans_count_full_text_list = []\n",
    "\n",
    "for ind in df.index:\n",
    "    count_500 = []\n",
    "    count_full_text = []\n",
    "    \n",
    "    # Get the lists of counts for both text_500 and text_txt\n",
    "    for key in key_list:\n",
    "        count_500.append(df.at[ind, key+\"_COUNT_IN_500\"])\n",
    "        count_full_text.append(df.at[ind, key+\"_COUNT_IN_FULL_TEXT\"])\n",
    "    \n",
    "    trans_count_500 = []\n",
    "    trans_count_full_text = []\n",
    "    \n",
    "    # Transform the counts\n",
    "    for i in range(len(count_500)):\n",
    "        trans_count_500[i] = math.log(max(count_500[i]+1, 5), 5)\n",
    "        trans_count_full_text = math.log10(max(count_full_text[i]+1, 10), 10) \n",
    "    # print(count_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of components you want to keep (e.g., 2 for 2D PCA)\n",
    "n_components = 2\n",
    "\n",
    "# Create PCA models\n",
    "pca_2 = PCA(n_components=2)\n",
    "pca_3 = PCA(n_components=3)\n",
    "\n",
    "# dataset folder\n",
    "datasets_folder = fpath.datasets_folder\n",
    "\n",
    "# PCA on the embeddings\n",
    "data_array_embeddings = np.array(text_embeddings)\n",
    "embeddings_dim_2 = pca_2.fit_transform(data_array_embeddings)\n",
    "embeddings_dim_3 = pca_3.fit_transform(data_array_embeddings)\n",
    "\n",
    "# Save the results\n",
    "np.save(os.path.join(datasets_folder, 'pca_embeddings_dim_3.npy'), embeddings_dim_2)    # .npy extension is added if not given\n",
    "np.save(os.path.join(datasets_folder, 'pca_embeddings_dim_3.npy'), embeddings_dim_3)\n",
    "\n",
    "# PCA on the count_500\n",
    "data_array_count_500 = np.array(count_500_list)\n",
    "count_500_dim_2 = pca_2.fit_transform(data_array_count_500)\n",
    "count_500_dim_3 = pca_3.fit_transform(data_array_count_500)\n",
    "\n",
    "# Save the results of the PCA\n",
    "np.save(os.path.join(datasets_folder, 'pca_count_500_dim_2.npy'), count_500_dim_2)    # .npy extension is added if not given\n",
    "np.save(os.path.join(datasets_folder, 'pca_count_500_dim_3.npy'), count_500_dim_3)\n",
    "\n",
    "# PCA on the trans_count_500\n",
    "data_array_trans_count_500 = np.array(trans_count_500_list)\n",
    "trans_count_500_dim_2 = pca_2.fit_transform(data_array_trans_count_500)\n",
    "trans_count_500_dim_3 = pca_3.fit_transform(data_array_trans_count_500)\n",
    "\n",
    "# Save the results of the PCA\n",
    "np.save(os.path.join(datasets_folder, 'pca_trans_count_500_dim_2.npy'), trans_count_500_dim_2)    # .npy extension is added if not given\n",
    "np.save(os.path.join(datasets_folder, 'pca_trans_count_500_dim_3.npy'), trans_count_500_dim_3)\n",
    "\n",
    "# PCA on the count_full_text\n",
    "data_array_count_full_text = np.array(count_full_text_list)\n",
    "count_full_text_dim_2 = pca_2.fit_transform(data_array_count_full_text)\n",
    "count_full_text_dim_3 = pca_3.fit_transform(data_array_count_full_text)\n",
    "\n",
    "# Save the results of the PCA\n",
    "np.save(os.path.join(datasets_folder, 'pca_count_full_text_dim_2.npy'), count_full_text_dim_2)    # .npy extension is added if not given\n",
    "np.save(os.path.join(datasets_folder, 'pca_count_full_text_dim_3.npy'), count_full_text_dim_3)\n",
    "\n",
    "# PCA on the trans_count_full_text\n",
    "data_array_trans_count_full_text = np.array(trans_count_full_text_list)\n",
    "trans_count_full_text_dim_2 = pca_2.fit_transform(data_array_trans_count_full_text)\n",
    "trans_count_full_text_dim_3 = pca_3.fit_transform(data_array_trans_count_full_text)\n",
    "\n",
    "# Save the results of the PCA\n",
    "np.save(os.path.join(datasets_folder, 'pca_trans_count_full_text_dim_2.npy'), trans_count_full_text_dim_2)    # .npy extension is added if not given\n",
    "np.save(os.path.join(datasets_folder, 'pca_trans_count_full_text_dim_3.npy'), trans_count_full_text_dim_3)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
