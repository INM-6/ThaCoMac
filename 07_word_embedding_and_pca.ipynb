{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-07 02:34:56 GM01X680 metapub.config[112297] WARNING NCBI_API_KEY was not set.\n"
     ]
    }
   ],
   "source": [
    "# import internal modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib\n",
    "import parameters as params\n",
    "import dataframe_columns as df_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predefined functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(sentence):\n",
    "    tokens = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        sentence_embedding = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "    \n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embedding(text):\n",
    "    # Split the text into sentences\n",
    "    sentences = sent_tokenize(text)  # You may need to use a more robust sentence tokenizer\n",
    "\n",
    "    # Initialize a list to store sentence embeddings\n",
    "    sentence_embeddings = []\n",
    "\n",
    "    # Tokenize and embed each sentence\n",
    "    for sentence in sentences:\n",
    "        embedding = sentence_embedding(sentence)\n",
    "        sentence_embeddings.append(embedding)\n",
    "\n",
    "    # Average pooling to obtain a single vector for the entire document\n",
    "    text_embedding = torch.mean(torch.stack(sentence_embeddings), dim=0)\n",
    "    \n",
    "    return text_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if my words are within the BERT vocabulary\n",
    "# db_path = fpath.poten_litera_db\n",
    "# df = pd.read_csv(db_path, header=None, sep=\",\")\n",
    "# df.columns = df_col.db_columns\n",
    "\n",
    "# for ind in df.index:\n",
    "#     index = int(df.at[ind, \"INDEX\"])\n",
    "    \n",
    "#     txt_file_name = str(index) + \".txt\"\n",
    "#     txt_500_path = os.path.join(fpath.processed_texts_of_length_500_folder, txt_file_name)\n",
    "    \n",
    "#     if os.path.exists(txt_500_path):\n",
    "#         with open(txt_500_path, \"r\", encoding='ascii') as f:\n",
    "#             txt_500 = f.read()\n",
    "        \n",
    "#         sentences = sent_tokenize(txt_500)\n",
    "#         for sentence in sentences:\n",
    "#             words_to_check = tokenizer(txt_500)\n",
    "#             print(words_to_check)\n",
    "\n",
    "#             # Check if each word is in the BERT vocabulary\n",
    "#             for word in words_to_check:\n",
    "#                 if word in tokenizer.get_vocab():\n",
    "#                     # print(f\"'{word}' is in the BERT vocabulary.\")\n",
    "#                     pass\n",
    "#                 else:\n",
    "#                     print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(index, title, abstract, keywords):\n",
    "    txt_file_name = str(index) + \".txt\"\n",
    "    txt_500_path = os.path.join(fpath.processed_texts_of_length_500_folder, txt_file_name)\n",
    "    \n",
    "    text_tak = \"\"\n",
    "    text_500 = \"\"\n",
    "    \n",
    "    if title == title:\n",
    "        text_tak = text_tak + title + \" \"\n",
    "    else:\n",
    "        pass  \n",
    "    if abstract == abstract:\n",
    "        text_tak = text_tak + abstract + \" \"\n",
    "    else:\n",
    "        pass\n",
    "    if keywords == keywords:\n",
    "        text_tak = text_tak + keywords + \" \"\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    text_tak = plib.process_text(text_tak, lower=True)\n",
    "    \n",
    "    if os.path.exists(txt_500_path):\n",
    "        with open(txt_500_path, \"r\", encoding='ascii') as f:\n",
    "            text_500 = f.read()    \n",
    "        text_500 = plib.process_text(text_500, lower=True)\n",
    "        \n",
    "    if len(text_tak.split()) >= len(text_500.split()):\n",
    "        text = text_tak\n",
    "    else:\n",
    "        text = text_500\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = fpath.poten_litera_db\n",
    "df = pd.read_csv(db_path, header=None, sep=\",\")\n",
    "df.columns = df_col.db_columns\n",
    "\n",
    "text_embeddings = []\n",
    "\n",
    "for ind in df.index:\n",
    "    index = int(df.at[ind, \"INDEX\"])\n",
    "    \n",
    "    title = df.at[ind, \"TITLE\"]\n",
    "    abstract = df.at[ind, \"ABSTRACT\"]\n",
    "    keywords = df.at[ind, \"KEYWORDS\"]\n",
    "    \n",
    "    text = get_text(index, title, abstract, keywords)\n",
    "        \n",
    "    text_embed = text_embedding(text)\n",
    "    text_embeddings.append(text_embed)\n",
    "\n",
    "print(len(text_embeddings))\n",
    "print(text_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords count transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate tht rows of poten_litera_db_kw_count and perform a function on the number of keywords in each row\n",
    "input_path = fpath.poten_litera_db_kw_count\n",
    "df = pd.read_csv(input_path, header=0, sep=',')\n",
    "df.columns = df_col.db_count_columns\n",
    "\n",
    "key_list = list(params.ranking_kw_groups.keys())\n",
    "\n",
    "count_500_list = []\n",
    "trans_count_500_list = []\n",
    "# count_full_text_list = []\n",
    "# trans_count_full_text_list = []\n",
    "\n",
    "for ind in df.index:\n",
    "    count_500 = []\n",
    "    # count_full_text = []\n",
    "    \n",
    "    # Get the lists of counts for both text_500 and text_txt\n",
    "    for key in key_list:\n",
    "        count_500.append(df.at[ind, key+\"_COUNT_IN_500\"])\n",
    "        # count_full_text.append(df.at[ind, key+\"_COUNT_IN_FULL_TEXT\"])\n",
    "    \n",
    "    trans_count_500 = []\n",
    "    trans_count_full_text = []\n",
    "    \n",
    "    # Transform the counts\n",
    "    for i in range(len(count_500)):\n",
    "        trans_count_500[i] = math.log(min(count_500[i]+1, 4), 4)\n",
    "        # trans_count_full_text = math.log10(min(count_full_text[i]+1, 10), 10) \n",
    "    # print(count_list)\n",
    "\n",
    "print(count_500_list)\n",
    "print(trans_count_500_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of components you want to keep (e.g., 2 for 2D PCA)\n",
    "n_components = 2\n",
    "\n",
    "# Create PCA models\n",
    "pca_2 = PCA(n_components=2)\n",
    "pca_3 = PCA(n_components=3)\n",
    "\n",
    "# dataset folder\n",
    "datasets_folder = fpath.datasets_folder\n",
    "\n",
    "# PCA on the embeddings\n",
    "data_array_embeddings = np.array(text_embeddings)\n",
    "embeddings_dim_2 = pca_2.fit_transform(data_array_embeddings)\n",
    "embeddings_dim_3 = pca_3.fit_transform(data_array_embeddings)\n",
    "\n",
    "# Save the results\n",
    "np.save(os.path.join(datasets_folder, 'pca_embeddings_dim_3.npy'), embeddings_dim_2)    # .npy extension is added if not given\n",
    "np.save(os.path.join(datasets_folder, 'pca_embeddings_dim_3.npy'), embeddings_dim_3)\n",
    "\n",
    "# PCA on the count_500\n",
    "data_array_count_500 = np.array(count_500_list)\n",
    "count_500_dim_2 = pca_2.fit_transform(data_array_count_500)\n",
    "count_500_dim_3 = pca_3.fit_transform(data_array_count_500)\n",
    "\n",
    "# Save the results of the PCA\n",
    "np.save(os.path.join(datasets_folder, 'pca_count_500_dim_2.npy'), count_500_dim_2)    # .npy extension is added if not given\n",
    "np.save(os.path.join(datasets_folder, 'pca_count_500_dim_3.npy'), count_500_dim_3)\n",
    "\n",
    "# PCA on the trans_count_500\n",
    "data_array_trans_count_500 = np.array(trans_count_500_list)\n",
    "trans_count_500_dim_2 = pca_2.fit_transform(data_array_trans_count_500)\n",
    "trans_count_500_dim_3 = pca_3.fit_transform(data_array_trans_count_500)\n",
    "\n",
    "# Save the results of the PCA\n",
    "np.save(os.path.join(datasets_folder, 'pca_trans_count_500_dim_2.npy'), trans_count_500_dim_2)    # .npy extension is added if not given\n",
    "np.save(os.path.join(datasets_folder, 'pca_trans_count_500_dim_3.npy'), trans_count_500_dim_3)\n",
    "\n",
    "# # PCA on the count_full_text\n",
    "# data_array_count_full_text = np.array(count_full_text_list)\n",
    "# count_full_text_dim_2 = pca_2.fit_transform(data_array_count_full_text)\n",
    "# count_full_text_dim_3 = pca_3.fit_transform(data_array_count_full_text)\n",
    "\n",
    "# # Save the results of the PCA\n",
    "# np.save(os.path.join(datasets_folder, 'pca_count_full_text_dim_2.npy'), count_full_text_dim_2)    # .npy extension is added if not given\n",
    "# np.save(os.path.join(datasets_folder, 'pca_count_full_text_dim_3.npy'), count_full_text_dim_3)\n",
    "\n",
    "# # PCA on the trans_count_full_text\n",
    "# data_array_trans_count_full_text = np.array(trans_count_full_text_list)\n",
    "# trans_count_full_text_dim_2 = pca_2.fit_transform(data_array_trans_count_full_text)\n",
    "# trans_count_full_text_dim_3 = pca_3.fit_transform(data_array_trans_count_full_text)\n",
    "\n",
    "# # Save the results of the PCA\n",
    "# np.save(os.path.join(datasets_folder, 'pca_trans_count_full_text_dim_2.npy'), trans_count_full_text_dim_2)    # .npy extension is added if not given\n",
    "# np.save(os.path.join(datasets_folder, 'pca_trans_count_full_text_dim_3.npy'), trans_count_full_text_dim_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
