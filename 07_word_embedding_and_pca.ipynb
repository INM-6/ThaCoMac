{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk import ngrams\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib\n",
    "import parameters as params\n",
    "import dataframe_columns as df_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predefined functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embeebeding():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embedding():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if my words are within the BERT vocabulary\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Your list of words to check\n",
    "words_to_check = tokenizer(text)\n",
    "\n",
    "# Check if each word is in the BERT vocabulary\n",
    "for word in words_to_check:\n",
    "    if word in tokenizer.get_vocab():\n",
    "        # print(f\"'{word}' is in the BERT vocabulary.\")\n",
    "        pass\n",
    "    else:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embedding\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load the BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "def text_embedding(text_500):\n",
    "    # Split the document into sentences\n",
    "    sentences = sent_tokenize(text_500)  # You may need to use a more robust sentence tokenizer\n",
    "\n",
    "    # Initialize a list to store sentence embeddings\n",
    "    sentence_embeddings = []\n",
    "\n",
    "    # Tokenize and embed each sentence\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens)\n",
    "            sentence_embedding = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "        sentence_embeddings.append(sentence_embedding)\n",
    "\n",
    "    # Average pooling to obtain a single vector for the entire document\n",
    "    text_embedding = torch.mean(torch.stack(sentence_embeddings), dim=0)\n",
    "    \n",
    "    return text_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the rows of the dataframe and embed the text\n",
    "input_path = fpath.poten_litera_db_kw_count\n",
    "output_path = fpath.poten_litera_db_kw_count_with_embedding\n",
    "\n",
    "df = pd.read_csv(input_path, header=0, sep=',')\n",
    "df.columns = df_col.db_ranked_columns\n",
    "\n",
    "# clear file\n",
    "plib.clear_file(output_path)\n",
    "\n",
    "for ind in df.index:\n",
    "    index = int(df.at[ind, \"INDEX\"])\n",
    "    \n",
    "    \n",
    "    text_500 = df.at[ind, \"TEXT_500\"]\n",
    "    text_500 = plib.process_text(text_500, lower=True)\n",
    "    text_embedding = text_embedding(text_500)\n",
    "    df.at[ind, \"TEXT_EMBEDDING\"] = text_embedding\n",
    "    \n",
    "    print(\"ind:\", ind, \"index:\", df.at[ind, \"INDEX\"])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "# Save the dataframe\n",
    "df.to_csv(fpath.output_path, index=False)\n",
    "\n",
    "# Read the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Sample data (replace with your data)\n",
    "data = [\n",
    "    [1.2, 2.3, 0.5, ...],  # High-dimensional data point 1\n",
    "    [0.8, 1.9, 0.3, ...],  # High-dimensional data point 2\n",
    "    # Add more data points\n",
    "]\n",
    "\n",
    "# Convert data to a NumPy array\n",
    "data_array = np.array(data)\n",
    "\n",
    "# Define the number of components you want to keep (e.g., 2 for 2D PCA)\n",
    "n_components = 2\n",
    "\n",
    "# Create a PCA model\n",
    "pca_2 = PCA(n_components=2)\n",
    "pca_3 = PCA(n_components=3)\n",
    "\n",
    "# Fit the model to your data and transform the data to the lower-dimensional space\n",
    "dim_2_data = pca_2.fit_transform(data_array)\n",
    "dim_3_data = pca_3.fit_transform(data_array)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
