{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib\n",
    "import parameters as params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predefined functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert_model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_sent(sent):\n",
    "    if not (sent.endswith('.') or sent.endswith('?') or sent.endswith('!')):\n",
    "        sent += '.'\n",
    "    \n",
    "    return sent\n",
    "# # --test--\n",
    "# text = \"distribution of the dopamine innervation in the macaque and human thalamus,\"\n",
    "# complete_sentence = complete_sent(text)\n",
    "# print(complete_sentence)\n",
    "# # --test--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_short_segments(text, max_tokens=400):\n",
    "    # Split text into sentences\n",
    "    text = plib.process_text(text, lower=True)\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Group sentences into segments\n",
    "    segments = []\n",
    "    current_segment = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = complete_sent(sentence)\n",
    "        # Check token count if this sentence is added\n",
    "        potential_segment = (current_segment + \" \" + sentence).strip()\n",
    "        tokens = bert_tokenizer.tokenize(potential_segment)\n",
    "        if len(tokens) > max_tokens:\n",
    "            # If limit exceeded, start a new segment\n",
    "            segments.append(current_segment.strip())\n",
    "            current_segment = sentence\n",
    "        else:\n",
    "            # Otherwise, add sentence to current segment\n",
    "            current_segment = potential_segment\n",
    "    \n",
    "    # Add the last segment if it's not empty\n",
    "    if current_segment:\n",
    "        segments.append(current_segment.strip())\n",
    "\n",
    "    return segments\n",
    "# # --test--\n",
    "# text = \"distribution of the dopamine innervation in the macaque and human thalamus. fax: +34 91 497 53 15. we recently defined the thalamic dopaminergic system in primates; it arises from numerous dopaminergic cell groups and selectively targets numerous thalamic nuclei. given the central position of the thalamus in subcortical and cortical interplay, and the functional relevance of dopamine neuromodulation in the brain, detailing dopamine distribution in the thalamus should supply important information. to this end we performed immunohistochemistry for dopamine and the dopamine transporter in the thalamus of macaque monkeys and humans to generate maps, in the stereotaxic coronal plane, of the distribution of dopaminergic axons. the dopamine innervation of the thalamus follows the same pattern in both species and is most dense in midline limbic nuclei, the mediodorsal and lateral posterior association nuclei, and in the ventral lateral and ventral anterior motor nuclei. this distribution suggests that thalamic dopamine has a prominent role in emotion, attention, cognition and complex somatosensory and visual processing, as well as in motor control. most thalamic dopaminergic axons are thin and varicose and target both the neuropil and small blood vessels, suggesting that, besides neuronal modulation, thalamic dopamine may have a direct influence on microcirculation. the maps provided here should be a useful reference in future experimental and neuroimaging studies aiming at clarifying the role of the thalamic dopaminergic system in health and in conditions involving brain dopamine, including parkinsons disease, drug addiction and schizophrenia. keywords dopamine thalamus monkey human primate dopamine transporter parkinson schizophrenia addiction introduction the thalamus is made up of multiple nuclei relaying information from subcortical centers or from other cortices to the cerebral cortex (sherman and guillery, 2005), as well as the striatum, the nucleus accumbens and the amygdala (steriade et al., 1997). in addition to specific subcortical and cortical afferents, the primate thalamus receives axons containing the neuromodulators acetylcholine (heckers et al., 1992), histamine (manning et al., 1996), serotonin (morrison and foote, 1986; lavoie and parent, 1991), and the catecholamines adrenaline (rico and cavada, 1998a), noradrenaline (morrison and foote, 1986; ginsberg et al., 1993) and dopamine (snchez-gonzlez et al., 2005). until recently, the existence of significant dopamine innervation in the primate thalamus has been largely ignored, probably because dopamine innervation of the rodent thalamus is very scant (groenewegen, 1988; papadopoulos and parnavelas, 1990). however, fragmentary data scattered through the literature endorse the presence of dopamine innervation in the primate thalamus. postmortem biochemical studies showed the presence of dopamine in the thalamus of macaques (brown et al., 1979; goldman-rakic and brown, 1981; pifl et al., 1990, 1991) and human subjects (oke and adams, 1987). later, receptor binding and in situ hybridization analyses detected the presence of dopamine d2-like (joyce et al., 1991; kessler et al., 1993; hall et al., 1996; langer et al., 1999; rieck et al., 2004) and d3-like receptors (gurevich and joyce, 1999) in several human thalamic nuclei. positron emission tomography (pet) radioligand studies have also demonstrated the presence of the dopamine transporter (dat) (wang et al., 1995; halldin et al., 1996; helfenbein et al., 1999; brownell et al., 2003) and of d2-like receptors (farde et al., 1997; langer et al., 1999; okubo et al., 1999; brownell et al., 2003; rieck et al., 2004) in the human and macaque thalamus. in the course of pet studies focusing on schizophrenia, d2- and d3-like radioligand binding was also found in the thalamus of control subjects (talvik et al., 2003; yasuno et al., 2004). finally, an immunohistochemical study using anti-dat antibodies detected the presence of dopaminergic axons in the mediodorsal nucleus (md) of the macaque thalamus (melchitzky and lewis, 2001).\"\n",
    "# segments = split_text_into_short_segments(text, max_tokens=400)\n",
    "# for i, segment in enumerate(segments):\n",
    "#     print(i, segment)\n",
    "# # --test--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text_segment(text_segment):\n",
    "    tokens = bert_tokenizer(text_segment, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**tokens)\n",
    "        text_segment_embedding = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "    \n",
    "    return text_segment_embedding\n",
    "# # --test--\n",
    "# text_segment = \"distribution of the dopamine innervation in the macaque and human thalamus. fax: +34 91 497 53 15. we recently defined the thalamic dopaminergic system in primates; it arises from numerous dopaminergic cell groups and selectively targets numerous thalamic nuclei. given the central position of the thalamus in subcortical and cortical interplay, and the functional relevance of dopamine neuromodulation in the brain, detailing dopamine distribution in the thalamus should supply important information.\"\n",
    "# text_segment_embedding = embed_text_segment(text_segment)\n",
    "# print(text_segment_embedding.shape)\n",
    "# print(text_segment_embedding)\n",
    "# # --test--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedd_text(text):\n",
    "    # Preprocess the text\n",
    "    text = plib.process_text(text, lower=True)\n",
    "    # Split the text into sentences\n",
    "    shorter_text_segements = split_text_into_short_segments(text, max_tokens=400)  # You may need to use a more robust sentence tokenizer\n",
    "\n",
    "    # Initialize a list to store sentence embeddings\n",
    "    text_segment_embeddings = []\n",
    "\n",
    "    # Tokenize and embed each sentence\n",
    "    for text_segment in shorter_text_segements:\n",
    "        embedding = embed_text_segment(text_segment)\n",
    "        text_segment_embeddings.append(embedding)\n",
    "\n",
    "    # Average pooling to obtain a single vector for the entire document\n",
    "    text_embedding = torch.mean(torch.stack(text_segment_embeddings), dim=0)\n",
    "    \n",
    "    return text_embedding\n",
    "# # --test--\n",
    "# text = \"distribution of the dopamine innervation in the macaque and human thalamus. fax: +34 91 497 53 15. we recently defined the thalamic dopaminergic system in primates; it arises from numerous dopaminergic cell groups and selectively targets numerous thalamic nuclei. given the central position of the thalamus in subcortical and cortical interplay, and the functional relevance of dopamine neuromodulation in the brain, detailing dopamine distribution in the thalamus should supply important information.\"\n",
    "# text_embedding = embedd_text(text)\n",
    "# print(text_embedding.shape)\n",
    "# print(text_embedding)\n",
    "# # --test--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_t_a_k(index):\n",
    "    db_path = fpath.poten_litera_db\n",
    "    df = pd.read_csv(db_path, header=0, sep=\"\\t\")\n",
    "    \n",
    "    # locate the title in the row where column INDEX has the value index\n",
    "    title = df.loc[df[\"INDEX\"].astype(int) == index, \"TITLE\"].values[0]\n",
    "    abstract = df.loc[df[\"INDEX\"].astype(int) == index, \"ABSTRACT\"].values[0]\n",
    "    keywords = df.loc[df[\"INDEX\"].astype(int) == index, \"KEYWORDS\"].values[0]\n",
    "    \n",
    "    if title != title:\n",
    "        title = \"\"\n",
    "        \n",
    "    if abstract != abstract:\n",
    "        abstract = \"\"\n",
    "        \n",
    "    if keywords != keywords:\n",
    "        keywords = \"\"\n",
    "    \n",
    "    return title, abstract, keywords\n",
    "# # --------------------Start of test code--------------------\n",
    "# index = 2\n",
    "# title, abstract, keywords = get_t_a_k(index)\n",
    "# print(title)\n",
    "# print(abstract)\n",
    "# print(keywords)\n",
    "# # ---------------------End of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(index, title, abstract, keywords):\n",
    "    txt_file_name = str(index) + \".txt\"\n",
    "    txt_500_path = os.path.join(fpath.processed_texts_of_length_500_folder, txt_file_name)\n",
    "    text_relevant_path = os.path.join(fpath.relevant_text_folder, txt_file_name)\n",
    "\n",
    "    text_tak = \"\"\n",
    "    text_500 = \"\"\n",
    "    text_tak_500 = \"\"\n",
    "    text_relevant = \"\"\n",
    "    \n",
    "    # text_tak\n",
    "    if abstract == \"\":\n",
    "        pass\n",
    "    else:\n",
    "        if title != \"\":\n",
    "            text_tak = text_tak + complete_sent(title) + \" \"\n",
    "        else:\n",
    "            pass  \n",
    "        if abstract != \"\":\n",
    "            text_tak = text_tak + complete_sent(abstract) + \" \"\n",
    "        else:\n",
    "            pass\n",
    "        if keywords != \"\":\n",
    "            text_tak = text_tak + complete_sent(keywords) + \" \"\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        text_tak = plib.process_text(text_tak, lower=True)\n",
    "    \n",
    "    # text_500\n",
    "    if os.path.exists(txt_500_path):\n",
    "        with open(txt_500_path, \"r\", encoding='ascii') as f:\n",
    "            text_500 = f.read()    \n",
    "        text_500 = plib.process_text(text_500, lower=True)\n",
    "    else:\n",
    "        text_500 = \"\"\n",
    "    \n",
    "    # text_relevant\n",
    "    if os.path.exists(text_relevant_path):\n",
    "        with open(text_relevant_path, \"r\", encoding='ascii') as f:\n",
    "            text_relevant = f.read()    \n",
    "        text_relevant = plib.process_text(text_relevant, lower=True)\n",
    "    else:\n",
    "        text_relevant = \"\"\n",
    "    # print(text_relevant)\n",
    "    \n",
    "    # text_tak_500\n",
    "    if text_tak != \"\":\n",
    "        text_tak_500 = text_tak\n",
    "    elif text_500 != \"\":\n",
    "        text_tak_500 = text_500\n",
    "    else:\n",
    "        text_tak_500 = \"\"\n",
    "        \n",
    "    return text_tak_500, text_relevant\n",
    "# # --------------------Start of test code--------------------\n",
    "# index = 0\n",
    "# title, abstract, keywords = get_t_a_k(index)\n",
    "# text_tak_500, text_relevant = get_text(index, title, abstract, keywords)\n",
    "# print(text_tak_500)\n",
    "# print(text_relevant)\n",
    "# # ---------------------End of test code---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main program:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: How does BERT deal with words it unseen before? <br>\n",
    "\n",
    "A: BERT does not provide word-level representations, but subword representations. This implies that when an unseen word is presented to BERT, it will slice it into multiple subwords, even reaching character subwords if needed. That is how it deals with unseen words. Therefore, BERT can handle out-of-vocabulary words. Some other questions and answers in this site can help you with the implementation details of BERT's subword tokenization, e.g. this, this or this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text embedding for testing_set_1000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1000 = fpath.poten_litera_testing_set_1000_labeled\n",
    "df_1000 = pd.read_csv(test_1000, header=0, sep=\",\")\n",
    "\n",
    "# tak embeddings and index list\n",
    "tak_embeddings = []\n",
    "tak_embeddings_index_list = []\n",
    "\n",
    "# relevant text embeddings and index list\n",
    "relevant_text_embeddings = []\n",
    "relevant_text_embeddings_index_list = []\n",
    "\n",
    "for ind in df_1000.index:\n",
    "    # get the text\n",
    "    index = int(df_1000.at[ind, \"INDEX\"])\n",
    "    title, abstract, keywords = get_t_a_k(index)\n",
    "    text_tak_500, text_relevant = get_text(index, title, abstract, keywords)\n",
    "    \n",
    "    if text_tak_500 == text_tak_500 and text_tak_500 != \"\":\n",
    "        tak_embedding = embedd_text(text_tak_500)\n",
    "        tak_embeddings.append(tak_embedding)\n",
    "        tak_embeddings_index_list.append(index)\n",
    "        \n",
    "    if text_relevant == text_relevant and text_relevant != \"\":\n",
    "        relevant_text_embedding = embedd_text(text_relevant)\n",
    "        relevant_text_embeddings.append(relevant_text_embedding)\n",
    "        relevant_text_embeddings_index_list.append(index)\n",
    "\n",
    "# Save the results\n",
    "datasets_folder = fpath.datasets_folder\n",
    "\n",
    "# Save the tak embeddings\n",
    "data_array_tak_embeddings = np.array(tak_embeddings)\n",
    "np.save(os.path.join(datasets_folder, 'tak_embeddings.npy'), data_array_tak_embeddings)    # .npy extension is added if not given\n",
    "with open(os.path.join(datasets_folder, 'tak_embeddings_index_list.txt'), 'w') as f:\n",
    "    for item in tak_embeddings_index_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "# Save the relevant text embeddings     \n",
    "data_array_relevant_text_embeddings = np.array(relevant_text_embeddings)\n",
    "np.save(os.path.join(datasets_folder, 'relevant_text_embeddings.npy'), data_array_relevant_text_embeddings)    # .npy extension is added if not given\n",
    "with open(os.path.join(datasets_folder, 'relevant_text_embeddings_index_list.txt'), 'w') as f:\n",
    "    for item in relevant_text_embeddings_index_list:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text embedding for testing_set_1000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords count transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Iterate tht rows of poten_litera_db_kw_count and perform a function on the number of keywords in each row\n",
    "# input_path = fpath.poten_litera_db_kw_count\n",
    "# df = pd.read_csv(input_path, header=0, sep='\\t')\n",
    "\n",
    "# key_list = list(params.ranking_kw_groups.keys())\n",
    "\n",
    "# count_list_tak = []\n",
    "# count_list_tak_index_list = []\n",
    "# count_list_500 = []\n",
    "# count_list_500_index_list = []\n",
    "# count_list_full_text = []\n",
    "# count_list_full_text_index_list = []\n",
    "\n",
    "# for ind in df.index:\n",
    "#     index = int(df.at[ind, \"INDEX\"])\n",
    "#     # print(index)\n",
    "    \n",
    "#     count_tak = []\n",
    "#     count_500 = []\n",
    "#     count_full_text = []\n",
    "    \n",
    "#     if df.at[ind, \"MACAQUE_COUNT_IN_TAK\"] == df.at[ind, \"MACAQUE_COUNT_IN_TAK\"]:\n",
    "#         count_list_tak_index_list.append(index)\n",
    "#         for key in key_list:\n",
    "#             count_tak.append(int(df.at[ind, key+\"_COUNT_IN_TAK\"]))\n",
    "#         count_list_tak.append(count_tak)\n",
    "    \n",
    "#     if df.at[ind, \"MACAQUE_COUNT_IN_500\"] == df.at[ind, \"MACAQUE_COUNT_IN_500\"]:\n",
    "#         count_list_500_index_list.append(index)\n",
    "#         for key in key_list:\n",
    "#             count_500.append(int(df.at[ind, key+\"_COUNT_IN_500\"]))\n",
    "#         count_list_500.append(count_500)\n",
    "                \n",
    "#     if df.at[ind, \"MACAQUE_COUNT_IN_FULL_TEXT\"] == df.at[ind, \"MACAQUE_COUNT_IN_FULL_TEXT\"]:\n",
    "#         count_list_full_text_index_list.append(index)\n",
    "#         for key in key_list:\n",
    "#             count_full_text.append(int(df.at[ind, key+\"_COUNT_IN_FULL_TEXT\"]))\n",
    "#         count_list_full_text.append(count_full_text)\n",
    "    \n",
    "# # Save the results\n",
    "# datasets_folder = fpath.datasets_folder\n",
    "\n",
    "# # Save the count_list_tak\n",
    "# data_array_count_list_tak = np.array(count_list_tak)\n",
    "# np.save(os.path.join(datasets_folder, 'count_list_tak.npy'), data_array_count_list_tak)    # .npy extension is added if not given\n",
    "# with open(os.path.join(datasets_folder, 'count_list_tak_index_list.txt'), 'w') as f:\n",
    "#     for item in count_list_tak_index_list:\n",
    "#         f.write(\"%s\\n\" % item)\n",
    "\n",
    "# # Save the count_list_500\n",
    "# # for l in count_list_500:\n",
    "# #     print(l)    \n",
    "# data_array_count_list_500 = np.array(count_list_500)\n",
    "# # print(count_list_500)\n",
    "# np.save(os.path.join(datasets_folder, 'count_list_500.npy'), data_array_count_list_500)    # .npy extension is added if not given\n",
    "# with open(os.path.join(datasets_folder, 'count_list_500_index_list.txt'), 'w') as f:\n",
    "#     for item in count_list_500_index_list:\n",
    "#         f.write(\"%s\\n\" % item)\n",
    "        \n",
    "# # Save the count_list_full_text     \n",
    "# data_array_count_list_full_text = np.array(count_list_full_text)\n",
    "# np.save(os.path.join(datasets_folder, 'count_list_full_text.npy'), data_array_count_list_full_text)    # .npy extension is added if not given\n",
    "# with open(os.path.join(datasets_folder, 'count_list_full_text_index_list.txt'), 'w') as f:\n",
    "#     for item in count_list_full_text_index_list:\n",
    "#         f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform_and_save_array(input_file):\n",
    "#     input_file_path = os.path.join(datasets_folder, input_file+'.npy')\n",
    "#     output_file_path = os.path.join(datasets_folder, 'trans_'+input_file + '.npy')\n",
    "    \n",
    "#     # Load the array from the input .npy file\n",
    "#     array = np.load(input_file_path)\n",
    "\n",
    "#     # Apply the transformation\n",
    "#     transformed_array = np.log(np.minimum(array + 1, 10)) / np.log(10)\n",
    "\n",
    "#     # Save the transformed array to the output .npy file\n",
    "#     np.save(output_file_path, transformed_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transform the counts and save the array\n",
    "# trans_count_list_tak = transform_and_save_array('count_list_tak')\n",
    "# trans_count_list_500 = transform_and_save_array('count_list_500')\n",
    "# trans_count_full_text = transform_and_save_array('count_list_full_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def perform_pca(original_array_name, pca_array_name, n_components):\n",
    "    original_path = os.path.join(datasets_folder, original_array_name + '.npy')\n",
    "    array = np.load(original_path)\n",
    "    # print(array)\n",
    "    \n",
    "    # # Standardizing the Data\n",
    "    # scaler = StandardScaler()\n",
    "    # data_scaled = scaler.fit_transform(array)\n",
    "    \n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(array)\n",
    "    \n",
    "    # Transform the data\n",
    "    transformed_data = pca.transform(array)\n",
    "    \n",
    "    # save the pca\n",
    "    pca_file_path = os.path.join(datasets_folder, pca_array_name + '.npy')\n",
    "    np.save(pca_file_path, transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PCA on the tak_embeddings\n",
    "# perform_pca('tak_embeddings', 'pca_tak_embeddings_2', 2)\n",
    "# perform_pca('tak_embeddings', 'pca_tak_embeddings_3', 3)\n",
    "\n",
    "# # PCA on the relevant_text_embeddings\n",
    "# perform_pca('relevant_text_embeddings', 'pca_relevant_text_embeddings_2', 2)\n",
    "# perform_pca('relevant_text_embeddings', 'pca_relevant_text_embeddings_3', 3)\n",
    "\n",
    "# PCA on the count_list_tak\n",
    "perform_pca('count_list_tak', 'pca_count_list_tak_2', 2)\n",
    "perform_pca('count_list_tak', 'pca_count_list_tak_3', 3)\n",
    "\n",
    "# PCA on the count_list_500\n",
    "perform_pca('count_list_500', 'pca_count_list_500_2', 2)\n",
    "perform_pca('count_list_500', 'pca_count_list_500_3', 3)\n",
    "\n",
    "# PCA on the count_list_full_text\n",
    "perform_pca('count_list_full_text', 'pca_count_list_full_text_2', 2)\n",
    "perform_pca('count_list_full_text', 'pca_count_list_full_text_3', 3)\n",
    "\n",
    "# PCA on the trans_count_list_tak\n",
    "perform_pca('trans_count_list_tak', 'pca_trans_count_list_tak_2', 2)\n",
    "perform_pca('trans_count_list_tak', 'pca_trans_count_list_tak_3', 3)\n",
    "\n",
    "# PCA on the trans_count_list_500\n",
    "perform_pca('trans_count_list_500', 'pca_trans_count_list_500_2', 2)\n",
    "perform_pca('trans_count_list_500', 'pca_trans_count_list_500_3', 3)\n",
    "\n",
    "# PCA on the trans_count_list_full_text\n",
    "perform_pca('trans_count_list_full_text', 'trans_pca_count_list_full_text_2', 2)\n",
    "perform_pca('trans_count_list_full_text', 'trans_pca_count_list_full_text_3', 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
