{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib\n",
    "import parameters as params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predefined functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert_model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_sent(sent):\n",
    "    if not (sent.endswith('.') or sent.endswith('?') or sent.endswith('!')):\n",
    "        sent += '.'\n",
    "    \n",
    "    return sent\n",
    "# # --test--\n",
    "# text = \"distribution of the dopamine innervation in the macaque and human thalamus,\"\n",
    "# complete_sentence = complete_sent(text)\n",
    "# print(complete_sentence)\n",
    "# # --test--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_short_segments(text, max_tokens=400):\n",
    "    # Split text into sentences\n",
    "    text = plib.process_text(text, lower=True)\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Group sentences into segments\n",
    "    segments = []\n",
    "    current_segment = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = complete_sent(sentence)\n",
    "        # Check token count if this sentence is added\n",
    "        potential_segment = (current_segment + \" \" + sentence).strip()\n",
    "        tokens = bert_tokenizer.tokenize(potential_segment)\n",
    "        if len(tokens) > max_tokens:\n",
    "            # If limit exceeded, start a new segment\n",
    "            segments.append(current_segment.strip())\n",
    "            current_segment = sentence\n",
    "        else:\n",
    "            # Otherwise, add sentence to current segment\n",
    "            current_segment = potential_segment\n",
    "    \n",
    "    # Add the last segment if it's not empty\n",
    "    if current_segment:\n",
    "        segments.append(current_segment.strip())\n",
    "\n",
    "    return segments\n",
    "# # --test--\n",
    "# text = \"distribution of the dopamine innervation in the macaque and human thalamus. fax: +34 91 497 53 15. we recently defined the thalamic dopaminergic system in primates; it arises from numerous dopaminergic cell groups and selectively targets numerous thalamic nuclei. given the central position of the thalamus in subcortical and cortical interplay, and the functional relevance of dopamine neuromodulation in the brain, detailing dopamine distribution in the thalamus should supply important information. to this end we performed immunohistochemistry for dopamine and the dopamine transporter in the thalamus of macaque monkeys and humans to generate maps, in the stereotaxic coronal plane, of the distribution of dopaminergic axons. the dopamine innervation of the thalamus follows the same pattern in both species and is most dense in midline limbic nuclei, the mediodorsal and lateral posterior association nuclei, and in the ventral lateral and ventral anterior motor nuclei. this distribution suggests that thalamic dopamine has a prominent role in emotion, attention, cognition and complex somatosensory and visual processing, as well as in motor control. most thalamic dopaminergic axons are thin and varicose and target both the neuropil and small blood vessels, suggesting that, besides neuronal modulation, thalamic dopamine may have a direct influence on microcirculation. the maps provided here should be a useful reference in future experimental and neuroimaging studies aiming at clarifying the role of the thalamic dopaminergic system in health and in conditions involving brain dopamine, including parkinsons disease, drug addiction and schizophrenia. keywords dopamine thalamus monkey human primate dopamine transporter parkinson schizophrenia addiction introduction the thalamus is made up of multiple nuclei relaying information from subcortical centers or from other cortices to the cerebral cortex (sherman and guillery, 2005), as well as the striatum, the nucleus accumbens and the amygdala (steriade et al., 1997). in addition to specific subcortical and cortical afferents, the primate thalamus receives axons containing the neuromodulators acetylcholine (heckers et al., 1992), histamine (manning et al., 1996), serotonin (morrison and foote, 1986; lavoie and parent, 1991), and the catecholamines adrenaline (rico and cavada, 1998a), noradrenaline (morrison and foote, 1986; ginsberg et al., 1993) and dopamine (snchez-gonzlez et al., 2005). until recently, the existence of significant dopamine innervation in the primate thalamus has been largely ignored, probably because dopamine innervation of the rodent thalamus is very scant (groenewegen, 1988; papadopoulos and parnavelas, 1990). however, fragmentary data scattered through the literature endorse the presence of dopamine innervation in the primate thalamus. postmortem biochemical studies showed the presence of dopamine in the thalamus of macaques (brown et al., 1979; goldman-rakic and brown, 1981; pifl et al., 1990, 1991) and human subjects (oke and adams, 1987). later, receptor binding and in situ hybridization analyses detected the presence of dopamine d2-like (joyce et al., 1991; kessler et al., 1993; hall et al., 1996; langer et al., 1999; rieck et al., 2004) and d3-like receptors (gurevich and joyce, 1999) in several human thalamic nuclei. positron emission tomography (pet) radioligand studies have also demonstrated the presence of the dopamine transporter (dat) (wang et al., 1995; halldin et al., 1996; helfenbein et al., 1999; brownell et al., 2003) and of d2-like receptors (farde et al., 1997; langer et al., 1999; okubo et al., 1999; brownell et al., 2003; rieck et al., 2004) in the human and macaque thalamus. in the course of pet studies focusing on schizophrenia, d2- and d3-like radioligand binding was also found in the thalamus of control subjects (talvik et al., 2003; yasuno et al., 2004). finally, an immunohistochemical study using anti-dat antibodies detected the presence of dopaminergic axons in the mediodorsal nucleus (md) of the macaque thalamus (melchitzky and lewis, 2001).\"\n",
    "# segments = split_text_into_short_segments(text, max_tokens=400)\n",
    "# for i, segment in enumerate(segments):\n",
    "#     print(i, segment)\n",
    "# # --test--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text_segment(text_segment):\n",
    "    tokens = bert_tokenizer(text_segment, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**tokens)\n",
    "        text_segment_embedding = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "    \n",
    "    return text_segment_embedding\n",
    "# # --test--\n",
    "# text_segment = \"distribution of the dopamine innervation in the macaque and human thalamus. fax: +34 91 497 53 15. we recently defined the thalamic dopaminergic system in primates; it arises from numerous dopaminergic cell groups and selectively targets numerous thalamic nuclei. given the central position of the thalamus in subcortical and cortical interplay, and the functional relevance of dopamine neuromodulation in the brain, detailing dopamine distribution in the thalamus should supply important information.\"\n",
    "# text_segment_embedding = embed_text_segment(text_segment)\n",
    "# print(text_segment_embedding.shape)\n",
    "# print(text_segment_embedding)\n",
    "# # --test--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedd_text(text):\n",
    "    # Preprocess the text\n",
    "    text = plib.process_text(text, lower=True)\n",
    "    # Split the text into sentences\n",
    "    shorter_text_segements = split_text_into_short_segments(text, max_tokens=400)  # You may need to use a more robust sentence tokenizer\n",
    "\n",
    "    # Initialize a list to store sentence embeddings\n",
    "    text_segment_embeddings = []\n",
    "\n",
    "    # Tokenize and embed each sentence\n",
    "    for text_segment in shorter_text_segements:\n",
    "        embedding = embed_text_segment(text_segment)\n",
    "        text_segment_embeddings.append(embedding)\n",
    "\n",
    "    # Average pooling to obtain a single vector for the entire document\n",
    "    text_embedding = torch.mean(torch.stack(text_segment_embeddings), dim=0)\n",
    "    \n",
    "    return text_embedding\n",
    "# # --test--\n",
    "# text = \"distribution of the dopamine innervation in the macaque and human thalamus. fax: +34 91 497 53 15. we recently defined the thalamic dopaminergic system in primates; it arises from numerous dopaminergic cell groups and selectively targets numerous thalamic nuclei. given the central position of the thalamus in subcortical and cortical interplay, and the functional relevance of dopamine neuromodulation in the brain, detailing dopamine distribution in the thalamus should supply important information.\"\n",
    "# text_embedding = embedd_text(text)\n",
    "# print(text_embedding.shape)\n",
    "# print(text_embedding)\n",
    "# # --test--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_t_a_k(index):\n",
    "    db_path = fpath.poten_litera_db\n",
    "    df = pd.read_csv(db_path, header=0, sep=\"\\t\")\n",
    "    \n",
    "    # locate the title in the row where column INDEX has the value index\n",
    "    title = df.loc[df[\"INDEX\"].astype(int) == index, \"TITLE\"].values[0]\n",
    "    abstract = df.loc[df[\"INDEX\"].astype(int) == index, \"ABSTRACT\"].values[0]\n",
    "    keywords = df.loc[df[\"INDEX\"].astype(int) == index, \"KEYWORDS\"].values[0]\n",
    "    \n",
    "    if title != title:\n",
    "        title = \"\"\n",
    "        \n",
    "    if abstract != abstract:\n",
    "        abstract = \"\"\n",
    "        \n",
    "    if keywords != keywords:\n",
    "        keywords = \"\"\n",
    "    \n",
    "    return title, abstract, keywords\n",
    "# # --------------------Start of test code--------------------\n",
    "# index = 2\n",
    "# title, abstract, keywords = get_t_a_k(index)\n",
    "# print(title)\n",
    "# print(abstract)\n",
    "# print(keywords)\n",
    "# # ---------------------End of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(index, title, abstract, keywords):\n",
    "    txt_file_name = str(index) + \".txt\"\n",
    "    txt_500_path = os.path.join(fpath.processed_texts_of_length_500_folder, txt_file_name)\n",
    "    text_relevant_path = os.path.join(fpath.relevant_text_folder, txt_file_name)\n",
    "\n",
    "    text_tak = \"\"\n",
    "    text_500 = \"\"\n",
    "    text_tak_500 = \"\"\n",
    "    text_relevant = \"\"\n",
    "    \n",
    "    # text_tak\n",
    "    if abstract == \"\":\n",
    "        pass\n",
    "    else:\n",
    "        if title != \"\":\n",
    "            text_tak = text_tak + complete_sent(title) + \" \"\n",
    "        else:\n",
    "            pass  \n",
    "        if abstract != \"\":\n",
    "            text_tak = text_tak + complete_sent(abstract) + \" \"\n",
    "        else:\n",
    "            pass\n",
    "        if keywords != \"\":\n",
    "            text_tak = text_tak + complete_sent(keywords) + \" \"\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        text_tak = plib.process_text(text_tak, lower=True)\n",
    "    \n",
    "    # text_500\n",
    "    if os.path.exists(txt_500_path):\n",
    "        with open(txt_500_path, \"r\", encoding='ascii') as f:\n",
    "            text_500 = f.read()    \n",
    "        text_500 = plib.process_text(text_500, lower=True)\n",
    "    else:\n",
    "        text_500 = \"\"\n",
    "    \n",
    "    # text_relevant\n",
    "    if os.path.exists(text_relevant_path):\n",
    "        with open(text_relevant_path, \"r\", encoding='ascii') as f:\n",
    "            text_relevant = f.read()    \n",
    "        text_relevant = plib.process_text(text_relevant, lower=True)\n",
    "    else:\n",
    "        text_relevant = \"\"\n",
    "    # print(text_relevant)\n",
    "    \n",
    "    # text_tak_500\n",
    "    if text_tak != \"\":\n",
    "        text_tak_500 = text_tak\n",
    "    elif text_500 != \"\":\n",
    "        text_tak_500 = text_500\n",
    "    else:\n",
    "        text_tak_500 = \"\"\n",
    "        \n",
    "    return text_tak_500, text_relevant\n",
    "# # --------------------Start of test code--------------------\n",
    "# index = 0\n",
    "# title, abstract, keywords = get_t_a_k(index)\n",
    "# text_tak_500, text_relevant = get_text(index, title, abstract, keywords)\n",
    "# print(text_tak_500)\n",
    "# print(text_relevant)\n",
    "# # ---------------------End of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embedding(data_array, index_list, name, save_path):\n",
    "    np.save(os.path.join(save_path, name+'.npy'), data_array)    # .npy extension is added if not given\n",
    "    with open(os.path.join(save_path, name+'_index_list.txt'), 'w') as f:\n",
    "        for index, item in enumerate(index_list):\n",
    "            f.write(f\"{item}\")\n",
    "            if index < len(index_list) - 1:\n",
    "                f.write(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_count_list_and_save(input_file_name):\n",
    "    save_path = fpath.embedding_and_pca_folder\n",
    "    input_file_path = os.path.join(save_path, input_file_name+'.npy')\n",
    "    output_file_path = os.path.join(save_path, 'trans_'+input_file_name+ '.npy')\n",
    "    \n",
    "    # Load the array from the input .npy file\n",
    "    array = np.load(input_file_path)\n",
    "\n",
    "    # Apply the transformation\n",
    "    transformed_array = np.log10(np.minimum(array + 1, 10))\n",
    "\n",
    "    # Save the transformed array to the output .npy file\n",
    "    np.save(output_file_path, transformed_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main program:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: How does BERT deal with words it unseen before? <br>\n",
    "\n",
    "A: BERT does not provide word-level representations, but subword representations. This implies that when an unseen word is presented to BERT, it will slice it into multiple subwords, even reaching character subwords if needed. That is how it deals with unseen words. Therefore, BERT can handle out-of-vocabulary words. Some other questions and answers in this site can help you with the implementation details of BERT's subword tokenization, e.g. this, this or this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text embedding for testing_set_1000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_1000 = fpath.poten_litera_testing_set_1000_labeled\n",
    "# df_1000 = pd.read_csv(test_1000, header=0, sep=\",\")\n",
    "\n",
    "# # tak_embeddings_1000 and tak_embeddings_index_list_1000\n",
    "# tak_500_embeddings_1000 = []\n",
    "# tak_500_embeddings_index_list_1000 = []\n",
    "\n",
    "# # relevant_text_embeddings_1000 and relevant_text_embeddings_index_list_1000\n",
    "# relevant_text_embeddings_1000 = []\n",
    "# relevant_text_embeddings_index_list_1000 = []\n",
    "\n",
    "# for ind in df_1000.index:\n",
    "#     # get the text\n",
    "#     index = int(df_1000.at[ind, \"INDEX\"])\n",
    "#     title, abstract, keywords = get_t_a_k(index)\n",
    "#     text_tak_500, text_relevant = get_text(index, title, abstract, keywords)\n",
    "    \n",
    "#     # embed text_tak_500\n",
    "#     if text_tak_500 == text_tak_500 and text_tak_500 != \"\":\n",
    "#         embedding = embedd_text(text_tak_500)\n",
    "#         tak_500_embeddings_1000.append(embedding)\n",
    "#         tak_500_embeddings_index_list_1000.append(index)\n",
    "    \n",
    "#     # embed relevant_text\n",
    "#     if text_relevant == text_relevant and text_relevant != \"\":\n",
    "#         embedding = embedd_text(text_relevant)\n",
    "#         relevant_text_embeddings_1000.append(embedding)\n",
    "#         relevant_text_embeddings_index_list_1000.append(index)\n",
    "\n",
    "# # Save the results\n",
    "# save_path = fpath.embedding_and_pca_folder\n",
    "# # Save the tak_embeddings_1000\n",
    "# save_embedding(tak_500_embeddings_1000, tak_500_embeddings_index_list_1000, 'tak_500_embeddings_1000', save_path)\n",
    "# # Save the relevant_text_embeddings_1000\n",
    "# save_embedding(relevant_text_embeddings_1000, relevant_text_embeddings_index_list_1000, 'relevant_text_embeddings_1000', save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text embedding for poten_litera_db.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_path = fpath.poten_litera_db\n",
    "# df_db = pd.read_csv(db_path, header=0, sep=\"\\t\")\n",
    "\n",
    "# # tak embeddings and index list\n",
    "# tak_embeddings_db = []\n",
    "# tak_embeddings_index_list_db = []\n",
    "\n",
    "# # relevant text embeddings and index list\n",
    "# relevant_text_embeddings_db = []\n",
    "# relevant_text_embeddings_index_list_db = []\n",
    "\n",
    "# for ind in df_db.index:\n",
    "#     # get the text\n",
    "#     index = int(df_db.at[ind, \"INDEX\"])\n",
    "#     title, abstract, keywords = get_t_a_k(index)\n",
    "#     text_tak_500, text_relevant = get_text(index, title, abstract, keywords)\n",
    "    \n",
    "#     if text_tak_500 == text_tak_500 and text_tak_500 != \"\":\n",
    "#         embedding = embedd_text(text_tak_500)\n",
    "#         tak_embeddings_db.append(embedding)\n",
    "#         tak_embeddings_index_list_db.append(index)\n",
    "        \n",
    "#     if text_relevant == text_relevant and text_relevant != \"\":\n",
    "#         embedding = embedd_text(text_relevant)\n",
    "#         relevant_text_embeddings_db.append(embedding)\n",
    "#         relevant_text_embeddings_index_list_db.append(index)\n",
    "\n",
    "# # Save the results\n",
    "# save_path = fpath.embedding_and_pca_folder\n",
    "# # Save the tak_embeddings_db\n",
    "# save_embedding(tak_embeddings_db, tak_embeddings_index_list_db, 'tak_embeddings_db', save_path)\n",
    "# # Save the relevant_text_embeddings_db     \n",
    "# save_embedding(relevant_text_embeddings_db, relevant_text_embeddings_index_list_db, 'relevant_text_embeddings_db', save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords count transformation for testing_set_1000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_count_path = fpath.poten_litera_db_kw_count\n",
    "# df_count = pd.read_csv(db_count_path, header=0, sep='\\t')\n",
    "\n",
    "# test_1000_path = fpath.poten_litera_testing_set_1000_labeled\n",
    "# df_1000 = pd.read_csv(test_1000_path, header=0, sep=\",\")\n",
    "\n",
    "# key_list = list(params.ranking_kw_groups.keys())\n",
    "\n",
    "# count_list_tak_1000 = []\n",
    "# count_list_tak_index_list_1000 = []\n",
    "# count_list_500_1000 = []\n",
    "# count_list_500_index_list_1000 = []\n",
    "# count_list_full_text_1000 = []\n",
    "# count_list_full_text_index_list_1000 = []\n",
    "\n",
    "# for ind in df_1000.index:\n",
    "#     index = int(df_1000.at[ind, \"INDEX\"])\n",
    "    \n",
    "#     # Get the ind in the df_count with the same index in the \"INDEX\" column\n",
    "#     ind_in_df_count = df_count.index[df_count[\"INDEX\"].astype(int) == index].tolist()[0]\n",
    "    \n",
    "#     # Get count_list_tak\n",
    "#     count_tak = []\n",
    "#     if df_count.at[ind_in_df_count, \"MACAQUE_COUNT_IN_TAK\"] == df_count.at[ind_in_df_count, \"MACAQUE_COUNT_IN_TAK\"]:\n",
    "#         count_list_tak_index_list_1000.append(index)\n",
    "#         for key in key_list:\n",
    "#             count_tak.append(int(df_count.at[ind_in_df_count, key+\"_COUNT_IN_TAK\"]))\n",
    "#         count_list_tak_1000.append(count_tak)\n",
    "    \n",
    "#     # Get count_list_500\n",
    "#     count_500 = []\n",
    "#     if df_count.at[ind_in_df_count, \"MACAQUE_COUNT_IN_500\"] == df_count.at[ind_in_df_count, \"MACAQUE_COUNT_IN_500\"]:\n",
    "#         count_list_500_index_list_1000.append(index)\n",
    "#         for key in key_list:\n",
    "#             count_500.append(int(df_count.at[ind_in_df_count, key+\"_COUNT_IN_500\"]))\n",
    "#         count_list_500_1000.append(count_500)\n",
    "\n",
    "#     # Get count_list_full_text\n",
    "#     count_full_text = []      \n",
    "#     if df_count.at[ind_in_df_count, \"MACAQUE_COUNT_IN_FULL_TEXT\"] == df_count.at[ind_in_df_count, \"MACAQUE_COUNT_IN_FULL_TEXT\"]:\n",
    "#         count_list_full_text_index_list_1000.append(index)\n",
    "#         for key in key_list:\n",
    "#             count_full_text.append(int(df_count.at[ind_in_df_count, key+\"_COUNT_IN_FULL_TEXT\"]))\n",
    "#         count_list_full_text_1000.append(count_full_text)\n",
    "    \n",
    "# # Save the results\n",
    "# save_path = fpath.embedding_and_pca_folder\n",
    "# # Save the count_list_tak_1000\n",
    "# count_list_tak_1000_array = np.array(count_list_tak_1000)\n",
    "# save_embedding(count_list_tak_1000_array, count_list_tak_index_list_1000, 'count_list_tak_1000', save_path)\n",
    "# # Save the count_list_500_1000\n",
    "# count_list_500_1000_array = np.array(count_list_500_1000)\n",
    "# save_embedding(count_list_500_1000_array, count_list_500_index_list_1000, 'count_list_500_1000', save_path)    \n",
    "# # Save the count_list_full_text_1000\n",
    "# count_list_full_text_1000_array = np.array(count_list_full_text_1000)\n",
    "# save_embedding(count_list_full_text_1000_array, count_list_full_text_index_list_1000, 'count_list_full_text_1000', save_path)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords count transformation for poten_litera_db.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_path = fpath.poten_litera_db_kw_count\n",
    "# df_count = pd.read_csv(input_path, header=0, sep='\\t')\n",
    "\n",
    "# key_list = list(params.ranking_kw_groups.keys())\n",
    "\n",
    "# count_list_tak_db = []\n",
    "# count_list_tak_index_list_db = []\n",
    "# count_list_500_db = []\n",
    "# count_list_500_index_list_db = []\n",
    "# count_list_full_text_db = []\n",
    "# count_list_full_text_index_list_db = []\n",
    "\n",
    "# for ind in df_count.index:\n",
    "#     index = int(df_count.at[ind, \"INDEX\"])\n",
    "    \n",
    "#     # Get count_list_tak\n",
    "#     count_tak = []\n",
    "#     if df_count.at[ind, \"MACAQUE_COUNT_IN_TAK\"] == df_count.at[ind, \"MACAQUE_COUNT_IN_TAK\"]:\n",
    "#         count_list_tak_index_list_db.append(index)\n",
    "#         for key in key_list:\n",
    "#             count_tak.append(int(df_count.at[ind, key+\"_COUNT_IN_TAK\"]))\n",
    "#         count_list_tak_db.append(count_tak)\n",
    "    \n",
    "#     # Get count_list_500\n",
    "#     count_500 = []\n",
    "#     if df_count.at[ind, \"MACAQUE_COUNT_IN_500\"] == df_count.at[ind, \"MACAQUE_COUNT_IN_500\"]:\n",
    "#         count_list_500_index_list_db.append(index)\n",
    "#         for key in key_list:\n",
    "#             count_500.append(int(df_count.at[ind, key+\"_COUNT_IN_500\"]))\n",
    "#         count_list_500_db.append(count_500)\n",
    "\n",
    "#     # Get count_list_full_text\n",
    "#     count_full_text = []      \n",
    "#     if df_count.at[ind, \"MACAQUE_COUNT_IN_FULL_TEXT\"] == df_count.at[ind, \"MACAQUE_COUNT_IN_FULL_TEXT\"]:\n",
    "#         count_list_full_text_index_list_db.append(index)\n",
    "#         for key in key_list:\n",
    "#             count_full_text.append(int(df_count.at[ind, key+\"_COUNT_IN_FULL_TEXT\"]))\n",
    "#         count_list_full_text_db.append(count_full_text)\n",
    "    \n",
    "# # Save the results\n",
    "# save_path = fpath.embedding_and_pca_folder\n",
    "# # Save the count_list_tak_db\n",
    "# count_list_tak_db_array = np.array(count_list_tak_db)\n",
    "# save_embedding(count_list_tak_db_array, count_list_tak_index_list_db, 'count_list_tak_db', save_path)\n",
    "# # Save the count_list_500_db\n",
    "# count_list_500_db_array = np.array(count_list_500_db)\n",
    "# save_embedding(count_list_500_db_array, count_list_500_index_list_db, 'count_list_500_db', save_path)    \n",
    "# # Save the count_list_full_text_db\n",
    "# count_list_full_text_db_array = np.array(count_list_full_text_db)\n",
    "# save_embedding(count_list_full_text_db_array, count_list_full_text_index_list_db, 'count_list_full_text_db', save_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transform the counts and save the array\n",
    "# transform_count_list_and_save('count_list_tak')\n",
    "# transform_count_list_and_save('count_list_500')\n",
    "# transform_count_list_and_save('count_list_full_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# def perform_pca_and_save(original_array_name, n_components):\n",
    "#     save_path = fpath.embedding_and_pca_folder\n",
    "#     original_path = os.path.join(save_path, original_array_name + '.npy')\n",
    "#     pca_file_path = os.path.join(save_path, 'pca_' + original_array_name + str(n_components) + '.npy')\n",
    "    \n",
    "#     array = np.load(original_path)\n",
    "    \n",
    "#     # Standardizing the Data\n",
    "#     scaler = StandardScaler()\n",
    "#     data_scaled = scaler.fit_transform(array)\n",
    "    \n",
    "#     # PCA model and fit\n",
    "#     pca = PCA(n_components=n_components)\n",
    "#     pca.fit(array)\n",
    "    \n",
    "#     # Transform the data\n",
    "#     transformed_data = pca.transform(array)\n",
    "    \n",
    "#     # save the pca\n",
    "#     np.save(pca_file_path, transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def perform_pca_data_list(data_list, n_components):\n",
    "#     for data_name in data_list:\n",
    "#         perform_pca_and_save(data_name, n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_list = [\n",
    "#     'tak_500_embeddings_1000', \n",
    "#     'relevant_text_embeddings_1000',\n",
    "     \n",
    "#     'tak_embeddings_db', \n",
    "#     'relevant_text_embeddings_db', \n",
    "    \n",
    "#     'count_list_tak_1000', \n",
    "#     'count_list_500_1000', \n",
    "#     'count_list_full_text_1000', \n",
    "    \n",
    "#     'trans_count_list_tak_1000',\n",
    "#     'trans_count_list_500_1000',\n",
    "#     'trans_count_list_full_text_1000',\n",
    "    \n",
    "#     'count_list_tak_db', \n",
    "#     'count_list_500_db', \n",
    "#     'count_list_full_text_db', \n",
    "    \n",
    "#     'trans_count_list_tak_db',\n",
    "#     'trans_count_list_500_db',\n",
    "#     'trans_count_list_full_text_db'\n",
    "#     ]\n",
    "# perform_pca_data_list('data_list', 2)\n",
    "# perform_pca_data_list('data_list', 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
