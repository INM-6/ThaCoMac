{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96a329a8-d411-429f-a9d3-8ed95a517c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import internal .py files\n",
    "import file_path_management as fpath\n",
    "import public_library as plib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "099606bd-fea9-49db-884c-a047de5621f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all the file paths in file_path_management.py\n",
    "# project_folder\n",
    "# gs_poten_urls\n",
    "# wos_poten_urls\n",
    "# pubmed_pmc_poten_urls\n",
    "# eupmc_poten_urls\n",
    "# path_poten_csv\n",
    "# path_related_urls\n",
    "# path_related_csv\n",
    "# pdf_folder_path\n",
    "# seed_paper_urls\n",
    "# connec_db_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37eb8cd1-594b-4af8-aff3-cc6a9c91b488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed2a1fb-4380-4179-bbdf-8192056b8b4c",
   "metadata": {},
   "source": [
    "In the next cell, we present all parameters that might have an effect on the search results, including:<br>\n",
    "1. searching keyword lexicon\n",
    "2. academic databases\n",
    "3. initial urls when searching academic databases\n",
    "4. seed paper list for spanning citations\n",
    "5. conenctome database\n",
    "6. seaching queries of the connectome database\n",
    "7. on-topic keyword lexicon\n",
    "8. weights of on-topic keywords when calculating relatedness of a literature\n",
    "9. ChatGPT queries for relatedness of topic\n",
    "10. meta categories when extracting information of related literature\n",
    "11. keywords for searching meta categories\n",
    "12. ChatGPT queries for extracting information of meta categories of related literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b6b2882-23e5-4670-8359-3d7367e1107b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "# searching keyword lexicon\n",
    "search_kws_lexicon = 'macaque AND (thalamus OR thalamocortical OR thalamo-cortical)' # in all fields\n",
    "\n",
    "# academic databases\n",
    "# Google Scholar: 'https://scholar.google.com/'\n",
    "# 78300 results\n",
    "# Web of Science: 'https://www.webofscience.com/wos/woscc/advanced-search' # can be exported to excel file\n",
    "# 961 results\n",
    "# PubMed Central PMC: 'https://pubmed.ncbi.nlm.nih.gov/advanced/' # can be exported to .csv file and abstract.txt file\n",
    "# 2448 results\n",
    "# Europe PMC = 'https://europepmc.org/advancesearch' # can be exported to .csv file or abstract and full open access file .xml\n",
    "# 5129 results\n",
    "acad_dbs = ['Google Scholar', 'Web of Science', 'PubMed_Central_PMC', 'Europe_PMC']\n",
    "\n",
    "# initial urls for specified searching keyword lexicon and all academic databases\n",
    "init_urls = {\n",
    "    'gs': 'https://scholar.google.com/scholar?start=0&q=macaque+thalamus+OR+thalamocortical+OR+thalamo-cortical&hl=en&as_sdt=1,5',\n",
    "    'wos': 'https://www.webofscience.com/wos/woscc/summary/79530a3c-47d5-4dd0-9b7d-b1d92fd11882-98d8472a/relevance/1',\n",
    "    'pubmed': 'https://pubmed.ncbi.nlm.nih.gov/?term=(((thalamus)%20OR%20(thalamocortical))%20OR%20(thalamo-cortical))%20AND%20(macaque)&sort=&page=1',\n",
    "    'eupmc': 'https://europepmc.org/search?query=%28%22macaque%22%20AND%20%28%22thalamus%22%20OR%20%22thalamocortical%22%20OR%20%22thalamo-cortical%22%29%20%29%20AND%20%28LANG%3A%22eng%22%20OR%20LANG%3A%22en%22%20OR%20LANG%3A%22us%22%29&page=1'\n",
    "}\n",
    "\n",
    "# seed papers specification\n",
    "seed_papers = []\n",
    "\n",
    "# connectome database and queries specification\n",
    "# we search the CoCoMac\n",
    "connec_db = ''\n",
    "connec_db_quries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8ee718e-e776-44d4-ba81-7f24ef51c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_google_scholar(init_url, headers):\n",
    "    # create a .txt file to record the urls of google scholar search results, clear the file if already exists\n",
    "    f = open(fpath.gs_poten_urls, 'w')\n",
    "    f.truncate()\n",
    "    f.close()\n",
    "\n",
    "    # request the first page and extract the number of pages of the search results\n",
    "    first_page = init_url\n",
    "    response = requests.get(first_page, headers = headers)\n",
    "    soup = BeautifulSoup(response.content,'lxml')\n",
    "    # print(soup)\n",
    "    num_results_str = soup.find_all('div', {'class': 'gs_ab_mdw'})[1].get_text().split()[1]\n",
    "    # print(num_results_str)\n",
    "    print(int(num_results_str))\n",
    "    num_results = int(re.sub(r'[^\\w\\s]', '', num_results_str))\n",
    "    pages = int(num_results/10)\n",
    "    print(pages)\n",
    "    \n",
    "    # iterate all pages and record the results\n",
    "    pages = 5\n",
    "    for page in range(pages):\n",
    "        time.sleep(random.randint(1, 10))\n",
    "        start = page * 10\n",
    "        # google scholar\n",
    "        page_url = init_url.split('?start=')[0] + '?start=' + str(start) + '&q=' + init_url.split('?start=')[1].split('&q=')[1]\n",
    "        # print(page_url)\n",
    "        # search a page\n",
    "        response = requests.get(page_url, headers = headers)\n",
    "        soup = BeautifulSoup(response.content,'lxml')\n",
    "        # print(soup)\n",
    "        print(soup.select('[data-lid]')) \n",
    "        for item in soup.find_all('[data-lid]'):\n",
    "            print(item)\n",
    "            add_url = item.find_all('h3')[0].find_all('a', href = True)[0]['href']\n",
    "            print(add_url)\n",
    "            try: \n",
    "                with open(FPM.gs_poten_urls, 'a') as url_file:\n",
    "                    # append text at the end of file\n",
    "                    url_file.write(f'{add_url}\\n')\n",
    "            except Exception as e: \n",
    "                print(\"Error when trying to write in google_scholar_poten_urls.txt\")\n",
    "                raise e\n",
    "    print(\"Searching Google Scholar complated!\")\n",
    "\n",
    "def search_webofscience(init_url, headers):\n",
    "    print(\"Searching Web of Science complated!\")\n",
    "\n",
    "def search_PubMed_Central_PMC(iinit_url, headers):\n",
    "    print(\"Searching PubMedd Central PMC complated!\")\n",
    "\n",
    "def search_Europe_PMC(init_url, headers):\n",
    "    print(\"Searching Europe PMC complated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72ddc49e-446e-4c4b-a8a7-a79b1335039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search academic databases, record the urls as a line in a .txt file from the webpages\n",
    "def search_acad_dbs(acad_dbs, init_urls, headers, proxy):\n",
    "    for acad_db in acad_dbs:\n",
    "        if acad_db == 'Google Scholar':\n",
    "            print(\"Searching Google Scholar...\")\n",
    "            search_google_scholar(init_urls['gs'], headers)\n",
    "        elif acad_db == 'Web of Science':\n",
    "            print(\"Searching Web of Science...\")\n",
    "            search_webofscience(init_urls['wos'], headers)\n",
    "        elif acad_db == 'PubMed_Central_PMC':\n",
    "            print(\"Searching PubMed Central PMC...\")\n",
    "            search_PubMed_Central_PMC(init_urls['pubmed'], headers)\n",
    "        elif acad_db == 'Europe_PMC':\n",
    "            print(\"Searching Europe PMC...\")\n",
    "            search_Europe_PMC(init_urls['pubmed'], headers)\n",
    "        else:\n",
    "            print(\"The specified academic database: \" + acad_db + \" is not supported by this function.\")\n",
    "            print(\"Plese choose one of the following databases:\",)\n",
    "            for db in ['Google Scholar', 'Web of Science', 'PubMed_Central_PMC', 'Europe_PMC']:\n",
    "                print(db)\n",
    "        \n",
    "        # for page in range(pages):\n",
    "        #     time.sleep(2)\n",
    "        #     start = page * 10\n",
    "        #     # google scholar\n",
    "        #     url = 'https://scholar.google.com/scholar?start=' + str(start) + '&q=macaque+thalamus+OR+thalamocortical+OR+thalamo-cortical&hl=en&as_sdt=1,5'\n",
    "        #     # pubmed\n",
    "        #     url = 'https://pubmed.ncbi.nlm.nih.gov/?term=macaque%20AND%20(thalamus%20OR%20cortex%20OR%20thalamocortical%20OR%20thalamo-cortical%20or%20corticothalamic%20OR%20cortico-thalamic)&page=1\n",
    "        #     response = requests.get(url,headers = headers)\n",
    "        #     # print(url)\n",
    "        #     soup = BeautifulSoup(response.content,'lxml') \n",
    "        #     #print(soup.select('[data-lid]')) \n",
    "        #     for item in soup.select('[data-lid]'): \n",
    "        #         try: \n",
    "        #             # print('----------------------------------------') \n",
    "        #             # print(item)  \n",
    "        #             # print(item.select('h3')[0])\n",
    "        #             with open(path_urls, 'a+') as url_file:\n",
    "        #                 url_file.seek(0)\n",
    "        #                 # If file is not empty then append '\\n'\n",
    "        #                 data = url_file.read(100)\n",
    "        #                 if len(data) > 0 :\n",
    "        #                     url_file.write('\\n')\n",
    "        #                     # Append text at the end of file\n",
    "        #                 url_file.write('----------------------------------------\\n')\n",
    "        #                 url_file.write(item.select('h3')[0].get_text())\n",
    "        #                 url_file.write('\\n')\n",
    "        #                 # print(item.select('h3')[0].get_text())\n",
    "        #                 for a in item.select('h3')[0].find_all('a', href=True):\n",
    "        #                     # print(a['href'])\n",
    "        #                     url_file.write(a['href'])\n",
    "        #                     url_file.write('\\n')\n",
    "        #                     # print(item.select('a'))\n",
    "        #                     # print(\"PDF link:\")\n",
    "        #                 url_file.write(item.select('a')[0]['href'])\n",
    "        #                 url_file.write('\\n')\n",
    "        #                 # print(item.select('a')[0]['href'])\n",
    "        #                 # print(item.select('.gs_rs')[0].get_text()) \n",
    "        #                 # print('----------------------------------------') \n",
    "        #         except Exception as e: \n",
    "        #             #raise e \n",
    "        #             print('')\n",
    "\n",
    "def span_citations(seed_papers, num_span_time, headers, proxy):\n",
    "    None\n",
    "    \n",
    "def search_conne_db(connec_db, connec_db_quries):\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e23fec62-fd3a-49c4-987c-143a20f524f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_search_results(headers):\n",
    "    # process gs_poten_urls\n",
    "    with open(FPM.gs_poten_urls, 'r') as file:\n",
    "        lines = []\n",
    "        for line in file:\n",
    "            print(line)\n",
    "            line = line.strip()\n",
    "            lines.append(line)\n",
    "    print(len(lines))\n",
    "    doi_list = []\n",
    "    for url in lines:\n",
    "        response = requests.get(url, headers = headers)\n",
    "        soup = BeautifulSoup(response.content,'lxml')\n",
    "        # print(soup)\n",
    "        num_results_str = soup.find_all('a', href = True)\n",
    "        for href in num_results_str:\n",
    "            if '//doi.org/' in href:\n",
    "                doi_list.append(href)\n",
    "    doi_df = pd.DataFrame({'DOI': doi_list})\n",
    "    PL.clear_file(fpath.path_poten_csv)\n",
    "    doi_df.to_csv(fpath.path_poten_csv, index=False)\n",
    "    \n",
    "    # process wos_poten_urls\n",
    "    doi_df = pd.read_csv(fpath.wos_poten_urls, sep=';')\n",
    "    # print(doi_df.columns)\n",
    "    # print(doi_df.head())\n",
    "    doi_df = doi_df[['DOI']]\n",
    "    doi_df.to_csv(fpath.path_poten_csv, mode='a', index=False, header=False)\n",
    "    \n",
    "    # process pubmed_pmc_poten_urls\n",
    "    doi_df = pd.read_csv(fpath.pubmed_pmc_poten_urls)\n",
    "    doi_df = doi_df[['DOI']]\n",
    "    doi_df.to_csv(fpath.path_poten_csv, mode='a', index=False, header=False)\n",
    "    \n",
    "    # process eupmc_poten_urls\n",
    "    doi_df = pd.read_csv(fpath.eupmc_poten_urls)\n",
    "    doi_df = doi_df[['DOI']]\n",
    "    doi_df.to_csv(fpath.path_poten_csv, mode='a', index=False, header=False)\n",
    "    \n",
    "    # eliminate duplicates\n",
    "    doi_df = pd.read_csv(fpath.path_poten_csv)\n",
    "    print(len(doi_df))\n",
    "    doi_df = doi_df.drop_duplicates(subset = 'DOI')\n",
    "    print(len(doi_df))\n",
    "    doi_df.to_csv(fpath.path_poten_csv, index=False)\n",
    "    \n",
    "    # end of merge_search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9df256b-106e-4d5d-847e-b068fbade613",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Google Scholar...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'FPM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m\n\u001b[1;32m      9\u001b[0m proxy \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m\"\u001b[39m: http_proxy, \n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m: https_proxy\n\u001b[1;32m     12\u001b[0m }\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# end of setting header and proxies\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# method 1: search acdemic databases using keywords\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43msearch_acad_dbs\u001b[49m\u001b[43m(\u001b[49m\u001b[43macad_dbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_urls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxy\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m, in \u001b[0;36msearch_acad_dbs\u001b[0;34m(acad_dbs, init_urls, headers, proxy)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m acad_db \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGoogle Scholar\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearching Google Scholar...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     \u001b[43msearch_google_scholar\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_urls\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m acad_db \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeb of Science\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearching Web of Science...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m, in \u001b[0;36msearch_google_scholar\u001b[0;34m(init_url, headers)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch_google_scholar\u001b[39m(init_url, headers):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# create a .txt file to record the urls of google scholar search results, clear the file if already exists\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[43mFPM\u001b[49m\u001b[38;5;241m.\u001b[39mgs_poten_urls, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m     f\u001b[38;5;241m.\u001b[39mtruncate()\n\u001b[1;32m      5\u001b[0m     f\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FPM' is not defined"
     ]
    }
   ],
   "source": [
    "# main program\n",
    "# first we need to search all related literature that might include data or information of thalamocortical connections\n",
    "# search for potentially related literature using the listed 3 methods\n",
    "\n",
    "# setting headers and proxies\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9'}\n",
    "http_proxy  = \"http://103.148.39.50:83\"\n",
    "https_proxy = \"https://47.254.158.115:20201\"\n",
    "proxy = {\n",
    "    \"http\": http_proxy, \n",
    "    \"https\": https_proxy\n",
    "}\n",
    "# end of setting header and proxies\n",
    "\n",
    "# method 1: search acdemic databases using keywords\n",
    "search_acad_dbs(acad_dbs, init_urls, headers, proxy)\n",
    "\n",
    "# # method 2: spanning citations of seed papers\n",
    "# span_citations(seed_papers, num_span_time, headers, proxy)\n",
    "\n",
    "# # method 3: search existing connectome databases\n",
    "# search_conne_db(connec_db, connec_db_quries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8379912b-0688-4d9e-8641-50753f32a54e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://journals.physiology.org/doi/full/10.1152/jn.2001.85.1.219\n",
      "\n",
      "https://www.sciencedirect.com/science/article/pii/0165017396000033\n",
      "\n",
      "https://www.sciencedirect.com/science/article/pii/S1053811906007609\n",
      "\n",
      "3\n",
      "8538\n",
      "7058\n"
     ]
    }
   ],
   "source": [
    "# merge all search results\n",
    "LS.merge_search_results(headers)\n",
    "\n",
    "# send .PDF publication of all potential related literatures to ChatPDF.con and ask for relatedness \n",
    "# then record the answer to the list_of_potential_related_literature.csv as well\n",
    "# ChatPDF_relatedness(path_urls, chatpdf_related_queries)\n",
    "\n",
    "# now we have a list of potential related literature and the information about relatedness \n",
    "# stored in the file \"list_of_potential_related_literature.csv\"\n",
    "# now we may perform a automatic filtering and manual filtering of the literature\n",
    "\n",
    "# automatic filtering\n",
    "#auto_filter(path_potential)\n",
    "\n",
    "# manual filtering\n",
    "# manual_filter(path_potential, path_related_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c754a0f-82ec-4c2c-b949-f06ab46d5d25",
   "metadata": {},
   "source": [
    "This is the end of semi-automated literature search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bf1484-5217-4a14-9c48-ff176f46d57b",
   "metadata": {},
   "source": [
    "Now we have a list of actually related literature stored in list_of_related_literature.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f23abd-ef1d-44b0-8658-3ce0bc705bbd",
   "metadata": {},
   "source": [
    "Next step: we perform a information search on the list of related literature\n",
    "We have a list of actually related literature at the moment, now we need to extract information we need from the literature. We intend to achieve this with a combination of automated searching and manual extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d00a5cd2-5c93-4fa2-ada2-138b94cee0a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# some test code, should comment-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4813f66-c983-4ecd-979a-3de2c97c5a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# test the redirect of the urls\\nheaders = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9'} \\n\\nresponse_pdf = requests.get('https://doi.org/10.1016/j.neuron.2020.01.005', allow_redirects=True, headers = headers)\\nprint(response_pdf.history)\\nprint(response_pdf.url)\\nresponse_pdf_1 = requests.get('https://linkinghub.elsevier.com/retrieve/pii/S0896627320300052', allow_redirects=True, headers = headers)\\nprint(response_pdf_1.history)\\nprint(response_pdf_1.url)\\n\\n\\nresponse_pdf = requests.get('https://onlinelibrary.wiley.com/doi/10.1111/ejn.13910', headers = headers)\\nsoup_pdf = BeautifulSoup(response_pdf.content,'lxml')\\nprint(soup_pdf)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# test the redirect of the urls\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9'} \n",
    "\n",
    "response_pdf = requests.get('https://doi.org/10.1016/j.neuron.2020.01.005', allow_redirects=True, headers = headers)\n",
    "print(response_pdf.history)\n",
    "print(response_pdf.url)\n",
    "response_pdf_1 = requests.get('https://linkinghub.elsevier.com/retrieve/pii/S0896627320300052', allow_redirects=True, headers = headers)\n",
    "print(response_pdf_1.history)\n",
    "print(response_pdf_1.url)\n",
    "\n",
    "\n",
    "response_pdf = requests.get('https://onlinelibrary.wiley.com/doi/10.1111/ejn.13910', headers = headers)\n",
    "soup_pdf = BeautifulSoup(response_pdf.content,'lxml')\n",
    "print(soup_pdf)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8d39c9c-623c-4e2c-b0b4-a44d79b96f88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "if '//doi.org/' in 'https://doi.org/10.1016/0165-0173(96)00003-3':\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40067374-c25b-40f8-ba36-8405a24e9a90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FPM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[43mFPM\u001b[49m\u001b[38;5;241m.\u001b[39mgs_poten_urls, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      3\u001b[0m     lines \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FPM' is not defined"
     ]
    }
   ],
   "source": [
    "# test \n",
    "with open(FPM.gs_poten_urls, 'r') as file:\n",
    "    lines = []\n",
    "    for line in file:\n",
    "        print(line)\n",
    "        line = line.strip()\n",
    "        lines.append(line)\n",
    "print(len(lines))\n",
    "doi_list = []\n",
    "for url in lines:\n",
    "    response = requests.get(url, headers = headers)\n",
    "    soup = BeautifulSoup(response.content,'lxml')\n",
    "    # print(soup)\n",
    "    num_results_str = soup.find_all('a', href = True)\n",
    "    print(num_results_str)\n",
    "    for href in num_results_str:\n",
    "        if '//doi.org/' in href['href']:\n",
    "            doi_list.append(href['href'])\n",
    "            print(href['href'])\n",
    "        else:\n",
    "            print(\"Ops! Did't find DOI on this page!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6f15ed5-64f4-489f-97a6-242bccfa7c69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Ops! Did't find DOI on this page!\n"
     ]
    }
   ],
   "source": [
    "# find DOI\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9'} \n",
    "# this link does not have 'DOI' in href form but text from\n",
    "url = 'https://www.jneurosci.org/content/28/43/11042.short'\n",
    "# url = 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2613515/'\n",
    "response = requests.get(url, headers = headers)\n",
    "soup = BeautifulSoup(response.content,'lxml')\n",
    "# print(soup)\n",
    "doi_list = []\n",
    "num_results_str = soup.find_all('a', href = True)\n",
    "# print(num_results_str)\n",
    "for item in num_results_str:\n",
    "    if '//doi.org/' in item['href']:\n",
    "        print(item['href'])\n",
    "        doi_list.append(item['href'].split('//doi.org/')[1])\n",
    "\n",
    "print(doi_list)\n",
    "        \n",
    "if len(doi_list) == 0:\n",
    "    print(\"Ops! Did't find DOI on this page!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "79686627-676e-4eec-bad4-dd64cbffd90b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [500]>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n## Initialize client\\nclient = ElsClient(config[\\'apikey\\'])\\n\\n## ScienceDirect (full-text) document example using DOI\\ndoi_doc = FullDoc(doi = \\'10.1016/j.biopsych.2004.10.014\\')\\nprint(doi_doc)\\nif doi_doc.read(client):\\n    print (\"doi_doc.title: \", doi_doc.title)\\n    doi_doc.write(\"doi_doc\")   \\nelse:\\n    print (\"Read document failed.\")\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# redirect when access the doi link\n",
    "from elsapy.elsdoc import FullDoc, AbsDoc\n",
    "from elsapy.elsclient import ElsClient\n",
    "import json\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9', \n",
    "    'X-ELS-APIKEY': \"310946e6e005957982c2c9cad6833ad3\",\n",
    "    'Accept': 'application/pdf',\n",
    "    'X-ELS-Insttoken': \"instToken\",\n",
    "    'view': 'FULL'\n",
    "} \n",
    "# url = 'https://www.jneurosci.org/content/28/43/11042.short'\n",
    " #url = 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2613515/'\n",
    "\n",
    "# Journal of Neurophysiology\n",
    "# url = 'https://doi.org/10.1152/jn.2001.85.1.219'\n",
    "# url = 'https://journals.physiology.org/doi/10.1152/jn.2001.85.1.219'\n",
    "\n",
    "# science direct\n",
    "# url = 'https://doi.org/10.1016/j.biopsych.2004.10.014'\n",
    "# url = 'https://linkinghub.elsevier.com/retrieve/pii/S0006322304010947'\n",
    "# url = 'https://www.sciencedirect.com/science/article/pii/S0006322304010947?via%3Dihub'\n",
    "url = 'https://api.elsevier.com/content/article/doi/{10.1016/j.biopsych.2004.10.014}'\n",
    "'''\n",
    "response = requests.get(url, headers = headers)\n",
    "soup = BeautifulSoup(response.content,'lxml')\n",
    "print(soup)\n",
    "print(response.history)\n",
    "print(response.url)\n",
    "# Load configuration\n",
    "con_file = open(\"config.json\")\n",
    "config = json.load(con_file)\n",
    "con_file.close()\n",
    "'''\n",
    "response = requests.get(url, headers = headers)\n",
    "print(response)\n",
    "'''\n",
    "## Initialize client\n",
    "client = ElsClient(config['apikey'])\n",
    "\n",
    "## ScienceDirect (full-text) document example using DOI\n",
    "doi_doc = FullDoc(doi = '10.1016/j.biopsych.2004.10.014')\n",
    "print(doi_doc)\n",
    "if doi_doc.read(client):\n",
    "    print (\"doi_doc.title: \", doi_doc.title)\n",
    "    doi_doc.write(\"doi_doc\")   \n",
    "else:\n",
    "    print (\"Read document failed.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7360fc9f-f51e-4111-be74-a2f3d392bfa5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pdftotext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# pdf to text\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpdftotext\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load your PDF\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/didihou/Downloads/fncir-09-00079.pdf \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pdftotext'"
     ]
    }
   ],
   "source": [
    "# pdf to text\n",
    "pip install PyPDF2\n",
    " \n",
    "# Load your PDF\n",
    "with open(\"/Users/didihou/Downloads/fncir-09-00079.pdf \", \"rb\") as f:\n",
    "    pdf = pdftotext.PDF(f)\n",
    "pritn(pdf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bd81a715-ee56-4f75-a193-08d92c9fcd48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "MINI REVIEWpublished: 02 December 2015doi: 10.3389/fncir.2015.00079Vestibular Interactions in theThalamusRajiv Wijesinghe1,Dario A. Protti2andAaron J. Camp1*1Sensory Systems and Integration Laboratory, Sydney Medical School, Discipline of Biomedical Science, University ofSydney, Sydney, NSW, Australia,2Vision Laboratory, Sydney Medical School, Discipline of Physiology, University of Sydney,Sydney, NSW, AustraliaEdited by:W. Martin Usrey,University of California, Davis, USAReviewed by:Elizabeth Quinlan,University of Maryland College Park,USAMarianne Dieterich,Ludwig-Maximilians-University,Germany*Correspondence:Aaron J. Campaaron.camp@sydney.edu.auReceived: 28 August 2015Accepted: 10 November 2015Published: 02 December 2015Citation:Wijesinghe R, Protti DA and Camp AJ(2015) Vestibular Interactions inthe Thalamus.Front. Neural Circuits 9:79.doi: 10.3389/fncir.2015.00079It has long been known that the vast majority of all information en route to the cerebralcortex must ﬁrst pass through the thalamus. The long held view that the thalamusserves as a simple hi ﬁdelity relay station for sensory information to the cortex, however,has over recent years been dispelled. Indeed, multiple projections from the vestibularnuclei to thalamic nuclei (including the ventrobasal nuclei, and the geniculate bodies)-regions typically associated with other modalities- have been described. Further, somethalamic neurons have been shown to respond to stimuli presented from across sensorymodalities. For example, neurons in the rat anterodorsal and laterodorsal nuclei of thethalamus respond to visual, vestibular, proprioceptive and somatosensory stimuli andintegrate this information to compute heading within the environment. Together, theseﬁndings imply that the thalamus serves crucial integrative functions, at least in regardto vestibular processing, beyond that imparted by a “simple” relay. In this mini reviewwe outline the vestibular inputs to the thalamus and provide some clinical context forvestibular interactions in the thalamus. We then focus on how vestibular inputs interactwith other sensory systems and discuss the multisensory integration properties of thethalamus.Keywords: vestibular, thalamus, LGN, multisensory integration, vestibular nucleiINTRODUCTIONThe vestibular system differs from the other primary sensory systems in a number offundamental ways. Most sensory systems are organized in a linear fashion, where peripheralorgan fibers project primarily through a modality-specific thalamic nucleus (for exampleLGN for the visual system, MGN for the auditory system) and only then onto theirrespective cortical or subcortical targets. These ordered projections through the thalamuscreate a sensory map that closely matches that created in the periphery, and this tends tobe maintained by downstream thalamocortical projections [for review, see Jones (1985)].Abbreviations: CL, Centrolateral Nucleus; CM, Centromedian Nucleus; IL, Interlaminar Nuclei; IVN, InferiorVestibular Nucleus; LGN, Lateral Geniculate Nucleus; LVN, Lateral Vestibular Nucleus; LP, Lateral Posterior Nucleus;LD, Lateral Dorsal Nucleus; MGN, Medial Geniculate Nucleus; MVN, Medial Vestibular Nucleus; PNF, ParafascicularNucleus; SpVN, spinal (descending) vestibular Nucleus; SuVN, Superior Vestibular Nucleus; VA, VentroanteriorNucleus; VI, Ventral Intermediate Nucleus; VL, Ventrolateral Nucleus; VP, Ventral Posterior Nucleus; VPL, VentralPosterior Lateral Nucleus; VPM, Ventral Posterior Medial Nucleus.Frontiers in Neural Circuits | www.frontiersin.org 1 December 2015 | Volume 9 | Article 79\n"
     ]
    }
   ],
   "source": [
    "# importing required modules\n",
    "from PyPDF2 import PdfReader\n",
    "  \n",
    "# creating a pdf reader object\n",
    "reader = PdfReader(\"/Users/didihou/Downloads/fncir-09-00079.pdf\")\n",
    "  \n",
    "# printing number of pages in pdf file\n",
    "print(len(reader.pages))\n",
    "  \n",
    "# getting a specific page from the pdf file\n",
    "page = reader.pages[0]\n",
    "  \n",
    "# extracting text from page\n",
    "text = ''.join(page.extract_text().splitlines())\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efb22af-4181-4ec0-af99-34d50e8a0c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
