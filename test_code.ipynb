{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 15:10:37 Didis-MacBook-Pro.local metapub.config[42116] WARNING NCBI_API_KEY was not set.\n"
     ]
    }
   ],
   "source": [
    "# import internal .py modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from requests.auth import HTTPProxyAuth\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_eupmc(source_path, output_path, columns):\n",
    "    print(\"Starting merging search results from Europe PMC...\")\n",
    "    # process eupmc search results\n",
    "    df = pd.read_csv(source_path, sep = \",\")\n",
    "    df = df[[\"DOI\", \"EXTERNAL_ID\", \"PMCID\", \"TITLE\"]]\n",
    "    df = df.rename(columns={\"EXTERNAL_ID\": \"PMID\", \"TITLE\": \"Title\"}, errors = \"raise\")\n",
    "    for ind in df.index:\n",
    "        print(ind)\n",
    "        if(ind%10 == 0):\n",
    "            time.sleep(random.randint(3,6)*10)\n",
    "        proxies = plib.get_proxies()\n",
    "        pmid = str(df[\"PMID\"][ind])\n",
    "        # print(pmid)\n",
    "        url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "        regex = \"[a-zA-Z]\"\n",
    "        if len(re.findall(regex, pmid)) == 0:\n",
    "            # print(\"pmid\")\n",
    "            soup = plib.request_webpage(url)\n",
    "            # print(soup)\n",
    "\n",
    "            # get PMCID\n",
    "            # print(df[\"PMCID\"][ind])\n",
    "            if df[\"PMCID\"][ind] is np.nan:\n",
    "                try:\n",
    "                    pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "                except:\n",
    "                    pmcid  =\"not found\"\n",
    "            else:\n",
    "                pmcid = str(df[\"PMCID\"][ind])\n",
    "            # print(pmcid)\n",
    "            # get DOI\n",
    "            if df[\"DOI\"][ind] is np.nan:\n",
    "                try:\n",
    "                    doi = soup.find_all(\"span\", {\"class\": \"identifier doi\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "                except:\n",
    "                    doi  =\"not found\"\n",
    "            else:\n",
    "                doi = str(df[\"DOI\"][ind])\n",
    "            # get full_text_url\n",
    "            if pmcid != \"not found\":\n",
    "                full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "                full_text_source = \"PMC\"\n",
    "            else:\n",
    "                try:\n",
    "                    full_text_url = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"href\"].strip()\n",
    "                    full_text_source = soup.find_all(\"div\", {\"class\": \"full-text-links-list\"})[0].find_all(\"a\", {\"class\": \"link-item dialog-focus\"})[0][\"data-ga-action\"].strip()\n",
    "                except:\n",
    "                    full_text_url = \"not found\"\n",
    "                    full_text_source = \"not found\"\n",
    "            # columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"First_Author\", \"full_text_url\", \"full_text_source\"]\n",
    "        else:\n",
    "            # print(\"not pmid\")\n",
    "            if df[\"DOI\"][ind] is np.nan:\n",
    "                doi  =\"not found\"\n",
    "            else:\n",
    "                doi = str(df[\"DOI\"][ind])\n",
    "            if df[\"PMCID\"][ind] is np.nan:\n",
    "                full_text_url = \"not found\"\n",
    "                full_text_source = \"not found\"\n",
    "                pmcid = \"not found\"\n",
    "            else:\n",
    "                full_text_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "                full_text_source = \"PMC\"\n",
    "                pmcid = str(df[\"PMCID\"][ind])\n",
    "            first_author = \"not found\"\n",
    "            pmid = \"not found\"\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"Title\": [str(df[\"Title\"][ind])],\n",
    "            \"First_Author\": [first_author],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"full_text_source\": [full_text_source]\n",
    "        }\n",
    "        # print(row)\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_eupmc\n",
    "# output_path = fpath.poten_litera_eupmc_processed\n",
    "# plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(source_path, sep=',')\n",
    "# df = df[[\"SOURCE\", \"DOI\", \"EXTERNAL_ID\", \"PMCID\", \"TITLE\"]]\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# col_one_list = set(df['SOURCE'].tolist())\n",
    "# print(col_one_list)\n",
    "# \n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# print(df[\"SOURCE\"].isnull().values.any())\n",
    "# print(df[\"DOI\"].isnull().values.any())\n",
    "# print(df[\"EXTERNAL_ID\"].isnull().values.any())\n",
    "# print(df[\"PMCID\"].isnull().values.any())\n",
    "# print(df[\"TITLE\"].isnull().values.any())\n",
    "# # PMID, Title don't contain np.nan\n",
    "# # DOI, PMCID contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# merge_eupmc(source_path, output_path, columns)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: json. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m doi \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m10.1007/BF00231845\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[39m# pmid = plib.doi2pmid(doi)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m# print(pmid)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m pmid \u001b[39m=\u001b[39m plib\u001b[39m.\u001b[39;49mid_converter_in_pubmed(doi, \u001b[39m\"\u001b[39;49m\u001b[39mpmid\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/myProjects/didihou_master_project/public_library.py:180\u001b[0m, in \u001b[0;36mid_converter_in_pubmed\u001b[0;34m(input, output)\u001b[0m\n\u001b[1;32m    178\u001b[0m         time\u001b[39m.\u001b[39msleep(random\u001b[39m.\u001b[39mrandint(\u001b[39m5\u001b[39m, \u001b[39m10\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m60\u001b[39m)\n\u001b[1;32m    179\u001b[0m         response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url, headers \u001b[39m=\u001b[39m plib\u001b[39m.\u001b[39mheaders)\n\u001b[0;32m--> 180\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39;49mcontent, \u001b[39m\"\u001b[39;49m\u001b[39mjson\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    181\u001b[0m \u001b[39mprint\u001b[39m(soup)\n\u001b[1;32m    182\u001b[0m records \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mselect(\u001b[39m\"\u001b[39m\u001b[39mp\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_text()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/bs4/__init__.py:250\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m     builder_class \u001b[39m=\u001b[39m builder_registry\u001b[39m.\u001b[39mlookup(\u001b[39m*\u001b[39mfeatures)\n\u001b[1;32m    249\u001b[0m     \u001b[39mif\u001b[39;00m builder_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[39mraise\u001b[39;00m FeatureNotFound(\n\u001b[1;32m    251\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a tree builder with the features you \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequested: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m. Do you need to install a parser library?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m             \u001b[39m%\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(features))\n\u001b[1;32m    255\u001b[0m \u001b[39m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39m# with the remaining **kwargs.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39mif\u001b[39;00m builder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: json. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "# get pmid from doi\n",
    "def doi2pmid(doi):\n",
    "    pmid = \"\"\n",
    "    if pmid == None:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return pmid\n",
    "# --------------------start of test code--------------------\n",
    "doi = \"10.1007/BF00231845\"\n",
    "# pmid = plib.doi2pmid(doi)\n",
    "# print(pmid)\n",
    "pmid = plib.id_converter_in_pubmed(doi, \"pmid\")\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_google_shcolar(source_path, output_path, columns):\n",
    "    print(\"Starting merging search results from Google Scholar...\")\n",
    "    df = pd.read_csv(source_path, header=None, sep=',')\n",
    "    df.columns = [\"title\", \"url\", \"full_text_url\", \"full_text_type\", \"full_text_source\"]\n",
    "    for ind in df.index:\n",
    "        # columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "        # get title\n",
    "        title = str(df[\"title\"][ind]).strip()\n",
    "        # get pmid\n",
    "        pmid = np.nan\n",
    "        # get pmcid\n",
    "        pmcid = np.nan\n",
    "\n",
    "        # get full_text_url\n",
    "        # get full_text_source\n",
    "        if str(df[\"full_text_type\"][ind]) == \"[HTML]\":\n",
    "            full_text_url, = plib.get_final_redirected_url(str(df[\"full_text_url\"][ind]).strip()).strip()\n",
    "            full_text_source = full_text_url.split(\"://\")[1].strip().split(\"/\")[0].strip()\n",
    "        elif str(df[\"full_text_type\"][ind]) == \"not found\" | \"[PDF]\" | \"UB\":\n",
    "            if str(df[\"url\"][ind]) != \"not found\":\n",
    "                full_text_url, = plib.get_final_redirected_url(str(df[\"url\"][ind]).strip()).strip()\n",
    "                full_text_source = full_text_url.split(\"://\")[1].strip().split(\"/\")[0].strip()\n",
    "            else:\n",
    "                full_text_url = np.nan\n",
    "                full_text_source = np.nan\n",
    "        else:\n",
    "            raise Exception(\"Found a full_text_type not expected!\")\n",
    "        # get pdf_url\n",
    "        # get pdf_source\n",
    "        if str(df[\"full_text_type\"][ind]) == \"[PDF]\":\n",
    "            pdf_url = str(df[\"full_text_url\"][ind])\n",
    "            pdf_source = pdf_url.split(\"://\")[1].strip().split(\"/\")[0].strip()\n",
    "        else:\n",
    "            pdf_url = np.nan\n",
    "            pdf_source = np.nan\n",
    "\n",
    "        # get DOI\n",
    "        proxies = plib.get_proxies()\n",
    "        url = full_text_url\n",
    "        doi = get_doi(url)\n",
    "        \n",
    "        # columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"Title\": [str(df[\"Title\"][ind])],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"full_text_source\": [full_text_source],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"pdf_source\": [pdf_source]\n",
    "        }\n",
    "        # print(row)\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_gs\n",
    "# output_path = fpath.poten_litera_gs_processed\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"title\", \"url\", \"full_text_url\", \"full_text_type\", \"full_text_source\"]\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# full_text_type = set(df['full_text_type'].tolist())\n",
    "# print(\"All full_text_type include: \",full_text_type)\n",
    "# full_text_source = set(df['full_text_source'].tolist())\n",
    "# print(\"All full_text_source include: \", full_text_source)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# # [\"title\", \"url\", \"full_text_url\", \"full_text_type\", \"full_text_source\"]\n",
    "# print(df[\"title\"].str.contains('not found').sum())\n",
    "# print(df[\"url\"].str.contains('not found').sum())\n",
    "# print(df[\"full_text_url\"].str.contains('not found').sum())\n",
    "# print(df[\"full_text_type\"].str.contains('not found').sum())\n",
    "# print(df[\"full_text_source\"].str.contains('not found').sum())\n",
    "# # \n",
    "# # title, url don't contain np.nan\n",
    "# # full_text_url, full_text_type, full_text_source contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# merge_google_shcolar(source_path, output_path, columns)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # options = Options()\n",
    "    # options.add_argument('--headless')\n",
    "    # driver = webdriver.Firefox(service=Service(GeckoDriverManager().install()))\n",
    "    # # driver = webdriver.Firefox(GeckoDriverManager().install())\n",
    "    # driver.get(\"https://www.pmid2cite.com/doi-to-pmid-converter\")\n",
    "    # WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.XPATH, \"//p[text()='Consent']\"))).click()\n",
    "    # doi = \"10.1113/JP282626\"\n",
    "    # try:\n",
    "    #     # print(str(doi))\n",
    "    #     WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"#formInput\"))).send_keys(str(doi).strip())\n",
    "    # except TimeoutException:\n",
    "    #     print(\"Waiting for clicking consent timeout\")\n",
    "    # try:\n",
    "    #     # driver.find_element(By.XPATH, \"/html/body/div[5]/div[2]/form/button\").click()\n",
    "    #     # EC.presence_of_element_located(By.XPATH, \"/html/body/div[5]/div[2]/form/button\")\n",
    "    #     WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.XPATH, \"/html/body/div[1]/div[2]/form/button\"))).click()\n",
    "    # except TimeoutException:\n",
    "    #     print(\"Waiting for clicking button timeout\")\n",
    "    # try:\n",
    "    #     WebDriverWait(driver, 30).until(EC.visibility_of_all_elements_located((By.XPATH, \"/html/body/div[7]/div[3]/p[1]/span[2]/a\")))\n",
    "    # except TimeoutException:\n",
    "    #     print(\"Waiting for getting PMID timeout\")\n",
    "    #     my_elem = driver.find_element(By.XPATH, \"button[aria-label='Leaving from']\").text\n",
    "    #     res = my_elem.text\n",
    "    #     pmid = res\n",
    "\n",
    "    #     pmid = np.nan\n",
    "    # print(pmid)\n",
    "    # driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
