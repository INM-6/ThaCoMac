{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "# import PyPDF2\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from requests.auth import HTTPProxyAuth\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "# from selenium import webdriver\n",
    "# from webdriver_manager.firefox import GeckoDriverManager\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.firefox.options import Options\n",
    "# from selenium.webdriver.support.wait import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from webdriver_manager.firefox import GeckoDriverManager\n",
    "# from selenium.webdriver.firefox.service import Service\n",
    "# from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException\n",
    "import os\n",
    "import re\n",
    "from lxml import etree\n",
    "from nltk import ngrams\n",
    "import json\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-28 21:12:11 Didis-MacBook-Pro.local metapub.config[44242] WARNING NCBI_API_KEY was not set.\n"
     ]
    }
   ],
   "source": [
    "# import internal modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Output fpath.poten_litera_db\n",
    "# input_path = fpath.poten_litera_db\n",
    "# df = pd.read_csv(input_path, header=0, sep='\\t')\n",
    "\n",
    "# for ind in df.index:\n",
    "#     index = df.at[ind, \"INDEX\"]\n",
    "#     doi = df.at[ind, \"DOI\"]\n",
    "#     pmid = df.at[ind, \"PMID\"]\n",
    "#     full_text_url = df.at[ind, \"FULL_TEXT_URL\"]\n",
    "#     pdf_url = df.at[ind, \"PDF_URL\"]\n",
    "#     # relevant = df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"]\n",
    "    \n",
    "#     line_number_in_csv = ind + 2\n",
    "#     print(\"Line number:\", line_number_in_csv, \"INDEX:\", index)\n",
    "#     print(\"ind:\", ind, )\n",
    "#     print(\"DOI:\", doi)\n",
    "#     print(\"PMID:\", pmid)\n",
    "#     print(\"full_text_url:\", full_text_url)\n",
    "#     print(\"pdf_url:\", pdf_url)\n",
    "#     # print(\"relevant: \", relevant)\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Output fpath.final_confirm_article_list\n",
    "# input_path = fpath.final_confirm_article_list\n",
    "# df = pd.read_csv(input_path, header=0, sep='\\t')\n",
    "\n",
    "# for ind in df.index:\n",
    "#     index = df.at[ind, \"INDEX\"]\n",
    "#     doi = df.at[ind, \"DOI\"]\n",
    "#     pmid = df.at[ind, \"PMID\"]\n",
    "#     full_text_url = df.at[ind, \"FULL_TEXT_URL\"]\n",
    "#     pdf_url = df.at[ind, \"PDF_URL\"]\n",
    "#     relevant = df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"]\n",
    "    \n",
    "#     line_number_in_csv = ind + 2\n",
    "#     print(\"Line number:\", line_number_in_csv, \"INDEX:\", index)\n",
    "#     print(\"ind:\", ind, )\n",
    "#     print(\"DOI:\", doi)\n",
    "#     print(\"PMID:\", pmid)\n",
    "#     print(\"full_text_url:\", full_text_url)\n",
    "#     print(\"pdf_url:\", pdf_url)\n",
    "#     print(\"relevant: \", relevant)\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Output fpath.relevant_reviews_and_ta_not_avaialble\n",
    "# input_path = fpath.relevant_reviews_and_ta_not_avaialble\n",
    "# df = pd.read_csv(input_path, header=0, sep='\\t')\n",
    "\n",
    "# for ind in df.index:\n",
    "#     index = df.at[ind, \"INDEX\"]\n",
    "#     doi = df.at[ind, \"DOI\"]\n",
    "#     pmid = df.at[ind, \"PMID\"]\n",
    "#     full_text_url = df.at[ind, \"FULL_TEXT_URL\"]\n",
    "#     pdf_url = df.at[ind, \"PDF_URL\"]\n",
    "#     relevant = df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"]\n",
    "    \n",
    "#     line_number_in_csv = ind + 2\n",
    "#     print(\"Line number:\", line_number_in_csv, \"INDEX:\", index)\n",
    "#     print(\"ind:\", ind, )\n",
    "#     print(\"DOI:\", doi)\n",
    "#     print(\"PMID:\", pmid)\n",
    "#     print(\"full_text_url:\", full_text_url)\n",
    "#     print(\"pdf_url:\", pdf_url)\n",
    "#     print(\"relevant: \", relevant)\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Output fpath.relevant_articles_YESES\n",
    "# df_path = fpath.relevant_articles_YESES_corrected\n",
    "# df = pd.read_csv(df_path, header=0, sep='\\t')\n",
    "\n",
    "# for ind in df.index:\n",
    "#     index = int(df.at[ind, \"INDEX\"])\n",
    "#     title = df.at[ind, \"TITLE\"]\n",
    "#     doi = df.at[ind, \"DOI\"]\n",
    "#     pmid = df.at[ind, \"PMID\"]\n",
    "#     full_text_url = df.at[ind, \"FULL_TEXT_URL\"]\n",
    "#     pdf_url = df.at[ind, \"PDF_URL\"]\n",
    "#     relevant = df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"]\n",
    "#     review = df.at[ind, \"REVIEW(Y/N)\"]\n",
    "#     # comment = df.at[ind, \"COMMENT\"]\n",
    "    \n",
    "#     line_number_in_csv = ind + 2\n",
    "#     print(\"Line number:\", line_number_in_csv, \"INDEX:\", index)\n",
    "#     # print(\"ind:\", ind)\n",
    "#     print(\"TITLE:\", title)\n",
    "#     print(\"DOI:\", doi)\n",
    "#     print(\"PMID:\", pmid)\n",
    "#     print(\"FULL_TEXT_URL:\", full_text_url)\n",
    "#     print(\"PDF_URL:\", pdf_url)\n",
    "#     print(\"RELEVANT?:\", relevant)\n",
    "#     print(\"REVIEW?\", review)\n",
    "#     # print(\"COMMENT:\", comment)\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# A = pd.read_csv('./datasets/macaque_corticoCortical_adjacency.csv', index_col=0)\n",
    "\n",
    "# # Create non-directed graph\n",
    "# G = nx.convert_matrix.from_pandas_adjacency(A, create_using=nx.Graph)\n",
    "\n",
    "# # print(G.nodes)\n",
    "\n",
    "# # pos = nx.spring_layout(G, seed=24)\n",
    "# # nx.draw_networkx(G, pos, with_labels=False, node_size=0, edge_cmap='Greys', width=0.1, alpha=0.5)\n",
    "\n",
    "# # # ALl this just to make the nodes certain colors\n",
    "# # macaque_regions = ['Visual', 'Temporal', 'Parietal', 'Somatosensory', 'Auditory',\n",
    "# #                    'Motor', 'Prefrontal', 'Frontal', 'Orbitofrontal']\n",
    "# # colors = ['#4c6ef5', '#228be6', '#15aabf', '#12b886', '#40c057', '#82c91e', '#fab005', '#fd7e14']\n",
    "# # for region, c in zip(macaque_regions, colors):\n",
    "# #     # subset = subset = macaque.get_areas_in_regions(region)\n",
    "# #     subset = get_areas_in_regions(region)\n",
    "# #     nodes = []\n",
    "# #     for node in subset:\n",
    "# #         if node in G.nodes:\n",
    "# #             nodes.append(node)\n",
    "# #     nx.draw_networkx_nodes(G, pos, nodelist=nodes, node_color=c, node_size=30, alpha=0.8, label=region)\n",
    "\n",
    "# # # Create labels on top\n",
    "# # nx.draw_networkx(G, pos, with_labels=True, node_size=0, width=0, alpha=1, font_size=7)\n",
    "\n",
    "# # plt.legend(frameon=False, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# # plt.savefig('Cortico_cortical_connectivity_macaque.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To show the adjacency matrix\n",
    "# plt.imshow(A)\n",
    "# plt.yticks(range(len(A.columns)), A.columns, fontsize=7)\n",
    "# plt.ylabel('Source areas', fontsize=10)\n",
    "# plt.xticks(range(len(A.columns)), A.columns, rotation=90, fontsize=7)\n",
    "# plt.xlabel('Target areas', fontsize=10)\n",
    "# plt.colorbar()\n",
    "\n",
    "# # plt.subplots_adjust(bottom=0.1)\n",
    "# plt.savefig('Cortico_cortical_connectivity_heatmap_macaque.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Data\n",
    "# labels1 = ['Google Scholar', 'Web of Science', 'PubMed', 'Europe PMC']\n",
    "# sizes1 = [980, 1993, 2612, 9178]\n",
    "# labels2 = ['Full text not available', 'Full text available']\n",
    "# sizes2 = [226, 10550]\n",
    "\n",
    "# # Custom function to format the label\n",
    "# def func(pct, allvalues): \n",
    "#     absolute = int(pct / 100.*np.sum(allvalues)) \n",
    "#     return \"{:.1f}%\\n({:d} )\".format(pct, absolute)\n",
    "\n",
    "# # Cold color palette\n",
    "# colors1 = ['#85C1E9', '#3498DB', '#2874A6', '#1B4F72']\n",
    "# colors2 = ['#AED6F1', '#2E86C1']\n",
    "\n",
    "# # Plotting\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))  # 1 row, 2 columns\n",
    "\n",
    "# # Font sizes\n",
    "# title_fontsize = 16\n",
    "# label_fontsize = 15\n",
    "# autopct_fontsize = 10\n",
    "\n",
    "# # First pie chart\n",
    "# ax1.pie(sizes1, labels=labels1, autopct=lambda pct: func(pct, sizes1), \n",
    "#         startangle=90, colors=colors1, textprops={'fontsize': label_fontsize})\n",
    "# ax1.set_title(\"A\", loc='left', fontsize=title_fontsize)\n",
    "# ax1.axis('equal')\n",
    "\n",
    "# # Second pie chart\n",
    "# ax2.pie(sizes2, labels=labels2, autopct=lambda pct: func(pct, sizes2), \n",
    "#         startangle=90, colors=colors2, textprops={'fontsize': label_fontsize})\n",
    "# ax2.set_title(\"B\", loc='left', fontsize=title_fontsize)\n",
    "# ax2.axis('equal')\n",
    "\n",
    "# # Show both pie charts side by side\n",
    "# plt.tight_layout()  # Adjusts the space between the two plots for better visualization\n",
    "# plt.show()\n",
    "\n",
    "# # Save figure\n",
    "# fig.savefig('pie_chart.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create poten_litera_testing_set_1000_labeled.csv\n",
    "# input_path = fpath.poten_litera_testing_set_1000_text_extract_and_count\n",
    "# output_path = fpath.poten_litera_testing_set_1000_labeled\n",
    "# # clear the file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(input_path, header=0, sep=',')\n",
    "# columns = [\n",
    "#     \"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"PDF_URL\", \n",
    "#     \"TITLE\", \"TEXT_TAK\", \"TEXT_500\"\n",
    "# ]\n",
    "# columns = columns + df_col.text_columns_to_add # add keyword group text\n",
    "# columns = columns + df_col.count_columns_to_add # add keyword group count\n",
    "# columns_to_add = [\"TT?(Y/N/MB/NA)\", \"MACAQUE?(Y/N/MB/NA)\", \"TC_OR_CT?(Y/N/MB/NA)\", \"RELEVANT?(Y/N/MB/NA)\", \"READ_BY(A/D/R)\", \"COMMENT\"] # add columns for documenting labels\n",
    "# columns = columns + columns_to_add\n",
    "# df.columns = columns\n",
    "\n",
    "\n",
    "# # select only the columns INDEX, DOI and PMID and save the dataframe as a csv file\n",
    "# df = df[[\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"TT?(Y/N/MB/NA)\", \"MACAQUE?(Y/N/MB/NA)\", \"TC_OR_CT?(Y/N/MB/NA)\", \"RELEVANT?(Y/N/MB/NA)\", \"READ_BY(A/D/R)\", \"COMMENT\"]]\n",
    "# df.to_csv(output_path, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create poten_litera_testing_set_1000_labeled.csv\n",
    "# input_path = fpath.poten_litera_testing_set_1000_text_extract_and_count\n",
    "# output_path = fpath.poten_litera_testing_set_1000_labeled\n",
    "# # clear the file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(input_path, header=0, sep=',')\n",
    "# columns = [\n",
    "#     \"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"PDF_URL\", \n",
    "#     \"TITLE\", \"TEXT_TAK\", \"TEXT_500\"\n",
    "# ]\n",
    "# columns = columns + df_col.text_columns_to_add # add keyword group text\n",
    "# columns = columns + df_col.count_columns_to_add # add keyword group count\n",
    "# columns_to_add = [\"TT?(Y/N/MB/NA)\", \"MACAQUE?(Y/N/MB/NA)\", \"TC_OR_CT?(Y/N/MB/NA)\", \"RELEVANT?(Y/N/MB/NA)\", \"READ_BY(A/D/R)\", \"COMMENT\"] # add columns for documenting labels\n",
    "# columns = columns + columns_to_add\n",
    "# df.columns = columns\n",
    "\n",
    "\n",
    "# # select only the columns INDEX, DOI and PMID and save the dataframe as a csv file\n",
    "# df = df[[\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"TT?(Y/N/MB/NA)\", \"MACAQUE?(Y/N/MB/NA)\", \"TC_OR_CT?(Y/N/MB/NA)\", \"RELEVANT?(Y/N/MB/NA)\", \"READ_BY(A/D/R)\", \"COMMENT\"]]\n",
    "# df.to_csv(output_path, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_path = fpath.poten_litera_db\n",
    "# with open(input_path, 'r') as file:\n",
    "#     content = file.read()\n",
    "\n",
    "# # Replace tabs with spaces\n",
    "# content_with_spaces = content.replace('\\t', ' ')\n",
    "\n",
    "# with open(input_path, 'w') as file:\n",
    "#     file.write(content_with_spaces)\n",
    "    \n",
    "# df = pd.read_csv(input_path, header=None, sep=\",\")\n",
    "# plib.clear_file(input_path)\n",
    "# df.columns = df_col.db_columns\n",
    "# df.to_csv(input_path, index=False, header=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = fpath.final_manually_read_csv_1_aitor\n",
    "# df_aitor = pd.read_csv(data_path, header=0, sep='\\t')\n",
    "\n",
    "# data_path = fpath.final_manually_read_csv_1_labeled\n",
    "# df_1_labeled  = pd.read_csv(data_path, header=0, sep='\\t')\n",
    "\n",
    "# for ind in df_aitor.index:\n",
    "#     index = int(df_aitor.at[ind, \"INDEX\"])\n",
    "    \n",
    "#     if index == 103:\n",
    "#         break\n",
    "    \n",
    "#     if df_aitor.at[ind,\"INDEX\"] == df_1_labeled.at[ind,\"INDEX\"]:\n",
    "#         df_1_labeled.at[ind,\"TT?(Y/N/MB/NA)\"] = df_aitor.at[ind,\"TT?(Y/N/MB/NA)\"]\n",
    "#         df_1_labeled.at[ind,\"MACAQUE?(Y/N/MB/NA)\"] = df_aitor.at[ind,\"MACAQUE?(Y/N/MB/NA)\"]\n",
    "#         df_1_labeled.at[ind,\"TC_OR_CT?(Y/N/MB/NA)\"] = df_aitor.at[ind,\"TC_OR_CT?(Y/N/MB/NA)\"]\n",
    "#         df_1_labeled.at[ind,\"RELEVANT?(Y/N/MB/NA)\"] = df_aitor.at[ind,\"RELEVANT?(Y/N/MB/NA)\"]\n",
    "#         df_1_labeled.at[ind, \"REVIEW(Y/N)\"] = df_aitor.at[ind, \"REVIEW(Y/N)\"]\n",
    "#         df_1_labeled.at[ind,\"READ_BY(A/D/R)\"] = df_aitor.at[ind,\"READ_BY(A/D/R)\"]\n",
    "#         df_1_labeled.at[ind,\"COMMENT\"] = df_aitor.at[ind,\"COMMENT\"]\n",
    "#     else:\n",
    "#         raise Exception(\"INDEX mismatch!\")\n",
    "    \n",
    "#     line_number_in_csv = ind + 2\n",
    "#     print(\"Line number:\", line_number_in_csv, \"INDEX:\", index)\n",
    "#     print(\"ind: \", ind)\n",
    "\n",
    "# df_1_labeled.to_csv(fpath.final_manually_read_csv_1_labeled, index=False, header=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the CSV files\n",
    "# df1 = pd.read_csv(fpath.final_confirm_article_list_labeled, header=0, sep='\\t')\n",
    "# df2 = pd.read_csv(fpath.relevant_article_and_is_review_labeled, header=0, sep='\\t')\n",
    "\n",
    "# # Paths to your CSV files\n",
    "# output_path = fpath.relevant_articles_YESES\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# # Filter rows where \"RELEVANT?(Y/N/MB/NA)\" is 'Y'\n",
    "# filtered_df1 = df1[df1[\"RELEVANT?(Y/N/MB/NA)\"] == 'Y']\n",
    "# filtered_df2 = df2[df2[\"RELEVANT?(Y/N/MB/NA)\"] == 'Y']\n",
    "\n",
    "# # Concatenate the filtered DataFrames\n",
    "# combined_df = pd.concat([filtered_df1, filtered_df2])\n",
    "\n",
    "# # Reset the index and drop the old one\n",
    "# combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # Write the combined DataFrame to a new CSV file\n",
    "# combined_df.to_csv(output_path, index=False, header=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the CSV files\n",
    "# df1 = pd.read_csv(fpath.relevant_articles_YESES, header=0, sep='\\t')\n",
    "# df_db = pd.read_csv(fpath.poten_litera_db, header=0, sep='\\t')\n",
    "\n",
    "# df1 = df1[df_col.final_confirm_df_columns]\n",
    "\n",
    "# # Paths to your CSV files\n",
    "# output_path = fpath.relevant_articles_YESES_corrected\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# # Combine data from df1 and df_db based on a matching index\n",
    "# for ind in df1.index:\n",
    "#     index = int(df1.at[ind, \"INDEX\"])\n",
    "    \n",
    "#     # Find the corresponding row in df_db\n",
    "#     db_row = df_db[df_db['INDEX'].astype(int) == index]\n",
    "\n",
    "#     # Directly update values from df_db to df1\n",
    "#     df1.at[ind, 'DOI'] = db_row['DOI'].values[0]\n",
    "#     df1.at[ind, 'PMID'] = db_row['PMID'].values[0]\n",
    "#     df1.at[ind, 'PMCID'] = db_row['PMCID'].values[0]\n",
    "#     df1.at[ind, 'FULL_TEXT_URL'] = db_row['FULL_TEXT_URL'].values[0]\n",
    "#     df1.at[ind, 'PDF_URL'] = db_row['PDF_URL'].values[0]\n",
    "#     df1.at[ind, 'TITLE'] = db_row['TITLE'].values[0]\n",
    "#     df1.at[ind, 'ABSTRACT'] = db_row['ABSTRACT'].values[0]\n",
    "#     df1.at[ind, 'KEYWORDS'] = db_row['KEYWORDS'].values[0]\n",
    "\n",
    "# # Write the combined DataFrame to a new CSV file\n",
    "# df1.to_csv(output_path, index=False, header=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracers:\n",
      "['AAV', 'BDA', 'Biocytin', 'Bis', 'CTB', 'CTB-gold', 'CTB-green', 'CTB-red', 'CVS strain', 'DG', 'DR', 'DT', 'DY', 'DY . 2HCl', 'DY/NY', 'EB', 'FB', 'FE', 'FG', 'FR', 'FR-DA', 'FS', 'FS-dextran', 'GB', 'GLM', 'HRP', 'LY', 'LYD', 'NY', 'PHA-L', 'PI', 'RB', 'RGB', 'RLM', 'TAA', 'TB', 'WGA', 'WGA-HRP', '[(35)S]-Methionine', '[^3H]-Fucose', '[^3H]-Leucine', '[^3H]-Lysine', '[^3H]-Proline', '[^3H]-WGA']\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "# Print the list of tracers\n",
    "\n",
    "# JSON file path\n",
    "tracer_path = \"./metadata_extraction/tracer.json\"\n",
    "\n",
    "# Read the JSON file\n",
    "with open(tracer_path) as json_file:\n",
    "    tracer_data = json.load(json_file)\n",
    "\n",
    "# Abbrievations of tracers, and rank the elements based on alphabetical order\n",
    "tracers_dict = sorted(tracer_data[\"index_of_abreviations\"].keys())\n",
    "tracers = tracer_data[\"tracers\"]\n",
    "\n",
    "if tracers_dict != tracers:\n",
    "    raise Exception(\"Error: Tracers are not consistent!\")\n",
    "\n",
    "print(\"Tracers:\")\n",
    "print(tracers)\n",
    "print(len(tracers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macaque subspecies:\n",
      "['NS', 'M. mulatta', 'M. fuscata', 'M. nemestrina', 'M. radiata', 'M. arctoides', 'M. fascicularis']\n"
     ]
    }
   ],
   "source": [
    "# Print the list of macaque subspecies\n",
    "\n",
    "# JSON file path\n",
    "macaque_path = \"./metadata_extraction/macaque_subspecies.json\"\n",
    "\n",
    "# Read the JSON file\n",
    "with open(macaque_path) as json_file:\n",
    "    macaque_data = json.load(json_file)\n",
    "\n",
    "# Print macaque subspecies\n",
    "macaque_subspecies = list(macaque_data[\"Macaque_subspecies\"].keys())\n",
    "print(\"Macaque subspecies:\")\n",
    "print(macaque_subspecies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pacellation scheme references:\n",
      "['Asanuma_1983a', 'Asanuma_1983b', 'Barbas_1989', 'Brodmann_1905', 'Campbell_1989', 'Dum_1993', 'Durif_2003', 'Jones_1985', 'Jones_1989', 'Jones_1990', 'Jones_1998b', 'Kusama_1970', 'Liu_2002', 'Matelli_1989', 'Matelli_1991', 'Matelli_1998', 'Morel_1997', 'Morel_2005', 'Muakkassa_1979', 'Olszewski_1952', 'Pandya_1982', 'Parent_1983', 'Paxinos_2000', 'Schell_1984', 'Seltzer_1984', 'Seltzer_1986', 'Tsang_2000', 'Walker_1940a', 'Walker_1940b', 'Wannier_2005', 'Wise_1981', 'Woolsey_1952']\n"
     ]
    }
   ],
   "source": [
    "# Print the list of pacelleation scheme references\n",
    "\n",
    "# JSON file path\n",
    "ps_path = \"./metadata_extraction/parcellation_scheme_reference.json\"\n",
    "\n",
    "# Read the JSON file\n",
    "with open(ps_path) as json_file:\n",
    "    ps_data = json.load(json_file)\n",
    "\n",
    "# Print all references of parcellation schemes\n",
    "ps = sorted(ps_data[\"PSR\"].keys())\n",
    "print(\"All pacellation scheme references:\")\n",
    "print(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M132 atlas areas:\n",
      "['1', '10', '11', '12', '13', '14', '2', '23', '24a', '24b', '24c', '24d', '25', '29/30', '3', '31', '32', '44', '45A', '45B', '46d', '46v', '5', '7A', '7B', '7m', '7op', '8B', '8l', '8m', '8r', '9', '9/46d', '9/46v', 'AIP', 'Core', 'DP', 'ENTO', 'F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'FST', 'Gu', 'INS', 'IPa', 'LB', 'LIP', 'MB', 'MIP', 'MST', 'MT', 'OPAI', 'OPRO', 'PBc', 'PBr', 'PERI', 'PGa', 'PIP', 'PIR', 'POLE', 'Pi', 'Pro. St.', 'ProM', 'SII', 'STPc', 'STPi', 'STPr', 'SUB', 'TEO', 'TEOm', 'TEa/ma', 'TEa/mp', 'TEad', 'TEav', 'TEpd', 'TEpv', 'TH/TF', 'TPt', 'V1', 'V2', 'V3', 'V3A', 'V4', 'V4t', 'V6', 'V6A', 'VIP']\n",
      "91\n"
     ]
    }
   ],
   "source": [
    "# Print the list of M132 cortical areas\n",
    "\n",
    "# JSON file path\n",
    "m132_path = \"./metadata_extraction/M132.json\"\n",
    "\n",
    "# Read the JSON file\n",
    "with open(m132_path) as json_file:\n",
    "    m132_data = json.load(json_file)\n",
    "\n",
    "# Cortical pacelleation schemes, and rank the elements based on alphabetical order\n",
    "m132_dict = sorted(m132_data[\"index_of_abreviations\"].keys())\n",
    "m132_areas = sorted(m132_data[\"areas\"])\n",
    "\n",
    "if m132_dict != m132_areas:\n",
    "    raise Exception(\"The number of elements in m132_dict and m132_areas are not the same!\")\n",
    "\n",
    "print(\"M132 atlas areas:\")\n",
    "print(m132_dict)\n",
    "print(len(m132_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D99s thalamic subregions:\n",
      "['AD', 'AM', 'AV', 'Hl', 'Hm', 'LD', 'LGN', 'LP', 'MDdc', 'MDmc', 'MDmf', 'MDpc', 'MGad', 'MGm', 'MGpd', 'MGv', 'MGz', 'PI', 'PL', 'PM', 'Pa', 'Pf', 'Pulo', 'Re', 'Sg', 'VAmc', 'VApc', 'VLc', 'VLo', 'VLps', 'VPI', 'VPLc', 'VPLo', 'VPM', 'VPMpc', 'cdc', 'cif', 'cim', 'cl', 'clc', 'cnMD', 'csl', 'pcn', 'pt', 'ptg', 'r', 'zic']\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "# Print the list of D99 thalamic subregions\n",
    "\n",
    "# JSON file path\n",
    "d99s_path = \"./metadata_extraction/D99_Thal.json\"\n",
    "\n",
    "# Read the JSON file\n",
    "with open(d99s_path) as json_file:\n",
    "    d99s_data = json.load(json_file)\n",
    "\n",
    "# Thalamic pacelleation schemes, and rank the elements based on alphabetical order\n",
    "d99s_dict = sorted(d99s_data[\"index_of_abreviations\"].keys())\n",
    "d99s_nuclei = sorted(d99s_data[\"subregions\"])\n",
    "\n",
    "if d99s_dict != d99s_nuclei:\n",
    "    raise Exception(\"The number of elements in d99s_dict and d99s_areas are not the same!\")\n",
    "\n",
    "print(\"D99s thalamic subregions:\")\n",
    "print(d99s_dict)\n",
    "print(len(d99s_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMTs thalamic subregions:\n",
      "['A', 'A+', 'APul', 'AR', 'CL-PC', 'CM', 'CMn-PF', 'CbThal', 'DLG', 'GThal', 'IAM', 'ILThal', 'IMD', 'IPul', 'LD', 'LP', 'LPul', 'Lim', 'MD', 'MG', 'MLThal', 'MPul', 'MThal', 'PNThal', 'PT-PV-sm', 'PThal', 'PVA', 'PVP', 'Pul', 'Re-Rh-Xi', 'Rt', 'SG', 'SPFC', 'SPFPC', 'SpThal', 'Thal', 'VA', 'VLA', 'VLPD', 'VLPV', 'VLX', 'VM', 'VMPo-VMB', 'VPI', 'VPM-VPL', 'VThal', 'bsc', 'cap']\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "# Print the list of NMT thalamic subregions\n",
    "\n",
    "# JSON file path\n",
    "nmts_path = \"./metadata_extraction/NMT_Thal.json\"\n",
    "\n",
    "# Read the JSON file\n",
    "with open(nmts_path) as json_file:\n",
    "    nmts_data = json.load(json_file)\n",
    "\n",
    "# Thalamic pacelleation schemes, and rank the elements based on alphabetical order\n",
    "nmts_dict = sorted(nmts_data[\"index_of_abreviations\"].keys())\n",
    "nmts_nuclei = sorted(nmts_data[\"subregions\"])\n",
    "\n",
    "if nmts_dict != nmts_nuclei:\n",
    "    raise Exception(\"The number of elements in nmts_dict and nmts_areas are not the same!\")\n",
    "\n",
    "print(\"NMTs thalamic subregions:\")\n",
    "print(nmts_dict)\n",
    "print(len(nmts_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAXs thalamic subregions:\n",
      "['AD', 'AM', 'AThal', 'AV', 'Apul', 'CL', 'CM', 'CMn', 'CMnL', 'CMnM', 'IAM', 'IMD', 'Ipul', 'LDSF', 'LDt', 'LThal', 'Lpul', 'MD', 'MDC', 'MDD', 'MDL', 'MDM', 'MG', 'MGD', 'MGM', 'MGV', 'MPul', 'MThal', 'MedLam', 'PC', 'PF', 'PT', 'PV', 'PVA', 'PVP', 'Pul', 'Re', 'Rt', 'SG', 'SPF', 'SPFPC', 'Thal', 'VA', 'VAL', 'VAL(VO)', 'VAL(VO)+pal', 'VAL(pal)', 'VAM', 'VAMC', 'VL', 'VLL', 'VLM', 'VPL', 'VPM', 'VPThal', 'Xi', 'eml', 'iml', 'ithp', 'mt']\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "# Print the list of PAX thalamic subregions\n",
    "\n",
    "# JSON file path\n",
    "paxs_path = \"./metadata_extraction/PAX_Thal.json\"\n",
    "\n",
    "# Read the JSON file\n",
    "with open(paxs_path) as json_file:\n",
    "    paxs_data = json.load(json_file)\n",
    "\n",
    "# Thalamic pacelleation schemes, and rank the elements based on alphabetical order\n",
    "paxs_dict = sorted(paxs_data[\"index_of_abreviations\"].keys())\n",
    "paxs_nuclei = sorted(paxs_data[\"subregions\"])\n",
    "\n",
    "if paxs_dict != paxs_nuclei:\n",
    "    raise Exception(\"The number of elements in paxs_dict and paxs_areas are not the same!\")\n",
    "\n",
    "print(\"PAXs thalamic subregions:\")\n",
    "print(paxs_dict)\n",
    "print(len(paxs_dict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
