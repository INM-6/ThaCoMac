{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal .py modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from requests.auth import HTTPProxyAuth\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException\n",
    "import os\n",
    "import re\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(input, output_path):\n",
    "    # combine all results\n",
    "    df = pd.DataFrame()\n",
    "    for search_result in input:\n",
    "        df_single = pd.read_csv(search_result, header=None, sep = \",\")\n",
    "        # df = df.append(df_single, ignore_index=True, sort=False)\n",
    "        df = pd.concat([df, df_single], ignore_index=True, sort=False)\n",
    "    df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "    df.to_csv(output_path, header=False, index=False)\n",
    "# --------------------start of test code--------------------\n",
    "# gos = fpath.poten_litera_gs_processed_step2\n",
    "# wos = fpath.poten_litera_wos_processed\n",
    "# pubmed = fpath.poten_litera_pubmed_processed\n",
    "# eupmc = fpath.poten_litera_eupmc_processed\n",
    "# input = [gos, wos, pubmed, eupmc]\n",
    "# output_path = fpath.poten_litera_combined\n",
    "# # plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# combine(input, output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "# # (14627, 8)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in elements missed, remove duplciations based on identifiers in the potential related literature\n",
    "def fill_in_missing(input_path, output_path, start, end):\n",
    "    df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "    df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "    \n",
    "    # fill in elements that are missing\n",
    "    for ind in range(start, end):\n",
    "        # initialzie\n",
    "        doi = np.nan\n",
    "        pmid = np.nan\n",
    "        pmcid = np.nan\n",
    "        title = str(df[\"Title\"][ind]).strip()\n",
    "        # print(title)\n",
    "        full_text_link = np.nan\n",
    "        pdf_link = np.nan\n",
    "\n",
    "        # doi, pmid\n",
    "        if df[\"DOI\"][ind] == df[\"DOI\"][ind]: # DOI -> PMID\n",
    "            doi = str(df[\"DOI\"][ind]).strip()\n",
    "            # print(doi)\n",
    "            if df[\"PMID\"][ind] == df[\"PMID\"][ind]:\n",
    "                pmid = str(df[\"PMID\"][ind]).strip()\n",
    "                # print(pmid)\n",
    "            else:\n",
    "                pmid = plib.doi2pmid(doi)\n",
    "                # print(pmid)\n",
    "                if pmid != pmid:\n",
    "                    pmid_cadidate = plib.title2pmid(title)\n",
    "                    if pmid_cadidate == pmid_cadidate:   \n",
    "                        doi_validate, a = plib.pmid2doi_pmcid(pmid_cadidate)\n",
    "                        if doi_validate == doi:\n",
    "                            pmid = pmid_cadidate\n",
    "                            # print(pmid)\n",
    "        elif df[\"PMID\"][ind] == df[\"PMID\"][ind]: # PMID -> DOI\n",
    "            pmid = str(df[\"PMID\"][ind]).strip()\n",
    "            # print(pmid)\n",
    "            doi, pmcid = plib.pmid2doi_pmcid(str(pmid).strip())\n",
    "            # print(doi)\n",
    "        elif df[\"PMCID\"][ind] == df[\"PMCID\"][ind]: # PMCID -> DOI, PMID\n",
    "            pmcid = str(df[\"PMCID\"][ind]).strip()\n",
    "            doi, pmid = plib.pmcid2doi_pmid(pmcid)\n",
    "            # print(doi)\n",
    "            # print(pmid)\n",
    "        else:\n",
    "            doi = np.nan\n",
    "            pmid = np.nan\n",
    "        # print(doi)\n",
    "        # print(pmid)\n",
    "        \n",
    "        # pmcid\n",
    "        if df[\"PMCID\"][ind] == df[\"PMCID\"][ind]:\n",
    "            pmcid = str(df[\"PMCID\"][ind]).strip()\n",
    "        elif pmid == pmid:\n",
    "            doi_1, pmcid = plib.pmid2doi_pmcid(str(pmid).strip())\n",
    "        else:\n",
    "            pmcid = np.nan\n",
    "        # print(pmcid)\n",
    "\n",
    "        title.lower()\n",
    "\n",
    "        # full_text_link\n",
    "        if pmcid == pmcid:\n",
    "            full_text_link = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + pmcid + \"/\"\n",
    "        elif doi == doi:\n",
    "            full_text_link = plib.get_final_redirected_url(str(\"https://doi.org/\" + doi).strip())\n",
    "        elif df[\"full_text_url\"][ind] == df[\"full_text_url\"][ind]:\n",
    "            full_text_link = plib.get_final_redirected_url(df[\"full_text_url\"][ind])\n",
    "        else:\n",
    "            full_text_link = np.nan\n",
    "        # print(full_text_link)\n",
    "\n",
    "        # pdf_link\n",
    "        if df[\"pdf_url\"][ind] == df[\"pdf_url\"][ind]:\n",
    "            pdf_link = plib.get_final_redirected_url(str(df[\"pdf_url\"][ind]).strip())\n",
    "        else:\n",
    "            pdf_link = np.nan\n",
    "        # print(pdf_link)\n",
    "    \n",
    "        columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_link\", \"pdf_link\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"Title\": [title],\n",
    "            \"full_text_link\": [full_text_link],\n",
    "            \"pdf_link\": [pdf_link],\n",
    "        }\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "\n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "input_path = fpath.poten_litera_combined\n",
    "output_path = fpath.poten_litera_filled\n",
    "# plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "fill_in_missing(input_path, output_path, 0, 14627)\n",
    "# fill_in_identifiers(input, output, start, end)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_remove_dupli(input_path, output_path): \n",
    "    df = pd.read_csv(input_path, header=None, sep = \",\")\n",
    "\n",
    "    # remove all duplicates\n",
    "    df = df.drop_duplicates(subset=['DOI'])\n",
    "    df = df.drop_duplicates(subset=['PMID'])\n",
    "    df = df.drop_duplicates(subset=['PMCID'])\n",
    "    print(\"Duplication in the potential related literature removed.\")\n",
    "\n",
    "    print(\"Found \" + len(df) + \" potential related literature in total.\")\n",
    "\n",
    "    # write the results into our output file\n",
    "    df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_link\", \"pdf_link\"]\n",
    "    df.to_csv(output_path, header=True, index=False)\n",
    "# --------------------start of test code--------------------\n",
    "# source_path = fpath.poten_litera_filled\n",
    "# output_path = fpath.filtered_poten_litra\n",
    "# plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_link\", \"pdf_link\"]\n",
    "# print(df.head(3))\n",
    "# print(df.shape)\n",
    "# # \n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# # merge all search results\n",
    "# identifier = [\"DOI\", \"PMID\", \"PMCID\"]\n",
    "# merge_remove_dupli(input_path, output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# www.nature.com\n",
    "def www_nature_com(url):\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    # load the webpage\n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver = webdriver.Chrome()\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"Extracting content from:\" + url + \" failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "    \n",
    "    try:\n",
    "        elems = driver.find_elements(By.XPATH, \"//span[@class='c-bibliographic-information__value')]\")\n",
    "        for elem in elems:\n",
    "            if \"doi.org/\" in elem.text:\n",
    "                doi = elem.text.split(\"doi.org/\")[1]\n",
    "    except:\n",
    "        doi = np.nan\n",
    "    pmid = np.nan\n",
    "    pmcid = np.nan\n",
    "    try:\n",
    "        title = driver.find_element(By.XPATH, \"//h1[contains(@class, 'c-article-title')]\").text\n",
    "    except:\n",
    "        title = np.nan\n",
    "    abstract = np.nan\n",
    "    keywords = np.nan\n",
    "    intro = np.nan\n",
    "    pdf_link = np.nan\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"introduction\": intro,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "    driver.quit\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "# url = \"https://www.nature.com/articles/387281a0\"\n",
    "# info = www_nature_com(url)\n",
    "# print(info[\"doi\"])\n",
    "# print(info[\"pmid\"])\n",
    "# print(info[\"pmcid\"])\n",
    "# print(info[\"title\"])\n",
    "# print(info[\"abstract\"])\n",
    "# print(info[\"keywords\"])\n",
    "# print(info[\"introduction\"])\n",
    "# print(info[\"pdf_link\"])\n",
    "# ---------------------end of test code---------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
