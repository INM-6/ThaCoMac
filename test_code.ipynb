{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal .py modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from requests.auth import HTTPProxyAuth\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get doi from url\n",
    "def url2doi(url, source):\n",
    "    url = str(url).strip()\n",
    "\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver = webdriver.Firefox(options, service=Service(GeckoDriverManager().install()))\n",
    "            driver.get(url)\n",
    "\n",
    "            # WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//p[text()='Consent']\"))).click()\n",
    "\n",
    "            # try:\n",
    "            #     WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"#formInput\"))).send_keys(str(doi).strip())\n",
    "            # except TimeoutException:\n",
    "            #     print(\"Waiting for clicking consent timeout\")\n",
    "            # try:\n",
    "            #     # WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"/html/body/div[1]/div[2]/form/button\"))).click()\n",
    "            #     WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, '//button[text()=\"Get PMID\"]'))).click()\n",
    "            # except TimeoutException:\n",
    "            #     print(\"Waiting for clicking button timeout\")\n",
    "            \n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"URL to DOI transformation failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "\n",
    "    try:\n",
    "        my_elem = driver.find_elements(By.XPATH, \"//*[contains(text(), 'doi.org') or contains(text(), 'DOI')]\")\n",
    "        doi = \"\"\n",
    "        doi = str(my_elem.find_element(By.XPATH, \"i\").get_attribute(\"innerHTML\"))\n",
    "    except:\n",
    "        doi = np.nan\n",
    "    driver.quit()\n",
    "    return doi\n",
    "# --------------------start of test code--------------------\n",
    "# url = \"https://www.tandfonline.com/doi/abs/10.1080/01616412.1985.11739692\"\n",
    "# doi = url2doi(url)\n",
    "# print(doi)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_redirected_url(url):\n",
    "    response = requests.get(url, headers = plib.headers)\n",
    "    \n",
    "    while(response.status_code != 200):\n",
    "        if response.status_code == 403:\n",
    "            final_url = url\n",
    "            break\n",
    "        elif response.status_code == 404:\n",
    "            final_url = np.nan\n",
    "            break\n",
    "        else:\n",
    "            print(response.status_code)     \n",
    "            # sleep for 5 minutes\n",
    "            time.sleep(300)\n",
    "            response = requests.get(url, headers = plib.headers)\n",
    "    if response.status_code == 200:\n",
    "        final_url = response.url\n",
    "    # history = response.history\n",
    "    return final_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0  \\\n",
      "0  Effect of attentive fixation in macaque thalam...   \n",
      "1  The thalamus of the Macaca, mulatta. An atlas ...   \n",
      "2  Distribution of the dopamine innervation in th...   \n",
      "\n",
      "                                                   1                        2  \\\n",
      "0  https://journals.physiology.org/doi/abs/10.115...  journals.physiology.org   \n",
      "1  https://www.cabdirect.org/cabdirect/welcome/?t...        www.cabdirect.org   \n",
      "2  https://www.sciencedirect.com/science/article/...    www.sciencedirect.com   \n",
      "\n",
      "                                                   3                        4  \n",
      "0  https://journals.physiology.org/doi/pdf/10.115...  journals.physiology.org  \n",
      "1                                                NaN                      NaN  \n",
      "2                                                NaN                      NaN  \n"
     ]
    }
   ],
   "source": [
    "def preprocess_google_shcolar_step1(source_path, output_path, start, end):\n",
    "    print(\"Starting merging search results from Google Scholar...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, header=None, sep=',')\n",
    "    df.columns = [\"title\", \"url\", \"url_type\", \"full_text_url\", \"full_text_type\", \"full_text_source\"]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # df[\"url_type\"][ind]: {'[CITATION][C]', '[PDF][PDF]', '[BOOK][B]', nan, '[HTML][HTML]'}\n",
    "        # if df[\"url_type\"][ind] == \"[CITATION][C]\" or \"[BOOK][B]\", skip this row\n",
    "        if (df[\"url_type\"][ind] == \"[CITATION][C]\") or (df[\"url_type\"][ind] == \"[BOOK][B]\"):\n",
    "            continue\n",
    "        \n",
    "        # if url or title doesn't exsit AND full_text_url doesn't exist\n",
    "        if (df[\"url\"][ind] != df[\"url\"][ind]) or (df[\"title\"][ind] != df[\"title\"][ind]):\n",
    "            continue\n",
    "        \n",
    "        # now every row has at least title and url, and the url_text = {\"[PDF][PDF]\", nan, \"[HTML][HTML]\"}\n",
    "        if df[\"url_type\"][ind] == \"[PDF][PDF]\":\n",
    "            # full_text_type = {'[HTML]', nan, '[PDF]', 'UB'}\n",
    "            if df[\"full_text_type\"][ind] != \"[HTML]\":\n",
    "                full_text_url = np.nan\n",
    "                full_text_source = np.nan\n",
    "            else:\n",
    "                link = str(df[\"full_text_url\"][ind]).strip()\n",
    "                full_text_url = get_final_redirected_url(link)\n",
    "                if full_text_url == full_text_url:\n",
    "                    full_text_source = full_text_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                else:\n",
    "                    full_text_source = np.nan\n",
    "            # get pdf_url, pdf_source\n",
    "            link = str(df[\"url\"][ind]).strip()\n",
    "            pdf_url = get_final_redirected_url(link)\n",
    "            if pdf_url == pdf_url:\n",
    "                pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "            else:\n",
    "                pdf_source = np.nan\n",
    "        else: # df[\"url_type\"][ind] == nan or '[HTML][HTML]'\n",
    "            link = str(df[\"url\"][ind]).strip()\n",
    "            full_text_url = get_final_redirected_url(link)\n",
    "            if full_text_url == full_text_url:\n",
    "                full_text_source = full_text_url.split(\"://\")[1].split(\"/\")[0]\n",
    "            else:\n",
    "                full_text_source = np.nan\n",
    "            # get pdf_url, pdf_source\n",
    "            # full_text_type = {'[HTML]', nan, '[PDF]', 'UB'}\n",
    "            if df[\"full_text_type\"][ind] == \"[PDF]\":\n",
    "                link = str(df[\"full_text_url\"][ind]).strip()\n",
    "                pdf_url = get_final_redirected_url(link)\n",
    "                if pdf_url == pdf_url:\n",
    "                    pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "                else:\n",
    "                    pdf_source = np.nan\n",
    "            else:\n",
    "                pdf_url = np.nan\n",
    "                pdf_source = np.nan\n",
    "        \n",
    "        columns = [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "        row = {\n",
    "            \"Title\": [str(df[\"title\"][ind]).strip()],\n",
    "            \"full_text_url\": [full_text_url],\n",
    "            \"full_text_source\": [full_text_source],\n",
    "            \"pdf_url\": [pdf_url],\n",
    "            \"pdf_source\": [pdf_source]\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "source_path = fpath.poten_litera_gs\n",
    "output_path = fpath.poten_litera_gs_processed_step1\n",
    "# plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(source_path, header=None, sep=',')\n",
    "# df.columns = [\"title\", \"url\", \"url_type\", \"full_text_url\", \"full_text_type\", \"full_text_source\"]\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# url_type = set(df['url_type'].tolist())\n",
    "# print(url_type)\n",
    "# # {'[CITATION][C]', '[PDF][PDF]', '[BOOK][B]', nan, '[HTML][HTML]'}\n",
    "# full_text_type = set(df['full_text_type'].tolist())\n",
    "# print(full_text_type)\n",
    "# # {nan, 'UB', '[HTML]', '[PDF]'}\n",
    "# full_text_source = set(df['full_text_source'].tolist())\n",
    "# print(full_text_source)\n",
    "# # {'ahajournals.org', 'lww.com', 'springer.com', 'academia.edu', 'plos.org', 'ieee.org', 'nature.com', \n",
    "# # 'mdpi.com', 'jpn.ca', 'uottawa.ca', nan, 'northwestern.edu', 'bmj.com', 'ekja.org', 'RWTH-Link', 'wiley.com', \n",
    "# # 'escholarship.org', 'nyu.edu', 'frontiersin.org', 'sciencedirect.com', 'eneuro.org', 'jneurosci.org', \n",
    "# # 'royalsocietypublishing.org', 'karger.com', 'harvard.edu', 'annualreviews.org', 'mcgill.ca', \n",
    "# # 'elifesciences.org', 'mirasmart.com', 'duke.edu', 'ucdavis.edu', 'physiology.org', 'cell.com', \n",
    "# # 'wustl.edu', 'epfl.ch', 'udc.es', 'psychiatryonline.org', 'jst.go.jp', 'core.ac.uk', 'rero.ch', \n",
    "# # 'zsp.com.pk', 'sagepub.com', 'europepmc.org', 'tandfonline.com', 'asahq.org', 'sonar.ch', 'koreamed.org', \n",
    "# # 'oup.com', 'science.org', 'scholarpedia.org', 'psu.edu', 'jordanbpeterson.com', 'pnas.org', 'uzh.ch', 'biorxiv.org', \n",
    "# # 'biomedcentral.com', 'umich.edu', 'ahuman.org', 'researchgate.net', 'ijpp.com', 'unav.edu', 'nih.gov', 'bu.edu'}\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# # [\"title\", \"url\", \"url_type\", \"full_text_url\", \"full_text_type\", \"full_text_source\"]\n",
    "# print(df[\"title\"].isnull().any().any())\n",
    "# print(df[\"url\"].isnull().any().any())\n",
    "# print(df[\"url_type\"].isnull().any().any())\n",
    "# print(df[\"full_text_url\"].isnull().any().any())\n",
    "# print(df[\"full_text_type\"].isnull().any().any())\n",
    "# print(df[\"full_text_source\"].isnull().any().any())\n",
    "# # True, True, True, True, True, True\n",
    "# # title, url, url_type, full_text_url, full_text_type, full_text_source contain np.nan\n",
    "# # we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_google_shcolar_step1(source_path, output_path, 0, 1000)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "df = pd.read_csv(output_path, header=None, sep=',')\n",
    "print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0  Effect of attentive fixation in macaque thalam...   \n",
      "1  The thalamus of the Macaca, mulatta. An atlas ...   \n",
      "2  Distribution of the dopamine innervation in th...   \n",
      "\n",
      "                                       full_text_url         full_text_source  \\\n",
      "0  https://journals.physiology.org/doi/abs/10.115...  journals.physiology.org   \n",
      "1  https://www.cabdirect.org/cabdirect/welcome/?t...        www.cabdirect.org   \n",
      "2  https://www.sciencedirect.com/science/article/...    www.sciencedirect.com   \n",
      "\n",
      "                                             pdf_url               pdf_source  \n",
      "0  https://journals.physiology.org/doi/pdf/10.115...  journals.physiology.org  \n",
      "1                                                NaN                      NaN  \n",
      "2                                                NaN                      NaN  \n",
      "{'www.degruyter.com', 'www.cabdirect.org', 'www.liebertpub.com', 'www.cell.com', 'onlinelibrary.wiley.com', nan, 'bmcneurosci.biomedcentral.com', 'nyaspubs.onlinelibrary.wiley.com', 'tbiomed.biomedcentral.com', 'www.jstage.jst.go.jp', 'link.springer.com', 'royalsocietypublishing.org', 'www.cambridge.org', 'escholarship.mcgill.ca', 'books.google.de', 'europepmc.org', 'www.biorxiv.org', 'www.jstor.org', 'www.tandfonline.com', 'wakespace.lib.wfu.edu', 'journals.lww.com', 'thejns.org', 'www.ahajournals.org', 'analyticalsciencejournals.onlinelibrary.wiley.com', 'open.bu.edu', 'www.nature.com', 'movementdisorders.onlinelibrary.wiley.com', 'www.eneuro.org', 'jamanetwork.com', 'elifesciences.org', 'n.neurology.org', 'agro.icm.edu.pl', 'www.science.org', 'journals.sagepub.com', 'jnnp.bmj.com', 'pure.mpg.de', 'ieeexplore.ieee.org', 'orca.cardiff.ac.uk', 'karger.com', 'elibrary.ru', 'www.jpn.ca', 'ajp.psychiatryonline.org', 'psycnet.apa.org', 'www.sciencedirect.com', 'ekja.org', 'physoc.onlinelibrary.wiley.com', 'pubs.asahq.org', 'www.pnas.org', 'neuro.psychiatryonline.org', 'var.scholarpedia.org', 'www.elibrary.ru', 'www.mdpi.com', 'www.frontiersin.org', 'www.taylorfrancis.com', 'www.jneurosci.org', 'journals.physiology.org', 'submissions.mirasmart.com', 'www.theses.fr', 'jpet.aspetjournals.org', 'academic.oup.com', 'journals.plos.org', 'www.ncbi.nlm.nih.gov'}\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def preprocess_google_shcolar_step2(source_path, output_path, columns, start, end):\n",
    "    print(\"Starting merging search results from Google Scholar...\")\n",
    "\n",
    "    df = pd.read_csv(source_path, header=None, sep=',')\n",
    "    df.columns = [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "\n",
    "    for ind in range(start, end):\n",
    "        # get doi from url\n",
    "        url = str(df[\"full_text_url\"][ind]).strip()\n",
    "        source = str(df[\"full_text_url\"][ind]).strip()\n",
    "        doi = url2doi(url, source)\n",
    "\n",
    "        # get pmid from DOI\n",
    "        if doi == doi: # there's doi\n",
    "            pmid = plib.doi2pmid(doi)\n",
    "        else: # doi not found\n",
    "            pmid = np.nan\n",
    "\n",
    "        # get pmcid\n",
    "        if pmid == pmid: # there's pmid\n",
    "            url = \"https://pubmed.ncbi.nlm.nih.gov/\" + pmid + \"/\"\n",
    "            # proxies = plib.get_proxies()\n",
    "            soup = plib.request_webpage(url)\n",
    "            # print(soup)\n",
    "            try:\n",
    "                pmcid = soup.find_all(\"span\", {\"class\": \"identifier pmc\"})[0].find_all(\"a\", {\"class\": \"id-link\"})[0].get_text().strip()\n",
    "            except: # pmcid not found\n",
    "                pmcid = np.nan\n",
    "        else: # pmid not found\n",
    "            pmcid = np.nan\n",
    "        # print(pmcid)\n",
    "\n",
    "        # columns = [\"DOI\", \"PMID\", \"PMCID\", \"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "        row = {\n",
    "            \"DOI\": [doi],\n",
    "            \"PMID\": [pmid],\n",
    "            \"PMCID\": [pmcid],\n",
    "            \"Title\": [str(df[\"Title\"][ind]).strip()],\n",
    "            \"full_text_url\": [str(df[\"full_text_url\"][ind]).strip()],\n",
    "            \"full_text_source\": [str(df[\"full_text_source\"][ind]).strip()],\n",
    "            \"pdf_url\": [str(df[\"pdf_url\"][ind]).strip()],\n",
    "            \"pdf_source\": [str(df[\"pdf_source\"][ind]).strip()],\n",
    "        }\n",
    "        # print(row)\n",
    "\n",
    "        if not plib.add_row_to_csv(output_path, row, columns):\n",
    "            print(\"Error detected when adding a row to csv!\")\n",
    "        \n",
    "        print(ind)\n",
    "# --------------------start of test code--------------------\n",
    "source_path = fpath.poten_litera_gs_processed_step1\n",
    "output_path = fpath.poten_litera_gs_processed_step2\n",
    "plib.clear_file(output_path)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "df = pd.read_csv(source_path, header=None, sep=',')\n",
    "df.columns = [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "print(df.head(3))\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "full_text_source = set(df['full_text_source'].tolist())\n",
    "print(full_text_source)\n",
    "# \n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# [\"Title\", \"full_text_url\", \"full_text_source\", \"pdf_url\", \"pdf_source\"]\n",
    "print(df[\"Title\"].isnull().any().any())\n",
    "print(df[\"full_text_url\"].isnull().any().any())\n",
    "print(df[\"full_text_source\"].isnull().any().any())\n",
    "print(df[\"pdf_url\"].isnull().any().any())\n",
    "print(df[\"pdf_source\"].isnull().any().any())\n",
    "# \n",
    "# contain np.nan\n",
    "# we need to fill in what are missing\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# preprocess_google_shcolar_step2(source_path, output_path, columns, 0, 1000)\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# df = pd.read_csv(output_path, header=None, sep=',')\n",
    "# print(df.head(3))\n",
    "# ---------------------end of test code---------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
