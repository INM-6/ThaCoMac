{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal .py modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from requests.auth import HTTPProxyAuth\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException\n",
    "import os\n",
    "import re\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# www.ncbi.nlm.nih.gov/pmc/\n",
    "def www_ncbi_nlm_nih_gov(url):\n",
    "    soup = plib.request_webpage(url)\n",
    "    \n",
    "    # extract information from loaded webpage\n",
    "    try:\n",
    "        doi = soup.find_all(\"span\", {\"class\": \"doi\"})[0].find_all(\"a\")[0].get_text().strip()\n",
    "    except:\n",
    "        doi = np.nan\n",
    "    try:\n",
    "        pmid = soup.find_all(\"span\", {\"class\": \"fm-citation-pmid\"})[0].find_all(\"a\")[0].get_text().strip()\n",
    "    except:\n",
    "        pmid = np.nan\n",
    "    try:\n",
    "        pmcid = soup.find_all(\"span\", {\"class\": \"fm-citation-pmcid\"})[0].find_all(\"a\")[0].get_text().strip()\n",
    "    except:\n",
    "        pmcid = np.nan\n",
    "    try:\n",
    "        title = soup.find_all(\"h1\", {\"class\": \"content-title\"})[0].get_text().strip()\n",
    "    except:\n",
    "        title = np.nan\n",
    "    try:\n",
    "        abstract = soup.find_all(\"p\", {\"class\": \"p p-first-last\"})[0][0].get_text().strip()\n",
    "    except:\n",
    "        abstract = np.nan\n",
    "    try:\n",
    "        keywords = soup.find_all(\"span\", {\"class\": \"kwd-text\"})[0].get_text().strip()\n",
    "    except:\n",
    "        keywords = np.nan\n",
    "    try:\n",
    "        intro = \"\"\n",
    "        elements = soup.find_all(\"div\", {\"id\": \"S1\"})[0].find_all(\"0\")\n",
    "        for element in elements:\n",
    "            intro = intro + element.get_text().strip()\n",
    "    except:\n",
    "        intro = np.nan\n",
    "    try:\n",
    "        pmcid = soup.find_all(\"li\", {\"class\": \"pdf-link other_item\"})[0].find_all(\"a\")[0][href]\n",
    "    except:\n",
    "        pdf_link = np.nan\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"introduction\": intro,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10133512/\"\n",
    "info = www_ncbi_nlm_nih_gov(url)\n",
    "print(info)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# www.frontiersin.org\n",
    "def www_frontiersin_org(url):\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    # load the webpage\n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver = webdriver.Chrome(executable_path=ChromeDriverManager().install(), options=options)\n",
    "            driver.get(url)\n",
    "            # WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//p[text()='Consent']\"))).click()\n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"Extracting content from:\" + url + \" failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "    \n",
    "    # extract information from loaded webpage\n",
    "    try:\n",
    "        doi = driver.find_element(By.XPATH, \"/html/body/main/article/section[3]/div/div[1]/div[1]/div[1]/div[1]/div/div[2]/span[2]/a\").text\n",
    "    except:\n",
    "        doi = np.nan\n",
    "    try:\n",
    "        pmid = driver.find_element(By.XPATH, \"/html/body/main/article/section[3]/div/div[1]/div[1]/div[1]/div[2]/div[2]/a\").text\n",
    "    except:\n",
    "        pmid = np.nan\n",
    "    try:\n",
    "        pmcid = driver.find_element(By.XPATH, \"/html/body/main/article/section[3]/div/div[1]/div[1]/div[1]/div[2]/div[1]/span[2]\").text\n",
    "    except:\n",
    "        pmcid = np.nan\n",
    "    try:\n",
    "        title = driver.find_element(By.XPATH, \"/html/body/main/article/section[3]/div/div[1]/div[1]/h1\").text\n",
    "    except:\n",
    "        title = np.nan\n",
    "    try:\n",
    "        abstract = driver.find_element(By.XPATH, \"/html/body/main/article/section[3]/div/div[1]/div[4]/div[2]/p\").text\n",
    "    except:\n",
    "        abstract = np.nan\n",
    "    try:\n",
    "        keywords = driver.find_element(By.XPATH, \"/html/body/main/article/section[3]/div/div[1]/div[4]/div[3]/span\").text\n",
    "    except:\n",
    "        keywords = np.nan\n",
    "    try:\n",
    "        intro = driver.find_element(By.XPATH, '//*[@id=\"S1\"]').text\n",
    "    except:\n",
    "        intro = np.nan\n",
    "    try:\n",
    "        pdf_link = driver.find_element(By.XPATH, \"/html/body/main/article/section[3]/div/div[1]/div[1]/div[1]/div[1]/div/div[2]/span[2]/a\").get_attribute('href')\n",
    "    except:\n",
    "        pdf_link = np.nan\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"introduction\": intro,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10133512/\"\n",
    "info = www_ncbi_nlm_nih_gov(url)\n",
    "print(info)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_from_webpage(url):\n",
    "    if url != url:\n",
    "        raise Exception(\"The given url is np.nan\")\n",
    "    \n",
    "    url = plib.gei_final_redirected_url(url)\n",
    "    source = url.split(\"://\")[1].split(\"/\")[0]\n",
    "    \n",
    "    for website in plib.websites:\n",
    "        if website in source:\n",
    "            # Get the function name by replacing \".\" with \"_\" and use globals() to call it\n",
    "            func_name = website.replace(\".\", \"_\")\n",
    "            func = globals().get(func_name)\n",
    "            return func(url)\n",
    "        else:\n",
    "            print(\"The url:\" + url + \" is not included in our websites database yet!\")\n",
    "            return None\n",
    "# --------------------start of test code--------------------\n",
    "# websites = [\"PMC\", \"frontiersin\", \"europepmc\", \"biorxiv\", \"jneurosci\", \"orca.cardiff\", \"science\", \"thejns\", \"cambridge\",\n",
    "#                 \"wiley\", \"ahajournals\", \"mdpi\", \"sciencedirect\", \"pnas\", \"nature\", \"cell\", \"eneuro\", \"physiology\", \"springer\",\n",
    "#                 \"ieee\", \"plos\", \"jstage.jst\", \"biomedcentral\", \"jamanetwork\", \"psycnet.apa\", \"jnnp.bmj\", \"degruyter\",\n",
    "#                 \"karger\", \"pure.mpg\", \"elifesciences\", \"neurology\", \"pubs.asahq\", \"sagepub\", \"ekja\", \"liebertpub\", \"lww\",\n",
    "#                 \"tandfonline\", \"aspetjournals\", \"oup\", \"royalsocietypublishing\", \"psychiatryonline\", \"jpn\", \"open.bu.edu\",\n",
    "#                 \"agro.icm\", \"lib.wfu\", \"mirasmart\", \"jstor\"]\n",
    "# if len(websites) == len(set(websites)):\n",
    "#     print(\"There are no duplicates in the list.\")\n",
    "# else:\n",
    "#     print(\"There are duplicates in the list.\")\n",
    "# ---------------------end of test code---------------------\n",
    "\n",
    "# --------------------start of test code--------------------\n",
    "# url = \"https://www.tandfonline.com/doi/abs/10.1080/01616412.1985.11739692\"\n",
    "# info = extract_info_from_webpage(url)\n",
    "# print(info)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get doi from url\n",
    "def url2doi(url):\n",
    "    if url != url:\n",
    "        raise Exception(\"The url given is np.nan\")\n",
    "    \n",
    "    url = str(url).strip()\n",
    "    info = extract_info_from_webpage(url) # dictionary\n",
    "    if info == None:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return info[\"doi\"]\n",
    "# --------------------start of test code--------------------\n",
    "# url = \"https://www.tandfonline.com/doi/abs/10.1080/01616412.1985.11739692\"\n",
    "# doi = url2doi(url)\n",
    "# print(doi)\n",
    "# ---------------------end of test code---------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
