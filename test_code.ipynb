{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib\n",
    "import extract_info\n",
    "import parameters as params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from requests.auth import HTTPProxyAuth\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException\n",
    "import os\n",
    "import re\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text from given .pdf file\n",
    "def pdf2text(pdf_path, text_path, page_start, page_end):\n",
    "    try:\n",
    "        # creating a pdf reader object\n",
    "        reader = PyPDF2.PdfReader(pdf_path)\n",
    "        \n",
    "        # printing number of pages in pdf file\n",
    "        page_max = len(reader.pages)\n",
    "        \n",
    "        # getting a specific page from the pdf file\n",
    "        text = \"\"\n",
    "        \n",
    "        for i in range(page_start, page_end + 1):\n",
    "            # print(i)\n",
    "            page = reader.pages[i]\n",
    "            text = text + \"\".join(page.extract_text().splitlines())\n",
    "\n",
    "        with open(text_path, \"w\") as f:\n",
    "            f.write(text)\n",
    "        f.close()\n",
    "\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "# --------------------start of test code--------------------\n",
    "# ind = 1\n",
    "# pdf_file_name = str(index) + \".pdf\"\n",
    "# pdf_folder = fpath.litera_pdf_folder\n",
    "# pdf_path = os.path.join(pdf_folder, pdf_file_name)\n",
    "\n",
    "# text_folder_path = fpath.litera_text_folder\n",
    "# text_file_name = pdf_file_name.split(\".pdf\")[0] + \".txt\"\n",
    "# text_path = os.path.join(text_folder_path, text_file_name)\n",
    "\n",
    "# start_page = 0\n",
    "# end_page = 1\n",
    "# if not pdf2text(pdf_path, text_path, start_page, end_page):\n",
    "#     print(\"Error: pdf2text() failed\")\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pdf to specified folder given pdf_url and ind\n",
    "def download_pdf(pdf_url, pdf_folder):  \n",
    "    url = pdf_url\n",
    "    if url != url:\n",
    "        raise Exception(\"pdf_url is np.nan\")\n",
    "    else:\n",
    "        source = url.split(\"://\")[1].split(\"/\")[0]\n",
    "    \n",
    "    # pdf_path = os.path.join(pdf_folder, file_name)\n",
    "      \n",
    "    # get the pdf content\n",
    "    # options = webdriver.ChromeOptions()\n",
    "    # options.add_experimental_option('prefs', {\n",
    "    # \"download.default_directory\": pdf_folder, #Change default directory for downloads\n",
    "    # \"download.prompt_for_download\": False, #To auto download the file\n",
    "    # \"download.directory_upgrade\": True,\n",
    "    # \"download.open_pdf_in_system_reader\": False,\n",
    "    # \"plugins.always_open_pdf_externally\": True #It will not show PDF directly in chrome\n",
    "    # })\n",
    "    # driver = webdriver.Chrome(options=options)\n",
    "    # # response = driver.get(pdf_url)\n",
    "    # driver.navigate().to(url)\n",
    "    # time.sleep(5)\n",
    "    if source == \"www.sciencedirect.com\":\n",
    "        os.environ['WDM_LOG'] = '0'\n",
    "        options = Options()\n",
    "        options.add_argument('--headless')\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)\n",
    "        element = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.XPATH, \"//li[@class='ViewPDF']/a\")))\n",
    "        element.click()\n",
    "        time.sleep(10)\n",
    "    else: \n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_experimental_option('prefs', {\n",
    "        \"download.default_directory\": pdf_folder, #Change default directory for downloads\n",
    "        \"download.prompt_for_download\": False, #To auto download the file\n",
    "        \"download.directory_upgrade\": True,\n",
    "        \"plugins.always_open_pdf_externally\": True #It will not show PDF directly in chrome\n",
    "        })\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        response = driver.get(url)\n",
    "        time.sleep(10)\n",
    "    driver.close()\n",
    "\n",
    "    # try:\n",
    "    #     with open(pdf_path, 'wb') as pdf_object:\n",
    "    #         pdf_object.write(response.content)\n",
    "    #         driver.close()\n",
    "    #         return True\n",
    "    # except:\n",
    "    #     print(f'Failed downloading PDF:', url)\n",
    "    #     driver.close()\n",
    "    #     # print(f'HTTP response status code: {response.status_code}')\n",
    "    #     return False\n",
    "# --------------------start of test code--------------------\n",
    "# # pdf_url = 'https://www.sciencedirect.com/science/article/pii/S0896627320300052/pdfft?md5=3f0648c6385e6fae3a5a73b053903014&pid=1-s2.0-S0896627320300052-main.pdf'\n",
    "# # pdf_url = \"https://www.sciencedirect.com/science/article/pii/S1053811909013299?via%3Dihub\"\n",
    "# # pdf_url = \"https://linkinghub.elsevier.com/retrieve/pii/S1053811909013299\"\n",
    "# # pdf_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1181753/pdf/jphysiol00456-0559.pdf\"\n",
    "# pdf_url = \"https://www.sciencedirect.com/science/article/pii/S0896627320300052?via%3Dihub\"\n",
    "\n",
    "# # wiley.com\n",
    "# # pdf_url = \"https://onlinelibrary.wiley.com/doi/pdf/10.1002/cne.903130106\"\n",
    "# # pdf_url = \"https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/cne.903130106\"\n",
    "# # pdf_url = \"https://onlinelibrary.wiley.com/doi/pdf/10.1002/cne.902770204\"\n",
    "# # pdf_url = \"https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/cne.902770204\"\n",
    "# # pdf_url = \"https://onlinelibrary.wiley.com/doi/pdf/10.1002/cne.902970309\"\n",
    "# # pdf_url = \"https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/cne.902970309\"\n",
    "\n",
    "# # physiology.org\n",
    "# # pdf_url = \"https://journals.physiology.org/doi/pdf/10.1152/jn.2001.85.1.219\"\n",
    "\n",
    "# # springer.com\n",
    "# # pdf_url = \"https://link.springer.com/content/pdf/10.1007/BF00236173.pdf\"\n",
    "\n",
    "# # cell.com\n",
    "# # pdf_url = \"https://www.cell.com/neuron/pdf/S0896-6273(09)00170-6.pdf\"\n",
    "\n",
    "# # jneurosci.org\n",
    "# # pdf_url = \"https://www.jneurosci.org/content/jneuro/22/18/8117.full.pdf\"\n",
    "# # pdf_url, code = plib.get_final_redirected_url(pdf_url)\n",
    "# # print(pdf_url)\n",
    "# ind = 10\n",
    "# file_name = str(ind) + \".pdf\"\n",
    "# pdf_folder = fpath.litera_pdf_folder\n",
    "# download_pdf(pdf_url, pdf_folder)\n",
    "# # rename_pdf(file_name, pdf_folder, time_to_wait=60)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_www_ncbi_nlm_nih_gov(url, ind, pdf_folder):\n",
    "    try:\n",
    "        file_name = str(ind) + \".pdf\"\n",
    "        response = requests.get(url, headers=plib.headers)\n",
    "        # response = requests.get(url, stream=True, allow_redirects=True, headers=plib.headers)\n",
    "        \n",
    "        # download the .pdf file to the pdf_file_path folder\n",
    "        # write content in pdf file\n",
    "        pdf_path = os.path.join(pdf_folder, file_name)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            with open(pdf_path, 'wb') as pdf_object:\n",
    "                pdf_object.write(response.content)\n",
    "            print(f'Successfully downloaded PDF:', ind)\n",
    "        else:\n",
    "            print(f'Failed downloading PDF:' + 'pdf_url')\n",
    "            print(f'HTTP response status code: {response.status_code}')\n",
    "    except:\n",
    "        print(f'Failed downloading PDF:' + 'pdf_url')\n",
    "# --------------------start of test code--------------------\n",
    "# pdf_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6577493/pdf/jneuro_14_5_2485.pdf\"\n",
    "# ind = 10\n",
    "# file_name = str(ind) + \".pdf\"\n",
    "# pdf_folder = \"/home/hou/myProjects/litera_pdfs\"\n",
    "# download_from_www_ncbi_nlm_nih_gov(pdf_url, ind, pdf_folder)\n",
    "# # rename_pdf(file_name, pdf_folder, time_to_wait=60)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_from_ELSEVIER(doi, file_to_save_to):\n",
    "#     url = 'http://api.elsevier.com/content/article/doi:'+doi+'?view=FULL'\n",
    "#     headers = {\n",
    "#         'X-ELS-APIKEY': \"63f58b8b10cbc1bc923011c01c6301bb\",\n",
    "#         'Accept': 'application/pdf'\n",
    "#     }\n",
    "#     r = requests.get(url, stream=True, headers=headers)\n",
    "#     if r.status_code == 200:\n",
    "#         for chunk in r.iter_content(chunk_size=1024*1024):\n",
    "#             file_to_save_to.write(chunk)\n",
    "#         return True\n",
    "\n",
    "# # doi_list = pd.read_excel('list.xls')\n",
    "# # doi_list.columns = ['DOIs']\n",
    "# count = 0\n",
    "# # for doi in doi_list['DOIs']:\n",
    "# doi = \"10.1016/0006-8993(95)01338-5\"\n",
    "# # pdf = \"10\"\n",
    "# ind = 10\n",
    "# file_name = str(ind) + \".pdf\"\n",
    "# # if not os.path.exists(f'path/{pdf}.pdf'):\n",
    "# with open(file_name, 'wb') as pdf_object:\n",
    "#     get_pdf(doi, pdf_object)\n",
    "# pdf_object.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pdf\n",
    "def download_pdf(pdf_url, pdf_source, pdf_folder):\n",
    "    for website in params.website_pdfs:\n",
    "        func = None\n",
    "        if website in pdf_source:\n",
    "            # Get the function name by replacing \".\" with \"_\" and use globals() to call it\n",
    "            func_name = \"download_from_\" + website.replace(\".\", \"_\")\n",
    "            # print(func_name)\n",
    "            func = globals().get(func_name)\n",
    "            # print(func)\n",
    "            break\n",
    "    if func != None:\n",
    "        # print(func)\n",
    "        func(pdf_url, pdf_folder)\n",
    "    else:\n",
    "        print(\"The given url is not from a supported website: \", pdf_url)\n",
    "        raise Exception(\"Function does not exist for website:\", pdf_url)\n",
    "\n",
    "# rename downloaded pdf\n",
    "def rename_pdf(ind, pdf_folder, time_to_wait=60):\n",
    "    newname = str(ind) + \".pdf\"\n",
    "    time_counter = 0\n",
    "    filename = max([f for f in os.listdir(pdf_folder)], key=lambda xa :   os.path.getctime(os.path.join(pdf_folder,xa)))\n",
    "    while '.part' in filename:\n",
    "        time.sleep(1)\n",
    "        time_counter += 1\n",
    "        if time_counter > time_to_wait:\n",
    "            raise Exception('Waited too long for file to download')\n",
    "    filename = max([f for f in os.listdir(pdf_folder)], key=lambda xa :   os.path.getctime(os.path.join(pdf_folder,xa)))\n",
    "    os.rename(os.path.join(pdf_folder, filename), os.path.join(pdf_folder, newname))\n",
    "\n",
    "# download and rename pdf\n",
    "def download_and_rename_pdf(pdf_url, ind, pdf_folder):  \n",
    "    if pdf_url != pdf_url:\n",
    "        raise Exception(\"pdf_url is np.nan\")\n",
    "    else:\n",
    "        pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "    download_pdf(pdf_url, pdf_source, pdf_folder)\n",
    "    rename_pdf(ind, pdf_folder, time_to_wait=60)\n",
    "    print(f\"Downloaded {ind}.pdf\")\n",
    "# --------------------start of test code--------------------\n",
    "# pdf_url = \"https://journals.physiology.org/doi/pdf/10.1152/jn.2001.85.1.219\"\n",
    "# ind = 10\n",
    "# pdf_folder = \"/home/hou/myProjects/litera_pdfs\"\n",
    "# download_and_rename_pdf(pdf_url, ind, pdf_folder)\n",
    "# ---------------------end of test code---------------------\n",
    "# https://journals.physiology.org/doi/10.1152/jn.2001.85.1.219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiley.com\n",
    "def wiley_com(url):\n",
    "    # initialize\n",
    "    info = {\n",
    "        \"doi\": np.nan,\n",
    "        \"pmid\": np.nan,\n",
    "        \"pmcid\": np.nan,\n",
    "        \"title\": np.nan,\n",
    "        \"abstract\": np.nan,\n",
    "        \"keywords\": np.nan,\n",
    "        \"pdf_link\": np.nan\n",
    "    }\n",
    "\n",
    "    # set up the webdriver\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "\n",
    "    # load the webpage\n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"Extracting content from:\" + url + \" failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "    \n",
    "    # doi\n",
    "    try:\n",
    "        doi = driver.find_element(By.XPATH, \"//a[@class='epub-doi']\").text.split(\"doi.org/\")[1]\n",
    "        doi = doi.strip()\n",
    "    except:\n",
    "        doi = np.nan\n",
    "    if doi == doi:\n",
    "        doi = doi.lower()\n",
    "\n",
    "    # pmid, pmcid\n",
    "    pmid = np.nan\n",
    "    pmcid = np.nan\n",
    "\n",
    "    # title\n",
    "    try:\n",
    "        title = driver.find_element(By.XPATH, \"//h1[@class='citation__title']\").text\n",
    "        title = title.strip()\n",
    "    except:\n",
    "        try:\n",
    "            title = driver.find_element(By.XPATH, \"//h2[@class='citation__title']\").text\n",
    "            title = title.strip()\n",
    "        except:\n",
    "            title = np.nan\n",
    "    \n",
    "    # abstract\n",
    "    try:\n",
    "        abstract = \"\"\n",
    "        try:\n",
    "            elems = driver.find_element(By.XPATH, \"//h3[text()='Abstract' or text()='ABSTRACT' or text()='Summary' or text()='SUMMARY']\").find_element(By.XPATH, \"following-sibling::div\").find_elements(By.XPATH, 'p')\n",
    "        except:\n",
    "            elems = driver.find_element(By.XPATH, \"//h2[text()='Abstract' or text()='ABSTRACT' or text()='Summary' or text()='SUMMARY']\").find_element(By.XPATH, \"following-sibling::div\").find_elements(By.XPATH, 'p')                \n",
    "        for elem in elems:\n",
    "            abstract = abstract + elem.text + \" \"\n",
    "        abstract = abstract.strip()\n",
    "    except:\n",
    "        abstract = np.nan\n",
    "    \n",
    "    # keywords\n",
    "    keywords = np.nan\n",
    "\n",
    "    # pdf_link\n",
    "    try:\n",
    "        pdf_link = driver.find_element(By.XPATH, \"//a[@title='ePDF']\").get_attribute('href')\n",
    "        pdf_link = pdf_link.strip()\n",
    "    except:\n",
    "        pdf_link = np.nan\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/abs/10.1002/cne.901980111\"\n",
    "# url = \"https://onlinelibrary.wiley.com/doi/10.1002/cne.21155\"\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/10.1002/(SICI)1096-9861(19981019)400:2%3C271::AID-CNE8%3E3.0.CO;2-6\"\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/10.1002/cne.902360304\"\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/10.1002/cne.902820107\"\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/10.1002/(SICI)1096-9861(19990726)410:2%3C211::AID-CNE4%3E3.0.CO;2-X\"\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/10.1002/cne.902940314\"\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/10.1002/cne.901990104\"\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/10.1002/(SICI)1096-9861(19981019)400:2%3C271::AID-CNE8%3E3.0.CO;2-6\"\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/10.1002/cne.902360304\"\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/10.1002/cne.24389\"\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/10.1002/(SICI)1096-9861(19960805)371:4%3C513::AID-CNE2%3E3.0.CO;2-7\"\n",
    "# # url = \"https://nyaspubs.onlinelibrary.wiley.com/doi/full/10.1196/annals.1300.030\"\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/10.1002/cne.23436\"\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/10.1002/9780470513545.ch4\"\n",
    "# info = wiley_com(url)\n",
    "# print(info[\"doi\"])\n",
    "# print(info[\"pmid\"])\n",
    "# print(info[\"pmcid\"])\n",
    "# print(info[\"title\"])\n",
    "# print(info[\"abstract\"])\n",
    "# print(info[\"keywords\"])\n",
    "# print(info[\"pdf_link\"])\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# www.science.org\n",
    "def www_science_org(url):\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    # load the webpage\n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver = webdriver.Firefox(options=options)\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"Extracting content from:\" + url + \" failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "    \n",
    "    # doi\n",
    "    try:\n",
    "        doi = driver.find_element(By.XPATH, \"//div[contains(@class, 'doi')]\").find_element(By.XPATH, \"a\").text.split(\"DOI: \")[1]\n",
    "    except:\n",
    "        doi = np.nan\n",
    "\n",
    "    pmid = np.nan\n",
    "    pmcid = np.nan\n",
    "\n",
    "    # title\n",
    "    try:\n",
    "        title = driver.find_element(By.XPATH, \"//h1[@property='name']\").text\n",
    "        title = title.strip()\n",
    "    except:        \n",
    "        title = np.nan\n",
    "    \n",
    "    # abstract\n",
    "    try:\n",
    "        abstract = driver.find_element(By.XPATH, \"//h2[text()='Abstract']\").find_element(By.XPATH, \"following-sibling::div\").text\n",
    "        abstract = abstract.strip()\n",
    "    except:\n",
    "        abstract = np.nan\n",
    "    \n",
    "    # keywords\n",
    "    keywords = np.nan\n",
    "\n",
    "    # pdf_link\n",
    "    try:\n",
    "        pdf_link = driver.find_element(By.XPATH, \"//div[@class='info-panel__formats info-panel__item']\").find_element(By.XPATH, \"//a[@class='btn btn--slim btn-secondary']\").get_attribute('href')\n",
    "        pdf_link = pdf_link.strip()\n",
    "    except:\n",
    "        pdf_link = np.nan\n",
    "    # pdf_link = np.nan\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "    driver.quit\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "# url = \"https://www.science.org/doi/full/10.1126/science.282.5391.1117\"\n",
    "# # url = \"https://www.science.org/doi/10.1126/science.7939688\"\n",
    "# # url = \"https://www.science.org/doi/10.1126/science.1109154\"\n",
    "# info = www_science_org(url)\n",
    "# print(info[\"doi\"])\n",
    "# print(info[\"pmid\"])\n",
    "# print(info[\"pmcid\"])\n",
    "# print(info[\"title\"])\n",
    "# print(info[\"abstract\"])\n",
    "# print(info[\"keywords\"])\n",
    "# print(info[\"pdf_link\"])\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.1001/archneurpsyc.1941.02280210028002\n",
      "nan\n",
      "nan\n",
      "CORPUS STRIATUM AND THALAMUS OF A PARTIALLY DECORTICATE MONKEY\n",
      "nan\n",
      "nan\n",
      "://jamanetwork.com/\n"
     ]
    }
   ],
   "source": [
    "# jamanetwork.com\n",
    "def jamanetwork_com(url):\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    # load the webpage\n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver = webdriver.Firefox(options=options)\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"Extracting content from:\" + url + \" failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "    \n",
    "    try:\n",
    "        doi = np.nan\n",
    "        elems = driver.find_element(By.XPATH, \"//div[contains(@class, 'meta-citation-wrap')]\").find_elements(By.TAG_NAME, \"span\")\n",
    "        for elem in elems:\n",
    "            if \"doi:\" in elem.text:\n",
    "                doi = elem.text.split(\"doi:\")[1]\n",
    "    except:\n",
    "        doi = np.nan\n",
    "    pmid = np.nan\n",
    "    pmcid = np.nan\n",
    "\n",
    "    # title\n",
    "    try:\n",
    "        title = driver.find_element(By.XPATH, \"//h1[contains(@class, 'meta-article-title')]\").text\n",
    "        title = title.strip()\n",
    "    except:\n",
    "        title = np.nan\n",
    "    \n",
    "    # abstract\n",
    "    try:\n",
    "        abstract = \"\"\n",
    "\n",
    "        # Find all div and p elements on the page\n",
    "        all_divs = driver.find_element(By.XPATH, \"//div[@class='article-full-text']\").find_elements(By.TAG_NAME, 'div')\n",
    "        all_ps = driver.find_element(By.XPATH, \"//div[@class='article-full-text']\").find_elements(By.TAG_NAME, 'p')\n",
    "\n",
    "        # Assuming the two div elements you're interested in are the first and second divs on the page\n",
    "        first_div = all_divs[0]\n",
    "        second_div = all_divs[1]\n",
    "\n",
    "        # Find the indices of these div elements in the list of all p and div elements\n",
    "        first_div_index = all_ps.index(first_div)\n",
    "        second_div_index = all_ps.index(second_div)\n",
    "\n",
    "        # Extract all p elements that are between the two divs\n",
    "        ps_between_divs = all_ps[first_div_index + 1: second_div_index]\n",
    "\n",
    "        # Do something with the p elements (e.g., print their text)\n",
    "        for elem in ps_between_divs:\n",
    "            abstract = abstract + elem.text + \" \"\n",
    "\n",
    "        # elems1 = driver.find_element(By.XPATH, \"//div[@class='article-full-text']/div[1]\").find_elements(By.XPATH, \"following-sibling::p\")\n",
    "        # elems2 = driver.find_element(By.XPATH, \"//div[@class='article-full-text']/div[2]\").find_elements(By.XPATH, \"following-sibling::p\")\n",
    "        # elems = elems1 - elems2\n",
    "        # for elem in elems:\n",
    "        #     abstract = abstract + elem.text + \" \"\n",
    "    except:\n",
    "        abstract = np.nan\n",
    "    \n",
    "    # keywords\n",
    "    keywords = np.nan\n",
    "\n",
    "    # pdf_link\n",
    "    # try:\n",
    "    #     pdf_link = driver.find_element(By.XPATH, \"//a[@id='pdf-link']\").get_attribute('data-article-url')\n",
    "    #     pdf_link = pdf_link.strip()\n",
    "    # except:\n",
    "    #     pdf_link = np.nan\n",
    "    pdf_link = \"://jamanetwork.com/\"\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "    driver.quit\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "url = \"https://jamanetwork.com/journals/archneurpsyc/article-abstract/648966\"\n",
    "# url = \"https://jamanetwork.com/journals/jamapsychiatry/fullarticle/482591\"\n",
    "# url = \"https://jamanetwork.com/journals/jamaneurology/article-abstract/577288\"\n",
    "# url = \"https://jamanetwork.com/journals/jamaneurology/article-abstract/567406\"\n",
    "# url = \"https://jamanetwork.com/journals/jamaophthalmology/fullarticle/412961\"\n",
    "# url = \"https://jamanetwork.com/journals/jamaneurology/article-abstract/574320\"\n",
    "# url = \"https://jamanetwork.com/journals/jamaophthalmology/article-abstract/632332\"\n",
    "info = jamanetwork_com(url)\n",
    "print(info[\"doi\"])\n",
    "print(info[\"pmid\"])\n",
    "print(info[\"pmcid\"])\n",
    "print(info[\"title\"])\n",
    "print(info[\"abstract\"])\n",
    "print(info[\"keywords\"])\n",
    "print(info[\"pdf_link\"])\n",
    "# ---------------------end of test code---------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
