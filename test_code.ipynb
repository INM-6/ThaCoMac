{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "# import PyPDF2\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from requests.auth import HTTPProxyAuth\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "# from selenium import webdriver\n",
    "# from webdriver_manager.firefox import GeckoDriverManager\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.firefox.options import Options\n",
    "# from selenium.webdriver.support.wait import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from webdriver_manager.firefox import GeckoDriverManager\n",
    "# from selenium.webdriver.firefox.service import Service\n",
    "# from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException\n",
    "import os\n",
    "import re\n",
    "from lxml import etree\n",
    "from nltk import ngrams\n",
    "import json\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib\n",
    "import extract_info\n",
    "import parameters as params\n",
    "import download_and_process_pdf as dpp\n",
    "import dataframe_columns as df_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # scan the rows in the test 1000 csv file and print the full_text_url and pdf_url of the papers that have no pdf_url\n",
    "# input_path = fpath.poten_litera_testing_set_1000_text_extract_and_count\n",
    "# df = pd.read_csv(input_path, header=0, sep=',')\n",
    "\n",
    "# for ind in df.index:\n",
    "#     index = df.at[ind, \"INDEX\"]\n",
    "#     full_text_url = df.at[ind, \"FULL_TEXT_URL\"]\n",
    "#     pdf_url = df.at[ind, \"PDF_URL\"]\n",
    "#     relevant = df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"]\n",
    "    \n",
    "#     line_number_in_csv = ind + 2\n",
    "#     print(\"Line number:\", line_number_in_csv, \"INDEX:\", index)\n",
    "#     print(\"ind: \", ind, )\n",
    "#     print(\"full_text_url: \", full_text_url)\n",
    "#     print(\"pdf_url: \", pdf_url)\n",
    "#     print(\"relevant: \", relevant)\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # scan the rows in the final_manually_read_csv_i.csv file\n",
    "# # print the full_text_url and pdf_url of the papers that have no pdf_url\n",
    "# input_path = fpath.final_manually_read_csv_2\n",
    "# df = pd.read_csv(input_path, header=0, sep='\\t')\n",
    "\n",
    "# for ind in df.index:\n",
    "#     index = df.at[ind, \"INDEX\"]\n",
    "#     full_text_url = df.at[ind, \"FULL_TEXT_URL\"]\n",
    "#     pdf_url = df.at[ind, \"PDF_URL\"]\n",
    "#     relevant = df.at[ind, \"RELEVANT?(Y/N/MB/NA)\"]\n",
    "    \n",
    "#     line_number_in_csv = ind + 2\n",
    "#     print(\"Line number:\", line_number_in_csv, \"INDEX:\", index)\n",
    "#     print(\"ind: \", ind, )\n",
    "#     print(\"full_text_url: \", full_text_url)\n",
    "#     print(\"pdf_url: \", pdf_url)\n",
    "#     print(\"relevant: \", relevant)\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# A = pd.read_csv('./datasets/macaque_corticoCortical_adjacency.csv', index_col=0)\n",
    "\n",
    "# # Create non-directed graph\n",
    "# G = nx.convert_matrix.from_pandas_adjacency(A, create_using=nx.Graph)\n",
    "\n",
    "# # print(G.nodes)\n",
    "\n",
    "# # pos = nx.spring_layout(G, seed=24)\n",
    "# # nx.draw_networkx(G, pos, with_labels=False, node_size=0, edge_cmap='Greys', width=0.1, alpha=0.5)\n",
    "\n",
    "# # # ALl this just to make the nodes certain colors\n",
    "# # macaque_regions = ['Visual', 'Temporal', 'Parietal', 'Somatosensory', 'Auditory',\n",
    "# #                    'Motor', 'Prefrontal', 'Frontal', 'Orbitofrontal']\n",
    "# # colors = ['#4c6ef5', '#228be6', '#15aabf', '#12b886', '#40c057', '#82c91e', '#fab005', '#fd7e14']\n",
    "# # for region, c in zip(macaque_regions, colors):\n",
    "# #     # subset = subset = macaque.get_areas_in_regions(region)\n",
    "# #     subset = get_areas_in_regions(region)\n",
    "# #     nodes = []\n",
    "# #     for node in subset:\n",
    "# #         if node in G.nodes:\n",
    "# #             nodes.append(node)\n",
    "# #     nx.draw_networkx_nodes(G, pos, nodelist=nodes, node_color=c, node_size=30, alpha=0.8, label=region)\n",
    "\n",
    "# # # Create labels on top\n",
    "# # nx.draw_networkx(G, pos, with_labels=True, node_size=0, width=0, alpha=1, font_size=7)\n",
    "\n",
    "# # plt.legend(frameon=False, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# # plt.savefig('Cortico_cortical_connectivity_macaque.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To show the adjacency matrix\n",
    "# plt.imshow(A)\n",
    "# plt.yticks(range(len(A.columns)), A.columns, fontsize=7)\n",
    "# plt.ylabel('Source areas', fontsize=10)\n",
    "# plt.xticks(range(len(A.columns)), A.columns, rotation=90, fontsize=7)\n",
    "# plt.xlabel('Target areas', fontsize=10)\n",
    "# plt.colorbar()\n",
    "\n",
    "# # plt.subplots_adjust(bottom=0.1)\n",
    "# plt.savefig('Cortico_cortical_connectivity_heatmap_macaque.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Data\n",
    "# labels1 = ['Google Scholar', 'Web of Science', 'PubMed', 'Europe PMC']\n",
    "# sizes1 = [980, 1993, 2612, 9178]\n",
    "# labels2 = ['Full text not available', 'Full text available']\n",
    "# sizes2 = [226, 10550]\n",
    "\n",
    "# # Custom function to format the label\n",
    "# def func(pct, allvalues): \n",
    "#     absolute = int(pct / 100.*np.sum(allvalues)) \n",
    "#     return \"{:.1f}%\\n({:d} )\".format(pct, absolute)\n",
    "\n",
    "# # Cold color palette\n",
    "# colors1 = ['#85C1E9', '#3498DB', '#2874A6', '#1B4F72']\n",
    "# colors2 = ['#AED6F1', '#2E86C1']\n",
    "\n",
    "# # Plotting\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))  # 1 row, 2 columns\n",
    "\n",
    "# # Font sizes\n",
    "# title_fontsize = 16\n",
    "# label_fontsize = 15\n",
    "# autopct_fontsize = 10\n",
    "\n",
    "# # First pie chart\n",
    "# ax1.pie(sizes1, labels=labels1, autopct=lambda pct: func(pct, sizes1), \n",
    "#         startangle=90, colors=colors1, textprops={'fontsize': label_fontsize})\n",
    "# ax1.set_title(\"A\", loc='left', fontsize=title_fontsize)\n",
    "# ax1.axis('equal')\n",
    "\n",
    "# # Second pie chart\n",
    "# ax2.pie(sizes2, labels=labels2, autopct=lambda pct: func(pct, sizes2), \n",
    "#         startangle=90, colors=colors2, textprops={'fontsize': label_fontsize})\n",
    "# ax2.set_title(\"B\", loc='left', fontsize=title_fontsize)\n",
    "# ax2.axis('equal')\n",
    "\n",
    "# # Show both pie charts side by side\n",
    "# plt.tight_layout()  # Adjusts the space between the two plots for better visualization\n",
    "# plt.show()\n",
    "\n",
    "# # Save figure\n",
    "# fig.savefig('pie_chart.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create poten_litera_testing_set_1000_labeled.csv\n",
    "# input_path = fpath.poten_litera_testing_set_1000_text_extract_and_count\n",
    "# output_path = fpath.poten_litera_testing_set_1000_labeled\n",
    "# # clear the file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(input_path, header=0, sep=',')\n",
    "# columns = [\n",
    "#     \"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"PDF_URL\", \n",
    "#     \"TITLE\", \"TEXT_TAK\", \"TEXT_500\"\n",
    "# ]\n",
    "# columns = columns + df_col.text_columns_to_add # add keyword group text\n",
    "# columns = columns + df_col.count_columns_to_add # add keyword group count\n",
    "# columns_to_add = [\"TT?(Y/N/MB/NA)\", \"MACAQUE?(Y/N/MB/NA)\", \"TC_OR_CT?(Y/N/MB/NA)\", \"RELEVANT?(Y/N/MB/NA)\", \"READ_BY(A/D/R)\", \"COMMENT\"] # add columns for documenting labels\n",
    "# columns = columns + columns_to_add\n",
    "# df.columns = columns\n",
    "\n",
    "\n",
    "# # select only the columns INDEX, DOI and PMID and save the dataframe as a csv file\n",
    "# df = df[[\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"TT?(Y/N/MB/NA)\", \"MACAQUE?(Y/N/MB/NA)\", \"TC_OR_CT?(Y/N/MB/NA)\", \"RELEVANT?(Y/N/MB/NA)\", \"READ_BY(A/D/R)\", \"COMMENT\"]]\n",
    "# df.to_csv(output_path, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create poten_litera_testing_set_1000_labeled.csv\n",
    "# input_path = fpath.poten_litera_testing_set_1000_text_extract_and_count\n",
    "# output_path = fpath.poten_litera_testing_set_1000_labeled\n",
    "# # clear the file\n",
    "# plib.clear_file(output_path)\n",
    "\n",
    "# df = pd.read_csv(input_path, header=0, sep=',')\n",
    "# columns = [\n",
    "#     \"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"FULL_TEXT_URL\", \"PDF_URL\", \n",
    "#     \"TITLE\", \"TEXT_TAK\", \"TEXT_500\"\n",
    "# ]\n",
    "# columns = columns + df_col.text_columns_to_add # add keyword group text\n",
    "# columns = columns + df_col.count_columns_to_add # add keyword group count\n",
    "# columns_to_add = [\"TT?(Y/N/MB/NA)\", \"MACAQUE?(Y/N/MB/NA)\", \"TC_OR_CT?(Y/N/MB/NA)\", \"RELEVANT?(Y/N/MB/NA)\", \"READ_BY(A/D/R)\", \"COMMENT\"] # add columns for documenting labels\n",
    "# columns = columns + columns_to_add\n",
    "# df.columns = columns\n",
    "\n",
    "\n",
    "# # select only the columns INDEX, DOI and PMID and save the dataframe as a csv file\n",
    "# df = df[[\"INDEX\", \"DOI\", \"PMID\", \"PMCID\", \"TT?(Y/N/MB/NA)\", \"MACAQUE?(Y/N/MB/NA)\", \"TC_OR_CT?(Y/N/MB/NA)\", \"RELEVANT?(Y/N/MB/NA)\", \"READ_BY(A/D/R)\", \"COMMENT\"]]\n",
    "# df.to_csv(output_path, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_path = fpath.poten_litera_db\n",
    "# with open(input_path, 'r') as file:\n",
    "#     content = file.read()\n",
    "\n",
    "# # Replace tabs with spaces\n",
    "# content_with_spaces = content.replace('\\t', ' ')\n",
    "\n",
    "# with open(input_path, 'w') as file:\n",
    "#     file.write(content_with_spaces)\n",
    "    \n",
    "# df = pd.read_csv(input_path, header=None, sep=\",\")\n",
    "# plib.clear_file(input_path)\n",
    "# df.columns = df_col.db_columns\n",
    "# df.to_csv(input_path, index=False, header=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = fpath."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
