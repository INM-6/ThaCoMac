{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 17:28:46 GM01X680 metapub.config[477500] WARNING NCBI_API_KEY was not set.\n"
     ]
    }
   ],
   "source": [
    "# import internal .py modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from requests.auth import HTTPProxyAuth\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException\n",
    "import os\n",
    "import re\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = [\n",
    "    'ncbi.nlm.nih.gov', 'elsevier.com', 'wiley.com', 'springer.com', 'physiology.org', 'oup.com', \n",
    "    'cambridge.org', 'karger.com', 'lww.com', 'nature.com', 'science.org', 'tandfonline.com', \n",
    "    'sagepub.com', 'jamanetwork.com', 'neurology.org', 'biorxiv.org', 'royalsocietypublishing.org', \n",
    "    'psycnet.apa.org', 'arvojournals.org', 'jstage.jst.go.jp', 'psychiatryonline.org', 'europepmc.org', \n",
    "    'mit.edu', 'thejns.org', 'annualreviews.org', 'snmjournals.org', 'aspetjournals.org', 'elibrary.ru', \n",
    "    'books.google.de', 'architalbiol.org', 'ahajournals.org', 'liebertpub.com', 'acs.org', 'degruyter.com', \n",
    "    'worldscientific.com', 'iospress.com', 'asahq.org', 'thieme-connect.de', 'neurologia.com', 'mpg.de', \n",
    "    'opg.optica.org', 'mcgill.ca', 'rbojournal.org', 'taylorfrancis.com', 'ekja.org', 'www.imrpress.com', \n",
    "    'theses.fr', 'ieee.org', 'ssrn.com', 'open.bu.edu', 'journals.biologists.com', 'aip.org', 'lib.wfu.edu', \n",
    "    'literatumonline.com', 'scholarpedia.org', 'isho.jp', 'mirasmart.com', 'microbiologyresearch.org', \n",
    "    'aps.org', 'ujms.net', 'www.ingentaconnect.com', 'symposium.cshlp.org', 'cabdirect.org', 'ajtmh.org'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# www.nature.com\n",
    "def www_nature_com(url):\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    # load the webpage\n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver = webdriver.Chrome()\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"Extracting content from:\" + url + \" failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "    \n",
    "    try:\n",
    "        elems = driver.find_elements(By.XPATH, \"//span[@class='c-bibliographic-information__value')]\")\n",
    "        for elem in elems:\n",
    "            if \"doi.org/\" in elem.text:\n",
    "                doi = elem.text.split(\"doi.org/\")[1]\n",
    "    except:\n",
    "        doi = np.nan\n",
    "    pmid = np.nan\n",
    "    pmcid = np.nan\n",
    "    try:\n",
    "        title = driver.find_element(By.XPATH, \"//h1[contains(@class, 'c-article-title')]\").text\n",
    "    except:\n",
    "        title = np.nan\n",
    "    abstract = np.nan\n",
    "    keywords = np.nan\n",
    "    intro = np.nan\n",
    "    pdf_link = np.nan\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"introduction\": intro,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "    driver.quit\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "# url = \"https://www.nature.com/articles/387281a0\"\n",
    "# info = www_nature_com(url)\n",
    "# print(info[\"doi\"])\n",
    "# print(info[\"pmid\"])\n",
    "# print(info[\"pmcid\"])\n",
    "# print(info[\"title\"])\n",
    "# print(info[\"abstract\"])\n",
    "# print(info[\"keywords\"])\n",
    "# print(info[\"introduction\"])\n",
    "# print(info[\"pdf_link\"])\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nature.com\n",
    "def func_nature_com(url):\n",
    "    # initialize\n",
    "    info = {\n",
    "        \"doi\": np.nan,\n",
    "        \"pmid\": np.nan,\n",
    "        \"pmcid\": np.nan,\n",
    "        \"title\": np.nan,\n",
    "        \"abstract\": np.nan,\n",
    "        \"keywords\": np.nan,\n",
    "        \"introduction\": np.nan,\n",
    "        \"pdf_link\": np.nan\n",
    "    }\n",
    "\n",
    "    # set up the webdriver\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # load the webpage\n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            wait = WebDriverWait(driver, 30)\n",
    "            wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Accept All Cookies')]\"))).click()\n",
    "            time.sleep(2)\n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"Extracting content from:\" + url + \" failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "    \n",
    "    # doi, pmid, pmcid\n",
    "    try:\n",
    "        doi = driver.find_element(By.XPATH, \"//div[contains(@class, 'ej-journal-info')]\").text.split(\"DOI: \")[1]\n",
    "        doi = doi.strip()\n",
    "    except:\n",
    "        doi = np.nan\n",
    "    pmid = np.nan\n",
    "    pmcid = np.nan\n",
    "\n",
    "    # title\n",
    "    try:\n",
    "        title = driver.find_element(By.XPATH, \"//h1[@class='ejp-article-title']\").text\n",
    "        title = title.strip()\n",
    "    except:\n",
    "        title = np.nan\n",
    "    \n",
    "    # abstract\n",
    "    try:\n",
    "        abstract = \"\"\n",
    "        elems = driver.find_element(By.XPATH, \"//div[@class='ejp-article-text-abstract']\").find_elements(By.TAG_NAME, 'p')\n",
    "        for elem in elems:\n",
    "            abstract = abstract + elem.text + \" \"\n",
    "        abstract = abstract.strip()\n",
    "    except:\n",
    "        abstract = np.nan\n",
    "    \n",
    "    # keywords\n",
    "    # try:\n",
    "    #     keywords = \"\"\n",
    "    #     elems = driver.find_element(By.XPATH, \"//div[@class='ejp-article-text-abstract']\").find_elements(By.TAG_NAME, 'a')\n",
    "    #     for elem in elems:\n",
    "    #         keywords = keywords + elem.text + \"; \"\n",
    "    #     keywords = keywords.strip()\n",
    "    # except:\n",
    "    #     keywords = np.nan\n",
    "    keywords = np.nan\n",
    "    \n",
    "    # # introduction\n",
    "    # try:\n",
    "    #     intro = \"\"\n",
    "    #     elements = driver.find_elements(By.TAG_NAME, \"h2\")\n",
    "    #     for element in elements:\n",
    "    #         if \"Introduction\" in element.text:\n",
    "    #             ele_paren = element.find_element(By.XPATH, \"..\")\n",
    "    #             intros = ele_paren.find_elements(By.TAG_NAME, \"p\")\n",
    "    #             for intro_ele in intros:\n",
    "    #                 intro = intro + intro_ele.text + \" \"\n",
    "    #             break\n",
    "    #     intro = intro.strip()\n",
    "    # except:\n",
    "    #     intro = np.nan\n",
    "    intro = np.nan\n",
    "\n",
    "    # pdf_link\n",
    "    # try:\n",
    "    #     pdf_link = driver.find_element(By.XPATH, \"//button[contains(text(), 'PDF')]\").click()\n",
    "    #     driver.switch_to_window(driver.window_handles[-1])\n",
    "    #     pdf_link = driver.current_url\n",
    "    #     pdf_link = pdf_link.strip()\n",
    "    # except:\n",
    "    #     pdf_link = np.nan\n",
    "    pdf_link = np.nan\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"introduction\": intro,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "# # url = \n",
    "# # url = \n",
    "# # url = \n",
    "# print(info[\"doi\"])\n",
    "# print(info[\"pmid\"])\n",
    "# print(info[\"pmcid\"])\n",
    "# print(info[\"title\"])\n",
    "# print(info[\"abstract\"])\n",
    "# print(info[\"keywords\"])\n",
    "# print(info[\"introduction\"])\n",
    "# print(info[\"pdf_link\"])\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# science.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tandfonline.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagepub.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jamanetwork.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neurology.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biorxiv.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text from given .pdf file\n",
    "def pdf2text(pdf_path, page_start, page_end):\n",
    "    # creating a pdf reader object\n",
    "    reader = PyPDF2.PdfReader(pdf_path)\n",
    "    \n",
    "    # printing number of pages in pdf file\n",
    "    page_max = len(reader.pages)\n",
    "    \n",
    "    # getting a specific page from the pdf file\n",
    "    text = \"\"\n",
    "    \n",
    "    for i in range(page_start, page_end + 1):\n",
    "        # print(i)\n",
    "        page = reader.pages[i]\n",
    "        text = text + \"\".join(page.extract_text().splitlines())\n",
    "\n",
    "    return text\n",
    "# --------------------start of test code--------------------\n",
    "# pdf_folder_path = fpath.litera_pdf_folder\n",
    "# pdf_file_name = \"1.pdf\"\n",
    "# pdf_path = os.path.join(pdf_folder_path, pdf_file_name)\n",
    "\n",
    "# text_folder_path = fpath.litera_text_folder\n",
    "# text_file_name = pdf_file_name.split(\".pdf\")[0] + \".txt\"\n",
    "# text_path = os.path.join(text_folder_path, text_file_name)\n",
    "\n",
    "# start_page = 0\n",
    "# end_page = 1\n",
    "# text = pdf2text(pdf_path, start_page, end_page)\n",
    "\n",
    "# with open(text_path, \"w\") as f:\n",
    "#     f.write(text)\n",
    "\n",
    "# f.close()\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pdf to specified folder given pdf_url and file name\n",
    "def download_pdf(pdf_url, pdf_folder_path, file_name):    \n",
    "    # get the pdf content\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_experimental_option('prefs', {\n",
    "    \"download.default_directory\": pdf_folder_path, #Change default directory for downloads\n",
    "    \"download.prompt_for_download\": False, #To auto download the file\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True #It will not show PDF directly in chrome\n",
    "    })\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    response  = driver.get(pdf_url)\n",
    "    time.sleep(5)\n",
    "    # response = requests.get(pdf_url, stream=True, headers=plib.headers)\n",
    "    \n",
    "    # download the .pdf file to the pdf_file_path folder\n",
    "    # write content in pdf file\n",
    "    # pdf_path = os.path.join(pdf_folder_path, file_name + '.pdf')\n",
    "    \n",
    "    # if response.status_code == 200:\n",
    "    #     with open(pdf_path, 'wb') as pdf_object:\n",
    "    #         pdf_object.write(response.content)\n",
    "    #         return True\n",
    "    # else:\n",
    "    #     print(f'Failed downloading PDF:' + 'pdf_url')\n",
    "    #     print(f'HTTP response status code: {response.status_code}')\n",
    "    #     return False\n",
    "# --------------------start of test code--------------------\n",
    "# pdf_url = 'https://www.sciencedirect.com/science/article/pii/S0896627320300052/pdfft?md5=3f0648c6385e6fae3a5a73b053903014&pid=1-s2.0-S0896627320300052-main.pdf'\n",
    "# pdf_folder_path = fpath.litera_pdf_folder\n",
    "# file_name = '4'\n",
    "# download_pdf(pdf_url, pdf_folder_path, file_name)\n",
    "# ---------------------end of test code---------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
