{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal modules\n",
    "import file_path_management as fpath\n",
    "import public_library as plib\n",
    "import extract_info\n",
    "import parameters as params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from requests.auth import HTTPProxyAuth\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException\n",
    "import os\n",
    "import re\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text from given .pdf file\n",
    "def pdf2text(pdf_path, text_path, page_start, page_end):\n",
    "    try:\n",
    "        # creating a pdf reader object\n",
    "        reader = PyPDF2.PdfReader(pdf_path)\n",
    "        \n",
    "        # printing number of pages in pdf file\n",
    "        page_max = len(reader.pages)\n",
    "        \n",
    "        # getting a specific page from the pdf file\n",
    "        text = \"\"\n",
    "        \n",
    "        for i in range(page_start, page_end + 1):\n",
    "            # print(i)\n",
    "            page = reader.pages[i]\n",
    "            text = text + \"\".join(page.extract_text().splitlines())\n",
    "\n",
    "        with open(text_path, \"w\") as f:\n",
    "            f.write(text)\n",
    "        f.close()\n",
    "\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "# --------------------start of test code--------------------\n",
    "# ind = 1\n",
    "# pdf_file_name = str(index) + \".pdf\"\n",
    "# pdf_folder = fpath.litera_pdf_folder\n",
    "# pdf_path = os.path.join(pdf_folder, pdf_file_name)\n",
    "\n",
    "# text_folder_path = fpath.litera_text_folder\n",
    "# text_file_name = pdf_file_name.split(\".pdf\")[0] + \".txt\"\n",
    "# text_path = os.path.join(text_folder_path, text_file_name)\n",
    "\n",
    "# start_page = 0\n",
    "# end_page = 1\n",
    "# if not pdf2text(pdf_path, text_path, start_page, end_page):\n",
    "#     print(\"Error: pdf2text() failed\")\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pdf to specified folder given pdf_url and ind\n",
    "def download_pdf(pdf_url, pdf_folder):  \n",
    "    url = pdf_url\n",
    "    if url != url:\n",
    "        raise Exception(\"pdf_url is np.nan\")\n",
    "    else:\n",
    "        source = url.split(\"://\")[1].split(\"/\")[0]\n",
    "    \n",
    "    # pdf_path = os.path.join(pdf_folder, file_name)\n",
    "      \n",
    "    # get the pdf content\n",
    "    # options = webdriver.ChromeOptions()\n",
    "    # options.add_experimental_option('prefs', {\n",
    "    # \"download.default_directory\": pdf_folder, #Change default directory for downloads\n",
    "    # \"download.prompt_for_download\": False, #To auto download the file\n",
    "    # \"download.directory_upgrade\": True,\n",
    "    # \"download.open_pdf_in_system_reader\": False,\n",
    "    # \"plugins.always_open_pdf_externally\": True #It will not show PDF directly in chrome\n",
    "    # })\n",
    "    # driver = webdriver.Chrome(options=options)\n",
    "    # # response = driver.get(pdf_url)\n",
    "    # driver.navigate().to(url)\n",
    "    # time.sleep(5)\n",
    "    if source == \"www.sciencedirect.com\":\n",
    "        os.environ['WDM_LOG'] = '0'\n",
    "        options = Options()\n",
    "        options.add_argument('--headless')\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)\n",
    "        element = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.XPATH, \"//li[@class='ViewPDF']/a\")))\n",
    "        element.click()\n",
    "        time.sleep(10)\n",
    "    else: \n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_experimental_option('prefs', {\n",
    "        \"download.default_directory\": pdf_folder, #Change default directory for downloads\n",
    "        \"download.prompt_for_download\": False, #To auto download the file\n",
    "        \"download.directory_upgrade\": True,\n",
    "        \"plugins.always_open_pdf_externally\": True #It will not show PDF directly in chrome\n",
    "        })\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        response = driver.get(url)\n",
    "        time.sleep(10)\n",
    "    driver.close()\n",
    "\n",
    "    # try:\n",
    "    #     with open(pdf_path, 'wb') as pdf_object:\n",
    "    #         pdf_object.write(response.content)\n",
    "    #         driver.close()\n",
    "    #         return True\n",
    "    # except:\n",
    "    #     print(f'Failed downloading PDF:', url)\n",
    "    #     driver.close()\n",
    "    #     # print(f'HTTP response status code: {response.status_code}')\n",
    "    #     return False\n",
    "# --------------------start of test code--------------------\n",
    "# # pdf_url = 'https://www.sciencedirect.com/science/article/pii/S0896627320300052/pdfft?md5=3f0648c6385e6fae3a5a73b053903014&pid=1-s2.0-S0896627320300052-main.pdf'\n",
    "# # pdf_url = \"https://www.sciencedirect.com/science/article/pii/S1053811909013299?via%3Dihub\"\n",
    "# # pdf_url = \"https://linkinghub.elsevier.com/retrieve/pii/S1053811909013299\"\n",
    "# # pdf_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1181753/pdf/jphysiol00456-0559.pdf\"\n",
    "# pdf_url = \"https://www.sciencedirect.com/science/article/pii/S0896627320300052?via%3Dihub\"\n",
    "\n",
    "# # wiley.com\n",
    "# # pdf_url = \"https://onlinelibrary.wiley.com/doi/pdf/10.1002/cne.903130106\"\n",
    "# # pdf_url = \"https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/cne.903130106\"\n",
    "# # pdf_url = \"https://onlinelibrary.wiley.com/doi/pdf/10.1002/cne.902770204\"\n",
    "# # pdf_url = \"https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/cne.902770204\"\n",
    "# # pdf_url = \"https://onlinelibrary.wiley.com/doi/pdf/10.1002/cne.902970309\"\n",
    "# # pdf_url = \"https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/cne.902970309\"\n",
    "\n",
    "# # physiology.org\n",
    "# # pdf_url = \"https://journals.physiology.org/doi/pdf/10.1152/jn.2001.85.1.219\"\n",
    "\n",
    "# # springer.com\n",
    "# # pdf_url = \"https://link.springer.com/content/pdf/10.1007/BF00236173.pdf\"\n",
    "\n",
    "# # cell.com\n",
    "# # pdf_url = \"https://www.cell.com/neuron/pdf/S0896-6273(09)00170-6.pdf\"\n",
    "\n",
    "# # jneurosci.org\n",
    "# # pdf_url = \"https://www.jneurosci.org/content/jneuro/22/18/8117.full.pdf\"\n",
    "# # pdf_url, code = plib.get_final_redirected_url(pdf_url)\n",
    "# # print(pdf_url)\n",
    "# ind = 10\n",
    "# file_name = str(ind) + \".pdf\"\n",
    "# pdf_folder = fpath.litera_pdf_folder\n",
    "# download_pdf(pdf_url, pdf_folder)\n",
    "# # rename_pdf(file_name, pdf_folder, time_to_wait=60)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_www_ncbi_nlm_nih_gov(url, ind, pdf_folder):\n",
    "    try:\n",
    "        file_name = str(ind) + \".pdf\"\n",
    "        response = requests.get(url, headers=plib.headers)\n",
    "        # response = requests.get(url, stream=True, allow_redirects=True, headers=plib.headers)\n",
    "        \n",
    "        # download the .pdf file to the pdf_file_path folder\n",
    "        # write content in pdf file\n",
    "        pdf_path = os.path.join(pdf_folder, file_name)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            with open(pdf_path, 'wb') as pdf_object:\n",
    "                pdf_object.write(response.content)\n",
    "            print(f'Successfully downloaded PDF:', ind)\n",
    "        else:\n",
    "            print(f'Failed downloading PDF:' + 'pdf_url')\n",
    "            print(f'HTTP response status code: {response.status_code}')\n",
    "    except:\n",
    "        print(f'Failed downloading PDF:' + 'pdf_url')\n",
    "# --------------------start of test code--------------------\n",
    "# pdf_url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6577493/pdf/jneuro_14_5_2485.pdf\"\n",
    "# ind = 10\n",
    "# file_name = str(ind) + \".pdf\"\n",
    "# pdf_folder = \"/home/hou/myProjects/litera_pdfs\"\n",
    "# download_from_www_ncbi_nlm_nih_gov(pdf_url, ind, pdf_folder)\n",
    "# # rename_pdf(file_name, pdf_folder, time_to_wait=60)\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_from_ELSEVIER(doi, file_to_save_to):\n",
    "#     url = 'http://api.elsevier.com/content/article/doi:'+doi+'?view=FULL'\n",
    "#     headers = {\n",
    "#         'X-ELS-APIKEY': \"63f58b8b10cbc1bc923011c01c6301bb\",\n",
    "#         'Accept': 'application/pdf'\n",
    "#     }\n",
    "#     r = requests.get(url, stream=True, headers=headers)\n",
    "#     if r.status_code == 200:\n",
    "#         for chunk in r.iter_content(chunk_size=1024*1024):\n",
    "#             file_to_save_to.write(chunk)\n",
    "#         return True\n",
    "\n",
    "# # doi_list = pd.read_excel('list.xls')\n",
    "# # doi_list.columns = ['DOIs']\n",
    "# count = 0\n",
    "# # for doi in doi_list['DOIs']:\n",
    "# doi = \"10.1016/0006-8993(95)01338-5\"\n",
    "# # pdf = \"10\"\n",
    "# ind = 10\n",
    "# file_name = str(ind) + \".pdf\"\n",
    "# # if not os.path.exists(f'path/{pdf}.pdf'):\n",
    "# with open(file_name, 'wb') as pdf_object:\n",
    "#     get_pdf(doi, pdf_object)\n",
    "# pdf_object.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pdf\n",
    "def download_pdf(pdf_url, pdf_source, pdf_folder):\n",
    "    for website in params.website_pdfs:\n",
    "        func = None\n",
    "        if website in pdf_source:\n",
    "            # Get the function name by replacing \".\" with \"_\" and use globals() to call it\n",
    "            func_name = \"download_from_\" + website.replace(\".\", \"_\")\n",
    "            # print(func_name)\n",
    "            func = globals().get(func_name)\n",
    "            # print(func)\n",
    "            break\n",
    "    if func != None:\n",
    "        # print(func)\n",
    "        func(pdf_url, pdf_folder)\n",
    "    else:\n",
    "        print(\"The given url is not from a supported website: \", pdf_url)\n",
    "        raise Exception(\"Function does not exist for website:\", pdf_url)\n",
    "\n",
    "# rename downloaded pdf\n",
    "def rename_pdf(ind, pdf_folder, time_to_wait=60):\n",
    "    newname = str(ind) + \".pdf\"\n",
    "    time_counter = 0\n",
    "    filename = max([f for f in os.listdir(pdf_folder)], key=lambda xa :   os.path.getctime(os.path.join(pdf_folder,xa)))\n",
    "    while '.part' in filename:\n",
    "        time.sleep(1)\n",
    "        time_counter += 1\n",
    "        if time_counter > time_to_wait:\n",
    "            raise Exception('Waited too long for file to download')\n",
    "    filename = max([f for f in os.listdir(pdf_folder)], key=lambda xa :   os.path.getctime(os.path.join(pdf_folder,xa)))\n",
    "    os.rename(os.path.join(pdf_folder, filename), os.path.join(pdf_folder, newname))\n",
    "\n",
    "# download and rename pdf\n",
    "def download_and_rename_pdf(pdf_url, ind, pdf_folder):  \n",
    "    if pdf_url != pdf_url:\n",
    "        raise Exception(\"pdf_url is np.nan\")\n",
    "    else:\n",
    "        pdf_source = pdf_url.split(\"://\")[1].split(\"/\")[0]\n",
    "    download_pdf(pdf_url, pdf_source, pdf_folder)\n",
    "    rename_pdf(ind, pdf_folder, time_to_wait=60)\n",
    "    print(f\"Downloaded {ind}.pdf\")\n",
    "# --------------------start of test code--------------------\n",
    "# pdf_url = \"https://journals.physiology.org/doi/pdf/10.1152/jn.2001.85.1.219\"\n",
    "# ind = 10\n",
    "# pdf_folder = \"/home/hou/myProjects/litera_pdfs\"\n",
    "# download_and_rename_pdf(pdf_url, ind, pdf_folder)\n",
    "# ---------------------end of test code---------------------\n",
    "# https://journals.physiology.org/doi/10.1152/jn.2001.85.1.219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiley.com\n",
    "def wiley_com(url):\n",
    "    # initialize\n",
    "    info = {\n",
    "        \"doi\": np.nan,\n",
    "        \"pmid\": np.nan,\n",
    "        \"pmcid\": np.nan,\n",
    "        \"title\": np.nan,\n",
    "        \"abstract\": np.nan,\n",
    "        \"keywords\": np.nan,\n",
    "        \"pdf_link\": np.nan\n",
    "    }\n",
    "\n",
    "    # set up the webdriver\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "\n",
    "    # load the webpage\n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"Extracting content from:\" + url + \" failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "    \n",
    "    # doi\n",
    "    try:\n",
    "        doi = driver.find_element(By.XPATH, \"//a[@class='epub-doi']\").text.split(\"doi.org/\")[1]\n",
    "        doi = doi.strip()\n",
    "    except:\n",
    "        doi = np.nan\n",
    "    if doi == doi:\n",
    "        doi = doi.lower()\n",
    "\n",
    "    # pmid, pmcid\n",
    "    pmid = np.nan\n",
    "    pmcid = np.nan\n",
    "\n",
    "    # title\n",
    "    try:\n",
    "        title = driver.find_element(By.XPATH, \"//h1[@class='citation__title']\").text\n",
    "        title = title.strip()\n",
    "    except:\n",
    "        try:\n",
    "            title = driver.find_element(By.XPATH, \"//h2[@class='citation__title']\").text\n",
    "            title = title.strip()\n",
    "        except:\n",
    "            title = np.nan\n",
    "    \n",
    "    # abstract\n",
    "    try:\n",
    "        abstract = \"\"\n",
    "        try:\n",
    "            elems = driver.find_element(By.XPATH, \"//h3[text()='Abstract' or text()='ABSTRACT' or text()='Summary' or text()='SUMMARY']\").find_element(By.XPATH, \"following-sibling::div\").find_elements(By.XPATH, 'p')\n",
    "        except:\n",
    "            elems = driver.find_element(By.XPATH, \"//h2[text()='Abstract' or text()='ABSTRACT' or text()='Summary' or text()='SUMMARY']\").find_element(By.XPATH, \"following-sibling::div\").find_elements(By.XPATH, 'p')                \n",
    "        for elem in elems:\n",
    "            abstract = abstract + elem.text + \" \"\n",
    "        abstract = abstract.strip()\n",
    "    except:\n",
    "        abstract = np.nan\n",
    "    \n",
    "    # keywords\n",
    "    keywords = np.nan\n",
    "\n",
    "    # pdf_link\n",
    "    try:\n",
    "        pdf_link = driver.find_element(By.XPATH, \"//a[@title='ePDF']\").get_attribute('href')\n",
    "        pdf_link = pdf_link.strip()\n",
    "    except:\n",
    "        pdf_link = np.nan\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/abs/10.1002/cne.901980111\"\n",
    "# url = \"https://onlinelibrary.wiley.com/doi/10.1002/cne.21155\"\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/10.1002/(SICI)1096-9861(19981019)400:2%3C271::AID-CNE8%3E3.0.CO;2-6\"\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/10.1002/cne.902360304\"\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/10.1002/cne.902820107\"\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/10.1002/(SICI)1096-9861(19990726)410:2%3C211::AID-CNE4%3E3.0.CO;2-X\"\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/10.1002/cne.902940314\"\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/10.1002/cne.901990104\"\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/10.1002/(SICI)1096-9861(19981019)400:2%3C271::AID-CNE8%3E3.0.CO;2-6\"\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/10.1002/cne.902360304\"\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/10.1002/cne.24389\"\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/10.1002/(SICI)1096-9861(19960805)371:4%3C513::AID-CNE2%3E3.0.CO;2-7\"\n",
    "# # url = \"https://nyaspubs.onlinelibrary.wiley.com/doi/full/10.1196/annals.1300.030\"\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/10.1002/cne.23436\"\n",
    "# # url = \"https://onlinelibrary.wiley.com/doi/10.1002/9780470513545.ch4\"\n",
    "# info = wiley_com(url)\n",
    "# print(info[\"doi\"])\n",
    "# print(info[\"pmid\"])\n",
    "# print(info[\"pmcid\"])\n",
    "# print(info[\"title\"])\n",
    "# print(info[\"abstract\"])\n",
    "# print(info[\"keywords\"])\n",
    "# print(info[\"pdf_link\"])\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neurology.org\n",
    "def neurology_org(url):\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    # load the webpage\n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver = webdriver.Firefox(options=options)\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"Extracting content from:\" + url + \" failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "    \n",
    "    try:\n",
    "        doi = driver.find_element(By.XPATH, \"//span[contains(@class, 'highwire-cite-metadata-doi highwire-cite-metadata')]\").text.split(\"doi.org/\")[1]\n",
    "    except:\n",
    "        doi = np.nan\n",
    "    if doi == doi:\n",
    "        doi = doi.lower()\n",
    "    pmid = np.nan\n",
    "    pmcid = np.nan\n",
    "    \n",
    "    # title\n",
    "    try:\n",
    "        title = driver.find_element(By.XPATH, \"//h1[@id='page-title']\").text\n",
    "        title = title.strip()\n",
    "    except:\n",
    "        title = np.nan\n",
    "    \n",
    "    # abstract\n",
    "    try:\n",
    "        abstract = \"\"\n",
    "        elems = driver.find_element(By.XPATH, \"//div[@class='section abstract']\").find_elements(By.XPATH, \"following-sibling::p\")\n",
    "        for elem in elems:\n",
    "            abstract = abstract + elem.text + \" \"\n",
    "        abstract = abstract.strip()\n",
    "    except:\n",
    "        abstract = np.nan\n",
    "    \n",
    "    # keywords\n",
    "    keywords = np.nan\n",
    "\n",
    "    # pdf_link\n",
    "    try:\n",
    "        pdf_link = driver.find_element(By.XPATH, \"//a[@class='link-icon']\").get_attribute('href')\n",
    "        pdf_link = pdf_link.strip()\n",
    "    except:\n",
    "        pdf_link = np.nan\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "    driver.quit\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "# url = \"https://n.neurology.org/content/64/6/1014.short\"\n",
    "# # url = \"https://n.neurology.org/content/64/6/1014\"\n",
    "# # url = \"https://n.neurology.org/content/64/6/1014\"\n",
    "# # url = \"https://n.neurology.org/content/32/10/1198\"\n",
    "# # url = \"https://n.neurology.org/content/22/9/998\"\n",
    "# info = neurology_org(url)\n",
    "# print(info[\"doi\"])\n",
    "# print(info[\"pmid\"])\n",
    "# print(info[\"pmcid\"])\n",
    "# print(info[\"title\"])\n",
    "# print(info[\"abstract\"])\n",
    "# print(info[\"keywords\"])\n",
    "# print(info[\"introduction\"])\n",
    "# print(info[\"pdf_link\"])\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# www.biorxiv.org\n",
    "def www_biorxiv_org(url):\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    # load the webpage\n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver = webdriver.Firefox(options=options)\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"Extracting content from:\" + url + \" failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "    \n",
    "    # doi\n",
    "    try:\n",
    "        doi = driver.find_element(By.XPATH, \"//span[@class='highwire-cite-metadata-doi highwire-cite-metadata']\").find_element(By.TAG_NAME, \"span\").text.split(\"doi.org/\")[1]\n",
    "    except:\n",
    "        doi = np.nan\n",
    "    \n",
    "    # pmid, pmcid\n",
    "    pmid = np.nan\n",
    "    pmcid = np.nan\n",
    "\n",
    "    # title\n",
    "    try:\n",
    "        title = driver.find_element(By.XPATH, \"//h1[@id='page-title']\").text\n",
    "    except:\n",
    "        title = np.nan\n",
    "    \n",
    "    # abstract\n",
    "    try:\n",
    "        abstract = \"\"\n",
    "        abstract = driver.find_element(By.XPATH, \"//div[@class='section abstract')]\").find_elements(By.XPATH, 'following-sibling::p')\n",
    "        for elem in abstract:\n",
    "            abstract = abstract + elem.text + \" \"\n",
    "    except:\n",
    "        abstract = np.nan\n",
    "    \n",
    "    # keywords\n",
    "    keywords = np.nan\n",
    "    \n",
    "    # pdf_link\n",
    "    try:\n",
    "        pdf_link = driver.find_element(By.XPATH, \"//a[@class='biorxiv_pdf_dl_link')]\").get_attribute(\"href\")\n",
    "    except:\n",
    "        pdf_link = np.nan\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "    driver.quit\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "# url = \"https://www.biorxiv.org/content/10.1101/398917v1.abstract\"\n",
    "# # url = \"https://www.biorxiv.org/content/10.1101/2022.05.18.492367v2\"\n",
    "# # url = \"https://www.biorxiv.org/content/10.1101/398917v1\"\n",
    "# # url = \"https://www.biorxiv.org/content/10.1101/2021.02.01.429141v2\"\n",
    "# info = www_biorxiv_org(url)\n",
    "# print(info[\"doi\"])\n",
    "# print(info[\"pmid\"])\n",
    "# print(info[\"pmcid\"])\n",
    "# print(info[\"title\"])\n",
    "# print(info[\"abstract\"])\n",
    "# print(info[\"keywords\"])\n",
    "# print(info[\"pdf_link\"])\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# europepmc.org\n",
    "def europepmc_org(url):\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    # load the webpage\n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver = webdriver.Firefox(options=options)\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"Extracting content from:\" + url + \" failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "    \n",
    "    # doi\n",
    "    try:\n",
    "        doi = driver.find_element(By.XPATH, \"//span[@class='metadata--doi']\").find_element(By.TAG_NAME, \"a\").text.split(\"doi.org/\")[1]\n",
    "    except:\n",
    "        doi = np.nan\n",
    "    if doi == doi:\n",
    "        doi = doi.lower()\n",
    "    \n",
    "    # pmid, pmcid\n",
    "    try:\n",
    "        pmid = driver.find_element(By.XPATH, \"//span[contains(@class, 'metadata--pmid')]\").text.split(\"PMID: \")[1]\n",
    "    except:\n",
    "        pmid = np.nan\n",
    "    try:\n",
    "        pmcid = driver.find_element(By.XPATH, \"//span[contains(@class, 'metadata--pmcid')]\").text.split(\"PMCID: \")[1]\n",
    "    except:\n",
    "        pmcid = np.nan\n",
    "    \n",
    "    # title\n",
    "    try:\n",
    "        title = driver.find_element(By.XPATH, \"//h1[@class='article-metadata-title']\").text\n",
    "    except:\n",
    "        title = np.nan\n",
    "    \n",
    "    # abstract\n",
    "    try:\n",
    "        abstract = driver.find_element(By.XPATH, \"//div[contains(@class, 'abstract')]\").text\n",
    "    except:\n",
    "        abstract = np.nan\n",
    "    \n",
    "    # keywords\n",
    "    keywords = np.nan\n",
    "    \n",
    "    # pdf_link\n",
    "    pdf_link = \"://europepmc.org/\"\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "    driver.quit\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "# url = \"https://europepmc.org/article/MED/37298594\"\n",
    "# # url = \"https://europepmc.org/article/med/8784824\"\n",
    "# # url = \"https://europepmc.org/article/med/823649\"\n",
    "# # url = \"https://europepmc.org/article/med/4220147\"\n",
    "# info = europepmc_org(url)\n",
    "# print(info[\"doi\"])\n",
    "# print(info[\"pmid\"])\n",
    "# print(info[\"pmcid\"])\n",
    "# print(info[\"title\"])\n",
    "# print(info[\"abstract\"])\n",
    "# print(info[\"keywords\"])\n",
    "# print(info[\"pdf_link\"])\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iovs.arvojournals.org\n",
    "def iovs_arvojournals_org(url):\n",
    "    # initialize\n",
    "    info = {\n",
    "        \"doi\": np.nan,\n",
    "        \"pmid\": np.nan,\n",
    "        \"pmcid\": np.nan,\n",
    "        \"title\": np.nan,\n",
    "        \"abstract\": np.nan,\n",
    "        \"keywords\": np.nan,\n",
    "        \"pdf_link\": np.nan\n",
    "    }\n",
    "\n",
    "    # set up the webdriver\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "\n",
    "    # load the webpage\n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"Extracting content from:\" + url + \" failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "    \n",
    "    # doi\n",
    "    try:\n",
    "        elems = driver.find_element(By.XPATH, \"//div[@class='ww-citation large-view-only']\").find_elements(By.TAG_NAME, \"span\")\n",
    "        for elem in elems:\n",
    "            if \"doi.org\" in elem.text:\n",
    "                doi = elem.text.split(\"doi.org/\")[1]\n",
    "                break\n",
    "        doi = doi.strip()\n",
    "    except:\n",
    "        doi = np.nan\n",
    "    if doi == doi:\n",
    "        doi = doi.lower()\n",
    "\n",
    "    # pmid, pmcid\n",
    "    pmid = np.nan\n",
    "    pmcid = np.nan\n",
    "\n",
    "    # title\n",
    "    try:\n",
    "        title = driver.find_element(By.XPATH, \"//div[@class='wi-article-title article-title-main']\").text\n",
    "        title = title.strip()\n",
    "    except:\n",
    "        title = np.nan\n",
    "    \n",
    "    # abstract\n",
    "    try:\n",
    "        abstract = \"\"\n",
    "        elems = driver.find_element(By.XPATH, \"//section[@class='abstract']\").find_element(By.TAG_NAME, \"p\")\n",
    "        for elem in elems:\n",
    "            abstract = abstract + elem.text + \" \"\n",
    "        abstract = abstract.strip()\n",
    "    except:\n",
    "        abstract = np.nan\n",
    "    \n",
    "    # keywords\n",
    "    keywords = np.nan\n",
    "\n",
    "    # pdf_link\n",
    "    try:\n",
    "        pdf_link = driver.find_element(By.XPATH, \"//a[@id='pdfLink']\").get_attribute('data-article-url')\n",
    "        pdf_link = pdf_link.strip()\n",
    "    except:\n",
    "        pdf_link = np.nan\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "# url = \"https://iovs.arvojournals.org/article.aspx?articleid=2124655\"\n",
    "# # url = \"https://iovs.arvojournals.org/article.aspx?articleid=2659608\"\n",
    "# info = iovs_arvojournals_org(url)\n",
    "# print(info[\"doi\"])\n",
    "# print(info[\"pmid\"])\n",
    "# print(info[\"pmcid\"])\n",
    "# print(info[\"title\"])\n",
    "# print(info[\"abstract\"])\n",
    "# print(info[\"keywords\"])\n",
    "# print(info[\"pdf_link\"])\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# royalsocietypublishing.org\n",
    "def royalsocietypublishing_org(url):\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    # load the webpage\n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver = webdriver.Firefox(options=options)\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"Extracting content from:\" + url + \" failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "    \n",
    "    # doi\n",
    "    try:\n",
    "        doi = driver.find_element(By.XPATH, \"//a[contains(@class, 'epub-section__doi__text')]\").text.split(\"doi.org/\")[1]\n",
    "    except:\n",
    "        doi = np.nan\n",
    "    \n",
    "    # pmid, pmcid\n",
    "    pmid = np.nan\n",
    "    pmcid = np.nan\n",
    "\n",
    "    # title\n",
    "    try:\n",
    "        title = driver.find_element(By.XPATH, \"//h1[@class='citation__title']\").text\n",
    "        title = title.strip()\n",
    "    except:\n",
    "        title = np.nan\n",
    "    \n",
    "    # abstract\n",
    "    try:\n",
    "        abstract = \"\"\n",
    "        elems = driver.find_element(By.XPATH, \"//div[@class='abstractSection abstractInFull']\").find_elements(By.XPATH, 'p')\n",
    "        for elem in elems:\n",
    "            abstract = abstract + elem.text + \" \"\n",
    "        abstract = abstract.strip()\n",
    "    except:\n",
    "        abstract = np.nan\n",
    "    \n",
    "    # keywords\n",
    "    keywords = np.nan\n",
    "\n",
    "    # pdf_link\n",
    "    pdf_link = \"://royalsocietypublishing.org/\"\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "    driver.quit\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "# url = \"https://royalsocietypublishing.org/doi/abs/10.1098/rstb.2002.1171\"\n",
    "# # url = \"https://royalsocietypublishing.org/doi/10.1098/rspb.1972.0087\"\n",
    "# # url = \"https://royalsocietypublishing.org/doi/10.1098/rstb.1984.0021\"\n",
    "# info = royalsocietypublishing_org(url)\n",
    "# print(info[\"doi\"])\n",
    "# print(info[\"pmid\"])\n",
    "# print(info[\"pmcid\"])\n",
    "# print(info[\"title\"])\n",
    "# print(info[\"abstract\"])\n",
    "# print(info[\"keywords\"])\n",
    "# print(info[\"pdf_link\"])\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psychiatryonline.org\n",
    "def psychiatryonline_org(url):\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    # load the webpage\n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver = webdriver.Firefox(options=options)\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"Extracting content from:\" + url + \" failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "    \n",
    "    # doi\n",
    "    try:\n",
    "        doi = driver.find_element(By.XPATH, \"//a[contains(@class, 'epub-section__doi__text')]\").text.split(\"doi.org/\")[1]\n",
    "    except:\n",
    "        doi = np.nan\n",
    "    \n",
    "    # pmid, pmcid\n",
    "    pmid = np.nan\n",
    "    pmcid = np.nan\n",
    "    \n",
    "    # title\n",
    "    try:\n",
    "        title = driver.find_element(By.XPATH, \"//h1[@class='citation__title']\").text\n",
    "        title = title.strip()\n",
    "    except:\n",
    "        title = np.nan\n",
    "    \n",
    "    # abstract\n",
    "    try:\n",
    "        abstract = \"\"\n",
    "        elems = driver.find_element(By.XPATH, \"//div[@class='abstractSection abstractInFull']\").find_elements(By.XPATH, 'p')\n",
    "        for elem in elems:\n",
    "            abstract = abstract + elem.text + \" \"\n",
    "        abstract = abstract.strip()\n",
    "    except:\n",
    "        abstract = np.nan\n",
    "    \n",
    "    # keywords\n",
    "    keywords = np.nan\n",
    "\n",
    "    # pdf_link\n",
    "    try:\n",
    "        elems = driver.find_element(By.XPATH, \"//ul[@title='coolBar__drop rlist w-slide--list cloned hidden-xs hidden-sm js--open']\").find_elements(By.TAG_NAME, \"li\")\n",
    "        for elem in elems:\n",
    "            t = elem.find_element(By.TAG_NAME, \"a\").find_element(By.TAG_NAME, \"span\")\n",
    "            if \"PDF\" in t:\n",
    "                pdf_link = elem.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "                break\n",
    "        pdf_link = pdf_link.strip()\n",
    "    except:\n",
    "        pdf_link = np.nan\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "    driver.quit\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "# url = \"https://ajp.psychiatryonline.org/doi/full/10.1176/appi.ajp.161.5.896\"\n",
    "# # url = \"https://ajp.psychiatryonline.org/doi/10.1176/ajp.156.11.1709\"\n",
    "# # url = \"https://ajp.psychiatryonline.org/doi/full/10.1176/appi.ajp.158.9.1411\"\n",
    "# info = psychiatryonline_org(url)\n",
    "# print(info[\"doi\"])\n",
    "# print(info[\"pmid\"])\n",
    "# print(info[\"pmcid\"])\n",
    "# print(info[\"title\"])\n",
    "# print(info[\"abstract\"])\n",
    "# print(info[\"keywords\"])\n",
    "# print(info[\"pdf_link\"])\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct.mit.edu\n",
    "def direct_mit_edu(url):\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    # load the webpage\n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver = webdriver.Firefox(options=options)\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"Extracting content from:\" + url + \" failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "    \n",
    "    # doi\n",
    "    try:\n",
    "        doi = driver.find_element(By.XPATH, \"//div[@class='citation-doi')]\").find_element(By.TAG_NAME, \"a\").text.split(\"doi.org/\")[1]\n",
    "    except:\n",
    "        doi = np.nan\n",
    "    \n",
    "    # pmid, pmcid\n",
    "    pmid = np.nan\n",
    "    pmcid = np.nan\n",
    "    \n",
    "    # title\n",
    "    try:\n",
    "        title = driver.find_element(By.XPATH, \"//h1[@class='wi-article-title article-title-main']\").text\n",
    "        title = title.strip()\n",
    "    except:\n",
    "        title = np.nan\n",
    "    \n",
    "    # abstract\n",
    "    try:\n",
    "        abstract = \"\"\n",
    "        elems = driver.find_element(By.XPATH, \"//section[@class='abstract']\").find_elements(By.XPATH, 'p')\n",
    "        for elem in elems:\n",
    "            abstract = abstract + elem.text + \" \"\n",
    "        abstract = abstract.strip()\n",
    "    except:\n",
    "        abstract = np.nan\n",
    "    \n",
    "    # keywords\n",
    "    keywords = np.nan\n",
    "\n",
    "    # pdf_link\n",
    "    try:\n",
    "        pdf_link = driver.find_element(By.XPATH, \"//a[contains(@title,'article-pdfLink']\").get_attribute(\"href\")\n",
    "        pdf_link = pdf_link.strip()\n",
    "    except:\n",
    "        pdf_link = np.nan\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "    driver.quit\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "# url = \"https://direct.mit.edu/jocn/article/10/6/691/3328/The-von-Restorff-Effect-in-Visual-Object\"\n",
    "# # url = \"https://direct.mit.edu/neco/article-abstract/15/4/735/6721/Modeling-Reverse-Phi-Motion-Selective-Neurons-in?redirectedFrom=fulltext\"\n",
    "# # url = \"https://direct.mit.edu/neco/article-abstract/24/7/1695/7782/Neural-Information-Processing-with-Feedback?redirectedFrom=fulltext\"\n",
    "# info = direct_mit_edu(url)\n",
    "# print(info[\"doi\"])\n",
    "# print(info[\"pmid\"])\n",
    "# print(info[\"pmcid\"])\n",
    "# print(info[\"title\"])\n",
    "# print(info[\"abstract\"])\n",
    "# print(info[\"keywords\"])\n",
    "# print(info[\"pdf_link\"])\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thejns.org\n",
    "def thejns_org(url):\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    # load the webpage\n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver = webdriver.Firefox(options=options)\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"Extracting content from:\" + url + \" failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "    \n",
    "    # doi\n",
    "    try:\n",
    "        doi = driver.find_element(By.XPATH, \"//a[@class='c-Button--link']/dd/span/a\").text.split(\"doi.org/\")[1]\n",
    "    except:\n",
    "        doi = np.nan\n",
    "    \n",
    "    # pmid, pmcid\n",
    "    pmid = np.nan\n",
    "    pmcid = np.nan\n",
    "\n",
    "    # title\n",
    "    try:\n",
    "        title = driver.find_element(By.TAG_NAME, \"h1\").text\n",
    "        title = title.strip()\n",
    "    except:    \n",
    "        title = np.nan\n",
    "    \n",
    "    # abstract\n",
    "    try:\n",
    "        abstract = \"\"\n",
    "        elems = driver.find_element(By.XPATH, \"//section[@class='abstract']\").find_elements(By.XPATH, 'p')\n",
    "        for elem in elems:\n",
    "            abstract = abstract + elem.text + \" \"\n",
    "        abstract = abstract.strip()\n",
    "    except:\n",
    "        abstract = np.nan\n",
    "    \n",
    "    # keywords\n",
    "    keywords = np.nan\n",
    "\n",
    "    # pdf_link\n",
    "    try:\n",
    "        pdf_link = driver.find_element(By.XPATH, \"//a[@title='Download PDF']\").get_attribute('href')\n",
    "        pdf_link = pdf_link.strip()\n",
    "    except:\n",
    "        pdf_link = np.nan\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "    driver.quit\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "# url = \"https://thejns.org/view/journals/j-neurosurg/86/1/article-p77.xml\"\n",
    "# url = \"https://thejns.org/view/journals/j-neurosurg/41/2/article-p217.xml\"\n",
    "# info = thejns_org(url)\n",
    "# print(info[\"doi\"])\n",
    "# print(info[\"pmid\"])\n",
    "# print(info[\"pmcid\"])\n",
    "# print(info[\"title\"])\n",
    "# print(info[\"abstract\"])\n",
    "# print(info[\"keywords\"])\n",
    "# print(info[\"pdf_link\"])\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# www.annualreviews.org\n",
    "def www_annualreviews_org(url):\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    # load the webpage\n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver = webdriver.Firefox(options=options)\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"Extracting content from:\" + url + \" failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "    \n",
    "    # doi\n",
    "    try:\n",
    "        doi = driver.find_element(By.XPATH, \"//div[@class='article-details]/p/a\").text.split(\"doi.org/\")[1]\n",
    "    except:\n",
    "        doi = np.nan\n",
    "    \n",
    "    # pmid, pmcid\n",
    "    pmid = np.nan\n",
    "    pmcid = np.nan\n",
    "\n",
    "    # title\n",
    "    try:\n",
    "        title = driver.find_element(By.TAG_NAME, \"h1\").text\n",
    "        title = title.strip()\n",
    "    except:    \n",
    "        title = np.nan\n",
    "    \n",
    "    # abstract\n",
    "    try:\n",
    "        abstract = \"\"\n",
    "        elems = driver.find_element(By.XPATH, \"//div[@class='abstractSection abstractInFull']\").find_elements(By.XPATH, 'p')\n",
    "        for elem in elems:\n",
    "            abstract = abstract + elem.text + \" \"\n",
    "        abstract = abstract.strip()\n",
    "    except:\n",
    "        abstract = np.nan\n",
    "    \n",
    "    # keywords\n",
    "    try:\n",
    "        keywords = \"\"\n",
    "        elems = driver.find_element(By.XPATH, \"//div[@class='hlFld-KeywordText']/p[1]/kwd-group[1]\").find_elements(By.XPATH, 'a')\n",
    "        for elem in elems:\n",
    "            keywords = keywords + elem.text + \", \"\n",
    "    except:\n",
    "        keywords = np.nan\n",
    "\n",
    "    # pdf_link\n",
    "    try:\n",
    "        pdf_link = driver.find_element(By.XPATH, \"//a[@class='btn icon-pdf']\").get_attribute('href')\n",
    "        pdf_link = pdf_link.strip()\n",
    "    except:\n",
    "        pdf_link = np.nan\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "    driver.quit\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "# url = \"https://www.annualreviews.org/doi/10.1146/annurev.ne.02.030179.001303\"\n",
    "# url = \"https://www.annualreviews.org/doi/10.1146/annurev.ps.23.020172.002023\"\n",
    "# url = \"https://www.annualreviews.org/doi/10.1146/annurev.neuro.23.1.127\"\n",
    "# info = www_annualreviews_org(url)\n",
    "# print(info[\"doi\"])\n",
    "# print(info[\"pmid\"])\n",
    "# print(info[\"pmcid\"])\n",
    "# print(info[\"title\"])\n",
    "# print(info[\"abstract\"])\n",
    "# print(info[\"keywords\"])\n",
    "# print(info[\"pdf_link\"])\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aspetjournals.org\n",
    "def aspetjournals_org(url):\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    # load the webpage\n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver = webdriver.Firefox(options=options)\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"Extracting content from:\" + url + \" failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "    \n",
    "    # doi\n",
    "    try:\n",
    "        doi = driver.find_element(By.XPATH, \"//span[contains(@class, 'cite-metadata-doi')]\").text.split(\"doi.org/\")[1]\n",
    "    except:\n",
    "        doi = np.nan\n",
    "    if doi == doi:\n",
    "        doi = doi.lower()\n",
    "\n",
    "    # pmid, pmcid\n",
    "    pmid = np.nan\n",
    "    pmcid = np.nan\n",
    "\n",
    "    # title\n",
    "    try:\n",
    "        title = driver.find_element(By.XPATH, \"//h1[@id='page-title']\").text\n",
    "        title = title.strip()\n",
    "    except:\n",
    "        title = np.nan\n",
    "    \n",
    "    # abstract\n",
    "    try:\n",
    "        abstract = \"\"\n",
    "        elems = driver.find_element(By.XPATH, \"//div[@class='section abstract']\").find_elements(By.XPATH, 'p')\n",
    "        for elem in elems:\n",
    "            abstract = abstract + elem.text + \" \"\n",
    "        abstract = abstract.strip()\n",
    "    except:\n",
    "        abstract = np.nan\n",
    "    \n",
    "    # keywords\n",
    "    keywords = np.nan\n",
    "\n",
    "    # pdf_link\n",
    "    # try:\n",
    "    #     pdf_link = driver.find_element(By.XPATH, \"//a[@data-panel-name='jnl_jpet_tab_pdf']\").get_attribute('href')\n",
    "    #     pdf_link = pdf_link.strip()\n",
    "    # except:\n",
    "    #     pdf_link = np.nan\n",
    "    pdf_link = np.nan\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "    driver.quit\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "# url = \"https://jpet.aspetjournals.org/content/321/1/116.short\"\n",
    "# # url = \"https://jpet.aspetjournals.org/content/316/2/772\"\n",
    "# # url = \"https://jpet.aspetjournals.org/content/319/2/561\"\n",
    "# # url = \"https://jpet.aspetjournals.org/content/325/2/629\"\n",
    "# info = aspetjournals_org(url)\n",
    "# print(info[\"doi\"])\n",
    "# print(info[\"pmid\"])\n",
    "# print(info[\"pmcid\"])\n",
    "# print(info[\"title\"])\n",
    "# print(info[\"abstract\"])\n",
    "# print(info[\"keywords\"])\n",
    "# print(info[\"pdf_link\"])\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jnm.snmjournals.org\n",
    "def jnm_snmjournals_org(url):\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    # load the webpage\n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver = webdriver.Firefox(options=options)\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"Extracting content from:\" + url + \" failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "    \n",
    "    # doi\n",
    "    # try:\n",
    "    #     doi = driver.find_element(By.XPATH, \"//span[contains(@class, 'cite-metadata-doi')]\").text.split(\"doi.org/\")[1]\n",
    "    # except:\n",
    "    #     doi = np.nan\n",
    "    # if doi == doi:\n",
    "    #     doi = doi.lower()\n",
    "    doi = np.nan\n",
    "\n",
    "    # pmid, pmcid\n",
    "    pmid = np.nan\n",
    "    pmcid = np.nan\n",
    "\n",
    "    # title\n",
    "    try:\n",
    "        title = driver.find_element(By.XPATH, \"//h1[@id='page-title']\").text\n",
    "        title = title.strip()\n",
    "    except:\n",
    "        title = np.nan\n",
    "    \n",
    "    # abstract\n",
    "    try:\n",
    "        abstract = \"\"\n",
    "        elems = driver.find_element(By.XPATH, \"//div[@class='section abstract']\").find_elements(By.XPATH, 'p')\n",
    "        for elem in elems:\n",
    "            abstract = abstract + elem.text + \" \"\n",
    "        abstract = abstract.strip()\n",
    "    except:\n",
    "        abstract = np.nan\n",
    "    \n",
    "    # keywords\n",
    "    keywords = np.nan\n",
    "\n",
    "    # pdf_link\n",
    "    try:\n",
    "        pdf_link = driver.find_element(By.XPATH, \"//a[@data-panel-name='jnl_snm_tab_pdf']\").get_attribute('href')\n",
    "        pdf_link = pdf_link.strip()\n",
    "    except:\n",
    "        pdf_link = np.nan\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "    driver.quit\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "# url = \"https://jnm.snmjournals.org/content/39/2/281.long\"\n",
    "# # url = \"https://jnm.snmjournals.org/content/45/5/878.long\"\n",
    "# # url = \"https://jnm.snmjournals.org/content/36/7/1275.long\"\n",
    "# info = jnm_snmjournals_org(url)\n",
    "# print(info[\"doi\"])\n",
    "# print(info[\"pmid\"])\n",
    "# print(info[\"pmcid\"])\n",
    "# print(info[\"title\"])\n",
    "# print(info[\"abstract\"])\n",
    "# print(info[\"keywords\"])\n",
    "# print(info[\"pdf_link\"])\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# www.architalbiol.org\n",
    "def www_architalbiol_org(url):\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    # load the webpage\n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver = webdriver.Firefox(options=options)\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"Extracting content from:\" + url + \" failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "    \n",
    "    # doi\n",
    "    # try:\n",
    "    #     doi = driver.find_element(By.XPATH, \"//span[contains(@class, 'cite-metadata-doi')]\").text.split(\"doi.org/\")[1]\n",
    "    # except:\n",
    "    #     doi = np.nan\n",
    "    # if doi == doi:\n",
    "    #     doi = doi.lower()\n",
    "    doi = np.nan\n",
    "\n",
    "    # pmid, pmcid\n",
    "    pmid = np.nan\n",
    "    pmcid = np.nan\n",
    "\n",
    "    # title\n",
    "    try:\n",
    "        title = driver.find_element(By.XPATH, \"//div[@id='articleTitle']\").find_element(By.TAG_NAME, \"h3\").text\n",
    "        title = title.strip()\n",
    "    except:\n",
    "        title = np.nan\n",
    "    \n",
    "    # abstract\n",
    "    # try:\n",
    "    #     abstract = \"\"\n",
    "    #     elems = driver.find_element(By.XPATH, \"//div[@class='section abstract']\").find_elements(By.XPATH, 'p')\n",
    "    #     for elem in elems:\n",
    "    #         abstract = abstract + elem.text + \" \"\n",
    "    #     abstract = abstract.strip()\n",
    "    # except:\n",
    "    #     abstract = np.nan\n",
    "    abstract = np.nan\n",
    "    \n",
    "    # keywords\n",
    "    keywords = np.nan\n",
    "\n",
    "    # pdf_link\n",
    "    # try:\n",
    "    #     pdf_link = driver.find_element(By.XPATH, \"//a[@data-panel-name='jnl_snm_tab_pdf']\").get_attribute('href')\n",
    "    #     pdf_link = pdf_link.strip()\n",
    "    # except:\n",
    "    #     pdf_link = np.nan\n",
    "    pdf_link = \"://www.architalbiol.org/\"\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "    driver.quit\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "# url = \"http://www.architalbiol.org/index.php/aib/article/view/11423/\"\n",
    "# # url = \"http://www.architalbiol.org/index.php/aib/article/view/140315/\"\n",
    "# # url = \"http://www.architalbiol.org/index.php/aib/article/view/122301/\"\n",
    "# info = www_architalbiol_org(url)\n",
    "# print(info[\"doi\"])\n",
    "# print(info[\"pmid\"])\n",
    "# print(info[\"pmcid\"])\n",
    "# print(info[\"title\"])\n",
    "# print(info[\"abstract\"])\n",
    "# print(info[\"keywords\"])\n",
    "# print(info[\"pdf_link\"])\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# www.ahajournals.org\n",
    "def www_ahajournals_org(url):\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    # load the webpage\n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver = webdriver.Firefox(options=options)\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"Extracting content from:\" + url + \" failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "    \n",
    "    # doi\n",
    "    try:\n",
    "        doi = driver.find_element(By.XPATH, \"//a[contains(@class, 'epub-section__doi__text')]\").text.split(\"doi.org/\")[1]\n",
    "    except:\n",
    "        doi = np.nan\n",
    "\n",
    "    # pmid, pmcid\n",
    "    pmid = np.nan\n",
    "    pmcid = np.nan\n",
    "\n",
    "    # title\n",
    "    try:\n",
    "        title = driver.find_element(By.XPATH, \"//h1[@class='citation__title']\").text\n",
    "        title = title.strip()\n",
    "    except:\n",
    "        title = np.nan\n",
    "    \n",
    "    # abstract\n",
    "    try:\n",
    "        abstract = \"\"\n",
    "        elems = driver.find_element(By.XPATH, \"//div[@class='abstractSection abstractInFull']\").find_elements(By.XPATH, 'p')\n",
    "        for elem in elems:\n",
    "            abstract = abstract + elem.text + \" \"\n",
    "        abstract = abstract.strip()\n",
    "    except:\n",
    "        abstract = np.nan\n",
    "    \n",
    "    # keywords\n",
    "    keywords = np.nan\n",
    "\n",
    "    # pdf_link\n",
    "    try:\n",
    "        pdf_link = driver.find_element(By.XPATH, \"//a[@class='btn btn--danger']\").get_attribute('href')\n",
    "        pdf_link = pdf_link.strip()\n",
    "    except:\n",
    "        pdf_link = np.nan\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "    driver.quit\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "# url = \"https://www.ahajournals.org/doi/full/10.1161/01.STR.0000087786.38997.9E\"\n",
    "# # url = \"https://www.ahajournals.org/doi/10.1161/01.STR.6.1.42\"\n",
    "# # url = \"https://www.ahajournals.org/doi/10.1161/01.STR.32.1.107\"\n",
    "# # url = \"https://www.ahajournals.org/doi/10.1161/01.STR.29.11.2377\"\n",
    "# info = www_ahajournals_org(url)\n",
    "# print(info[\"doi\"])\n",
    "# print(info[\"pmid\"])\n",
    "# print(info[\"pmcid\"])\n",
    "# print(info[\"title\"])\n",
    "# print(info[\"abstract\"])\n",
    "# print(info[\"keywords\"])\n",
    "# print(info[\"introduction\"])\n",
    "# print(info[\"pdf_link\"])\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pubs.acs.org\n",
    "def pubs_acs_org(url):\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    # load the webpage\n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver = webdriver.Firefox(options=options)\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"Extracting content from:\" + url + \" failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "    \n",
    "    # doi\n",
    "    try:\n",
    "        doi = driver.find_element(By.XPATH, \"//div[contains(@class, 'article_header-doiurl')]\").find_element(By.TAG_NAME, \"a\").text.split(\"doi.org/\")[1]\n",
    "    except:\n",
    "        doi = np.nan\n",
    "\n",
    "    # pmid, pmcid\n",
    "    pmid = np.nan\n",
    "    pmcid = np.nan\n",
    "\n",
    "    # title\n",
    "    try:\n",
    "        title = driver.find_element(By.XPATH, \"//h1[@class='article_header-title']\").find_element(By.TAG_NAME, \"span\").text\n",
    "        title = title.strip()\n",
    "    except:\n",
    "        title = np.nan\n",
    "    \n",
    "    # abstract\n",
    "    try:\n",
    "        abstract = driver.find_element(By.XPATH, \"//p[@class='articleBody_abstractText']\").text\n",
    "        abstract = abstract.strip()\n",
    "    except:\n",
    "        abstract = np.nan\n",
    "    \n",
    "    # keywords\n",
    "    keywords = np.nan\n",
    "\n",
    "    # pdf_link\n",
    "    # try:\n",
    "    #     pdf_link = driver.find_element(By.XPATH, \"//a[@class='btn btn--danger']\").get_attribute('href')\n",
    "    #     pdf_link = pdf_link.strip()\n",
    "    # except:\n",
    "    #     pdf_link = np.nan\n",
    "    pdf_link = np.nan\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "    driver.quit\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "# url = \"https://pubs.acs.org/doi/10.1021/jm030384e\"\n",
    "# # url = \"https://pubs.acs.org/doi/10.1021/jm301597s\"\n",
    "# # url = \"https://pubs.acs.org/doi/10.1021/acs.molpharmaceut.8b01209\"\n",
    "# info = pubs_acs_org(url)\n",
    "# print(info[\"doi\"])\n",
    "# print(info[\"pmid\"])\n",
    "# print(info[\"pmcid\"])\n",
    "# print(info[\"title\"])\n",
    "# print(info[\"abstract\"])\n",
    "# print(info[\"keywords\"])\n",
    "# print(info[\"pdf_link\"])\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# www.thieme-connect.de\n",
    "def www_thieme_connect_de(url):\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    # load the webpage\n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver = webdriver.Firefox(options=options)\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"Extracting content from:\" + url + \" failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "    \n",
    "    # doi\n",
    "    try:\n",
    "        doi = driver.find_element(By.XPATH, \"//div[contains(@class, 'doi')]\").find_element(By.TAG_NAME, \"a\").text.split(\"DOI: \")[1]\n",
    "    except:\n",
    "        doi = np.nan\n",
    "\n",
    "    # pmid, pmcid\n",
    "    pmid = np.nan\n",
    "    pmcid = np.nan\n",
    "\n",
    "    # title\n",
    "    try:\n",
    "        title = driver.find_element(By.TAG_NAME, \"h1\").text\n",
    "        title = title.strip()\n",
    "    except:\n",
    "        title = np.nan\n",
    "    \n",
    "    # abstract\n",
    "    try:\n",
    "        abstract = driver.find_element(By.XPATH, \"//h3[contains(text(),'Abstract')]\").find_element(By.XPATH, \"followingslibing::p\").text\n",
    "        abstract = abstract.strip()\n",
    "    except:\n",
    "        abstract = np.nan\n",
    "    \n",
    "    # keywords\n",
    "    try:\n",
    "        keywords = driver.find_element(By.XPATH, \"//div[@class='articleKeywords')]\").find_element(By.TAG_NAME, \"p\").text\n",
    "        keywords = keywords.strip()\n",
    "    except:\n",
    "        keywords = np.nan\n",
    "    # keywords = np.nan\n",
    "\n",
    "    # pdf_link\n",
    "    # try:\n",
    "    #     pdf_link = driver.find_element(By.XPATH, \"//a[@class='btn btn--danger']\").get_attribute('href')\n",
    "    #     pdf_link = pdf_link.strip()\n",
    "    # except:\n",
    "    #     pdf_link = np.nan\n",
    "    pdf_link = \"://www.thieme-connect.de/\"\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "    driver.quit\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "# url = \"https://www.thieme-connect.de/products/ejournals/abstract/10.1055/s-2007-973495\"\n",
    "# # url = \"https://www.thieme-connect.de/products/ejournals/abstract/10.1055/s-0031-1299170\"\n",
    "# info = www_thieme_connect_de(url)\n",
    "# print(info[\"doi\"])\n",
    "# print(info[\"pmid\"])\n",
    "# print(info[\"pmcid\"])\n",
    "# print(info[\"title\"])\n",
    "# print(info[\"abstract\"])\n",
    "# print(info[\"keywords\"])\n",
    "# print(info[\"pdf_link\"])\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pubs.asahq.org\n",
    "def pubs_asahq_org(url):\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    # load the webpage\n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver = webdriver.Firefox(options=options)\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"Extracting content from:\" + url + \" failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "    \n",
    "    # doi\n",
    "    try:\n",
    "        doi = driver.find_element(By.XPATH, \"//div[contains(@class, 'citation-doi')]\").find_element(By.TAG_NAME, \"a\").text.split(\"doi.org/\")[1]\n",
    "    except:\n",
    "        doi = np.nan\n",
    "\n",
    "    # pmid, pmcid\n",
    "    pmid = np.nan\n",
    "    pmcid = np.nan\n",
    "\n",
    "    # title\n",
    "    try:\n",
    "        title = driver.find_element(By.XPATH, \"//h1[contains(@class,'article-title')]\").text\n",
    "        title = title.strip()\n",
    "    except:\n",
    "        title = np.nan\n",
    "    \n",
    "    # abstract\n",
    "    try:\n",
    "        abstract = driver.find_element(By.XPATH, \"//section[@class='abstract']\").text\n",
    "        abstract = abstract.strip()\n",
    "    except:\n",
    "        abstract = np.nan\n",
    "    \n",
    "    # keywords\n",
    "    try:\n",
    "        keywords = \"\"\n",
    "        elems = driver.find_element(By.XPATH, \"//div[@class='content-metadata-topics')]\").find_elements(By.TAG_NAME, \"a\")\n",
    "        for elem in elems:\n",
    "            keywords = keywords + elem.text + \"; \"\n",
    "        keywords = keywords.strip()\n",
    "    except:\n",
    "        keywords = np.nan\n",
    "    # keywords = np.nan\n",
    "\n",
    "    # pdf_link\n",
    "    # try:\n",
    "    #     pdf_link = driver.find_element(By.XPATH, \"//a[contains(@class,'article-pdfLink']\").get_attribute('href')\n",
    "    #     pdf_link = pdf_link.strip()\n",
    "    # except:\n",
    "    #     pdf_link = np.nan\n",
    "    pdf_link = \"://www.thieme-connect.de/\"\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "    driver.quit\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "# url = \"https://pubs.asahq.org/anesthesiology/article/116/2/372/13001/Ketamine-induced-Neuroapoptosis-in-the-Fetal-and\"\n",
    "# # url = \"https://pubs.asahq.org/anesthesiology/article/66/1/39/29057/The-Effects-of-Dextrose-Infusion-and-Head-Position\"\n",
    "# # url = \"https://pubs.asahq.org/anesthesiology/article/98/5/1101/40355/Neural-Mechanism-of-Propofol-Anesthesia-in-Severe\"\n",
    "# info = pubs_asahq_org(url)\n",
    "# print(info[\"doi\"])\n",
    "# print(info[\"pmid\"])\n",
    "# print(info[\"pmcid\"])\n",
    "# print(info[\"title\"])\n",
    "# print(info[\"abstract\"])\n",
    "# print(info[\"keywords\"])\n",
    "# print(info[\"pdf_link\"])\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# www.ingentaconnect.com\n",
    "def www_ingentaconnect_com(url):\n",
    "    os.environ['WDM_LOG'] = '0'\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    # load the webpage\n",
    "    error_label = 0\n",
    "    while(error_label == 0):\n",
    "        try:\n",
    "            driver = webdriver.Firefox(options=options)\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            error_label = 1\n",
    "        except:\n",
    "            print(\"Extracting content from:\" + url + \" failed, retrying... This might take longer than 5 minutes...\")\n",
    "            time.sleep(5*60)\n",
    "            error_label = 0\n",
    "    \n",
    "    # doi\n",
    "    # try:\n",
    "    #     doi = driver.find_element(By.XPATH, \"//div[contains(@class, 'citation-doi')]\").find_element(By.TAG_NAME, \"a\").text.split(\"doi.org/\")[1]\n",
    "    # except:\n",
    "    #     doi = np.nan\n",
    "    doi = np.nan\n",
    "\n",
    "    # pmid, pmcid\n",
    "    pmid = np.nan\n",
    "    pmcid = np.nan\n",
    "\n",
    "    # title\n",
    "    try:\n",
    "        title = driver.find_element(By.XPATH, \"//h1[contains(@class,'abstract-heading')]\").text\n",
    "        title = title.strip()\n",
    "    except:\n",
    "        title = np.nan\n",
    "    \n",
    "    # abstract\n",
    "    try:\n",
    "        abstract = driver.find_element(By.XPATH, \"//div[@class='Abst']\").text\n",
    "        abstract = abstract.strip()\n",
    "    except:\n",
    "        abstract = np.nan\n",
    "    \n",
    "    # keywords\n",
    "    try:\n",
    "        keywords = \"\"\n",
    "        elems = driver.find_element(By.XPATH, \"//div[@class='content-metadata-topics')]\").find_elements(By.TAG_NAME, \"a\")\n",
    "        for elem in elems:\n",
    "            keywords = keywords + elem.text + \"; \"\n",
    "        keywords = keywords.strip()\n",
    "    except:\n",
    "        keywords = np.nan\n",
    "    # keywords = np.nan\n",
    "\n",
    "    # pdf_link\n",
    "    # try:\n",
    "    #     pdf_link = driver.find_element(By.XPATH, \"//ul[contains(@class,'right-col-download contain']\").get_attribute('href')\n",
    "    #     pdf_link = pdf_link.strip()\n",
    "    # except:\n",
    "    #     pdf_link = np.nan\n",
    "    pdf_link = \"://www.ingentaconnect.com/\"\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    info = {\n",
    "        \"doi\": doi,\n",
    "        \"pmid\": pmid,\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"pdf_link\": pdf_link\n",
    "    }\n",
    "    driver.quit\n",
    "\n",
    "    return info\n",
    "# --------------------start of test code--------------------\n",
    "# url = \"https://www.ingentaconnect.com/content/aalas/cm/2000/00000050/00000002/art00006;jsessionid=9jxpglps7nq4.x-ic-live-03\"\n",
    "# info = www_ingentaconnect_com(url)\n",
    "# print(info[\"doi\"])\n",
    "# print(info[\"pmid\"])\n",
    "# print(info[\"pmcid\"])\n",
    "# print(info[\"title\"])\n",
    "# print(info[\"abstract\"])\n",
    "# print(info[\"keywords\"])\n",
    "# print(info[\"pdf_link\"])\n",
    "# ---------------------end of test code---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ujms.net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# journals.biologists.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# www.microbiologyresearch.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# journals.aps.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# www.imrpress.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# www.researchsquare.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ieeexplore.ieee.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# papers.ssrn.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
